id,pr_number,body,distilbert_sentiment_label,codebert_sentiment_label,deberta_sentiment_label,distilbert_confidence,codebert_confidence,deberta_confidence,majority_label,final_decision,decision_reason
1374636485,11695,"note, this isn't making keys evenly distributed, as you note, different buckets can have different amount of keys. this might not be a big deal, but should be noted.",0,0,0,0.9723939299583436,0.956656277179718,0.980928122997284,0.0,accept,unanimous_agreement
1377703989,11695,"thanks for looking at this and providing feedback. as it stands couple areas for improvement are: * making random more fair. * caching/pushing through slot id so we don't need to calculate hashes again. another small debt on my side might be writing some tests for scan in cluster mode, to ensure that combined cursor+slot logic works properly (i've manually tested it so far and was relying on few tests that already exist, i haven't checked if this case is covered by existing tests). i'll try to address these issues, meanwhile let's continue discussion in the comments and see if any other area needs to be improved as well.",1,1,1,0.9214761257171632,0.7683625817298889,0.9689920544624328,1.0,accept,unanimous_agreement
1378046083,11695,"i've tested your approach with caching hashes for the last key, and at first couldn't believe the numbers, read performance doubled, write performance increased 20-30%! but then i've realized that it's unfortunately broken, the problem seems to be that addresses are getting reused and we can't be sure that pointers being the same mean actually keys being the same. ![a link] on this screenshot you can see lastkey is pointing to the same place as the key, but hash is actually different from the one calculated before. (i've added extra call to _keyhashslot just to catch this issue more easily) in fact jemalloc seem to be reusing same address over and over again for different keys. so we'll probably need to use a different approach for this.",-1,-1,-1,0.8005994558334351,0.8674191832542419,0.9657402634620668,-1.0,accept,unanimous_agreement
1379322469,11695,"if we do what suggested and move slot id to lsb then cursor can occupy remaining 50 bits. my reasoning for using 48 was to leave sign bit alone to make sure that numbers look the same on the client, even if signed long is used there, also left 1 extra bit for parity and potential future improvements.",0,0,0,0.983226239681244,0.991644561290741,0.993175446987152,0.0,accept,unanimous_agreement
1379410975,11695,"got you. i do like the idea of lsb because in reality cursor will not take 48 or 50 bits, we would have more msb left for other potential uses in future. it gave use more flexibilities.",1,1,1,0.9684770107269288,0.962292492389679,0.9881924986839294,1.0,accept,unanimous_agreement
1379665652,11695,we had some conversation in another thread about a key handle that would cache the crc and the hash value after the initial lookup. re-using this handle would remove the extra computation. i think this is still the most promising way forward.,1,0,0,0.5593775510787964,0.9644664525985718,0.9597346782684326,0.0,accept,majority_agreement
1381397812,11695,"**updated pr with following changes:** * now using lsb for slot in the scan cursor. * caching current dictionary in the redis db to avoid crc hash calculation. * rebased on top of `unstable` and resolved some conflicts. **benchmarks** i ran some benchmarks on gets and sets the result is that this implementation and `unstable` branch look almost identical (sets are on the left, gets are on the right, branch is mentioned in the terminal prompt): ![a link] **commands that were used:** sets: `src/redis-benchmark -r 1000000000 -t set -n 100000000 -p 10` gets: `src/redis-benchmark -r 1000000000 -t get -n 10000000 -p 10` next i'm going to run more benchmarks, for other potentially affected commands as well, and will make some flamegraphs, but this looks promising.",-1,0,1,0.5891590714454651,0.9849706292152404,0.5890649557113647,,review,no_majority_disagreement
1406127392,11695,"folks, i think i've addressed all feedback, the last bit remaining is performance. i'm going to run a set of benchmarks and try to optimize any rough edges. please take another look and let me know if there are any more suggestions or concerns.",0,0,1,0.8805537819862366,0.5049099922180176,0.8324590921401978,0.0,accept,majority_agreement
1413240475,11695,"as promised before, i've done some performance analysis. **methodology** i've tested gets/sets with cluster mode yes/no, using request pipelining to make any dictionary related issues more distinctive, 50m keys and 100 bytes of data per entry using `redis-benchmark`. (see exact commands on screenshots below). tests were ran on `c5.9xlarge x86_64 linux` cloud machine. **here is a summary of results.** **cluster mode enabled** * sets are **~7-8% faster** on this branch. * gets are **~1% faster** on this branch. benchmark comparison in cluster mode: ![a link] here are flamegraphs for sets in cluster mode: this branch ![a link] unstable ![a link] looking at the flamegraph, we can see that primary reason for sets being faster is because `slottokeyaddentry` has been eliminated which was eating ~5% of performance. gets: this branch ![a link] unstable ![a link] one curious thing to note here is that `dictsdskeycompare` reduced by almost 8%, but other code paths, including `dictfind` gained slightly. need to look a bit more into this, maybe there is some potential to improve it even further. **in cluster mode disabled** * sets have about same performance. * gets are **~1-2% slower** on this branch. ![a link] looking at the flamegraphs i couldn't pinpoint the reason for slowness on get. this branch ![a link] unstable ![a link] i'm going to investigate this added slowness on get when cluster is mode disabled, meanwhile let me know if you have any thoughts or recommendations on testing process or methodology.",0,0,0,0.7847604155540466,0.980970025062561,0.9335562586784364,0.0,accept,unanimous_agreement
1413456481,11695,might have an idea about performance. do any of the benchmarks on grafana run in cluster mode?,0,0,0,0.9840800166130066,0.9920284152030944,0.9881619811058044,0.0,accept,unanimous_agreement
1415549997,11695,"hi, i quickly skimmed though the discussion (not the code, or code-review comments). i'm quite concerned about this change, here's a random set of concerns: 1. as you said, we have an impact on dbsize, which used to be very fast and maybe callers abuse it. let's see a benchmark and consider optimizing it. 2. the effects on incremental rehashing, random and probably other things we didn't realize yet need some thinking and may take time till we can fully trust this. 3. i would like to learn more about the possible negative impacts of this on non-clustered deployments. the first thing that comes to mind is memory usage, specifically on small databases. 4. i didn't yet look at the code and the attempt to avoid re-hashing or re-finding the slot (if we have a cache that cleared after each command), i worry that maybe you made some optimizations to reduce the negative impacts of this pr on some cases, but can't use these tricks for other cases and they're left unoptimized and the pr has a negative impact. e.g. multi-slot scripts, mget, sunionstore, etc. i'd note that in both 6.2 and 7.2 we added some severe performance regression for certain (even popular) workloads without realizing it, and it took some time until the community reported these regressions, so this drastic change worries me and i would want to seek more confidence.",0,0,-1,0.354159951210022,0.8712942004203796,0.9494422674179076,0.0,accept,majority_agreement
1415770526,11695,"some thoughts on points 3-4: 3: i don't think there's any memory impact in standalone mode. there's only one dict, not 16k as in cluster mode. 4: mget and sunionstore raise -crossslot if keys are of different slots, so that shoudn't be an issue. multi-slot scripts might be a problem though, if they're supported in cluster mode. i'm not sure what we allow in this area.",0,0,0,0.8801166415214539,0.9601106643676758,0.9540453553199768,0.0,accept,unanimous_agreement
1415780546,11695,"ohh. i didn't know that in non cluster mode there's one dictionary (didn't look at the code yet). so what impact does it have on non cluster mode, or any other negative impact on cluster mode?",0,0,0,0.6588507890701294,0.7532702684402466,0.8509688377380371,0.0,accept,unanimous_agreement
1415790426,11695,"for performance impact in cluster and non-cluster mode, see vitarb's comment above, the one with the flame graphs. (theoretically, there shouldn't be any impact on non-cluster. i don't know where it comes from.) for cluster mode, there is a large fixed memory impact (16k dicts), but a 16b/key save and improved latency.",0,0,0,0.9799149632453918,0.9872028231620787,0.9812730550765992,0.0,accept,unanimous_agreement
1416678563,11695,", thanks for your feedback. let me try to address some of your concerns. we should be able to get back to o(1) by simply adding a key counter at the redisdb level. incremental rehashing should not be any less efficient than it was before. we are not adding any additional memory usage per entry, as well as we won't be allocating extra dictionaries if cluster mode is disabled. can you please elaborate what exactly concerns you? it is true that slot caching optimization works only for single slot access (e.g. get/set/etc), multislot scripts would take a small hit requiring key hash calculation on db access. i will measure the impact to make it more clear if it is concerning or not, but i don't expect significant performance degradation there.",1,1,1,0.699740469455719,0.5518373847007751,0.946574091911316,1.0,accept,unanimous_agreement
1423659044,11695,"updated pr with following changes: * slot id is now used from the current client instead of a new field in `redisdb` * dbsize operation is o(1), instead of o(slot count), thanks to a new `key_count` field in the `redisdb` * used 64 bit random instead of 32 bit version to cleaner support key spaces larger than 2^32. also i've done another round of performance testing, and i don't see any regressions in cluster mode disabled anymore.",0,0,0,0.9547219276428224,0.9891467094421388,0.9456474781036376,0.0,accept,unanimous_agreement
1430070543,11695,"we reviewed the high level of the pr today with the core team. consensus was that it's a complex change, but we are still inclined to push forward for 7.2. it needs to be in rc1 since it will require the most time to bake before ga.",0,0,0,0.9117557406425476,0.990189254283905,0.9728508591651917,0.0,accept,unanimous_agreement
1449688772,11695,reviewers usually don't like force-pushes. it makes it harder to see what's new since last review. it's better to use merge commits. when the pr is ready it will be squash-merged anyways.,-1,0,0,0.7733201384544373,0.9311330914497375,0.9416013956069946,0.0,accept,majority_agreement
1459641699,11695,"please avoid force-pushes, it makes it harder to keep track of what was reviewed and what wasn't. instead, since this is gonna be squash-merged, if you wanna rebase from unstable, just use a plain merge.",0,0,0,0.9635444283485411,0.9710125923156738,0.9737668633461,0.0,accept,unanimous_agreement
1461102753,11695,"just ran a quick test, filled 1gb database with keys using benchmark (standard small values), was able to fit 22% more keys on this branch (12998272 keys) comparing to the unstable (10644416 keys). entry size (entry+key+value) went down from 88 to 72 bytes (in line with expected 16 bytes reduction per entry). substantial improvement for small key/value pairs.",0,0,0,0.9126774072647096,0.9451914429664612,0.5461055636405945,0.0,accept,unanimous_agreement
1467404704,11695,here is some data that visualizes lru eviction on this branch for random keys: ![a link] versus unstable: ![a link] i don't see any quality degradation based on the above and other tests that i ran.,-1,0,0,0.8766717314720154,0.9575509428977966,0.8599820733070374,0.0,accept,majority_agreement
1476349204,11695,where your workloads perfectly uniform?,0,0,0,0.9820138812065125,0.9812731742858888,0.9921407103538512,0.0,accept,unanimous_agreement
1476411517,11695,"random keys, so not perfectly uniform, but not artificially skewed either.",0,0,0,0.9681774377822876,0.6893794536590576,0.962061882019043,0.0,accept,unanimous_agreement
1478718305,11695,"as per recent feedback, i re-ran benchmarks, and perf results are looking good (even better than during my original run). cluster mode disabled: ![a link] ![a link] cluster mode enabled: ![a link] ![a link] also i've engineered an absolute worst case scenario for the allkeys lru, when entire db is filled with keys that are in the single slot, and then more keys are added into random slots. as expected, lru has poor performance in this case, evicting most keys from the recent batch, because random slot selection is not weighted by the slot size and is as likely to return a slot with 1 key as it is to return a slot with 1k keys. ![a link] i see a couple of ideas for how to improve fairness for large slots, let me explore them and i'll post an update.",0,0,1,0.5332515239715576,0.7423321008682251,0.8534764051437378,0.0,accept,majority_agreement
1480454348,11695,"good news, folks, i came up with an implementation that provides fair random and does it fast enough. basic idea is quite simple, we randomly select a target element from 1 to dbsize and find a slot that contains it (in a list of linearly ordered slots), main caveat is that unlike my first naive implementation that used to iterate through all slots in the dictionary (which was extremely slow), it uses specialized data structure called binary index tree (also known as fenwick tree, best explanation of this data structure can be found [a link], that gives us a way to query cumulative number of keys in a range of slots (e.g. from 0 to n) in o(log(n)) time (in our case o(log(cluster_slots)). with this capability, we can do a simple binary search and find which slot contains target-th element. total complexity of this algorithm is o(log^2(cluster_slots)). i ran a brief performance benchmark and numbers are looking good. this fixes lru fairness problem that we had before when slot sizes were uneven, here is how eviction looks now for the same scenario that degraded lru performance of the previous implementation (packing one slot to `maxmemory` and then writing into random slots with `allkeys-lru` policy). ![a link] in addition to fast and fair lru, this binary indexed tree should also allow us to remove `non_empty_dicts` intset field from the `redisdb` and implement quick iteration that would allow us to skip empty dictionaries (to find next slot, all you need is to find total number of keys up until current slot, and then find the slot where next key is, which can be done with exact same binary search as `getfairrandomdict` uses in log time).",1,1,1,0.966874361038208,0.9766311049461364,0.9850008487701416,1.0,accept,unanimous_agreement
1487683828,11695,"with random fairness issue out of the way, i think this change is ready to go, is there anything else you want me to address before we get final approvals and merge it in?",0,1,0,0.9715421795845032,0.7978190779685974,0.8672723174095154,0.0,accept,majority_agreement
1495089198,11695,pinging you to still take a look if you can.,0,0,0,0.97540944814682,0.915463387966156,0.938133955001831,0.0,accept,unanimous_agreement
1498548988,11695,all your previous comments should be addressed at this point.,0,0,0,0.9808040261268616,0.988226592540741,0.9938210844993592,0.0,accept,unanimous_agreement
1501063389,11695,"lgtm, this is a significant improvement overall. i still think this change should be parked in unstable for some time before being released. one minor comment about scan api: so far, the cursor was considered opaque (with an exception of zero value), and i think we should officially keep it so.",0,0,1,0.8538121581077576,0.6004367470741272,0.9557509422302246,0.0,accept,majority_agreement
1503565310,11695,"decision was to move this to merge into redis 8, so we will plan to merge this in right after we have stabilized the redis 7.2 release.",0,0,0,0.9861282110214232,0.9930344820022584,0.99243825674057,0.0,accept,unanimous_agreement
1550187420,11695,"in the previous core group we also decided that we wanted to see if we could make the implementation cleaner, and i spent some time reviewing it again and didn't come up with much. pinging you again to see if there is anything specifically you wish was better integrated.",0,0,0,0.972396194934845,0.9724307656288148,0.9822738766670228,0.0,accept,unanimous_agreement
1643311555,11695,merged latest changes from unstable.,0,0,0,0.978120744228363,0.9682998061180116,0.9919604659080504,0.0,accept,unanimous_agreement
1708675329,11695,i thought the reviewer resolves it if they find the changes fine. anyway it's not allowing me to mark the comments as resolved any longer. i've marked with a thumbs up for the one's i've addressed.,0,-1,0,0.9363431334495544,0.58243727684021,0.963697612285614,0.0,accept,majority_agreement
1709544673,11695,"i don't think it matters who resolves them as long as they're resolved when each one is addressed or the discussion ended (no further arguments). i have a limited attention span, so since you're going over them one by one, i'd rather you keep them organized, but anyway if you can't then i just went over them and resolved the ones you marked.",0,0,0,0.7818586230278015,0.9410814642906188,0.9519487023353576,0.0,accept,unanimous_agreement
1741712813,11695,this took longer than expected. / if you could take a look again that would be great. thanks :folded_hands:,1,1,1,0.9368343949317932,0.9897035956382751,0.995971143245697,1.0,accept,unanimous_agreement
1741713587,11695,would we be able to run the suite of benchmark on this change. that would be really helpful. /i will also be posting some of the benchmark results we've ran early next week.,1,1,1,0.7638969421386719,0.9831420183181764,0.8996239900588989,1.0,accept,unanimous_agreement
1745496137,11695,i tried to find all of the major pending items and put them in a section at the top. (or at least those without a very recent comment),0,0,0,0.9734052419662476,0.9880619049072266,0.9901717305183412,0.0,accept,unanimous_agreement
1745603500,11695,"i'm unable to modify the top comment as i'm neither the original author nor maintainer. thanks for summarizing it above. * renamed the `dictslots`/`dbslots` to `dictbuckets`/`dbbuckets`. * introduced defragctx to pass on db/slot to defragkey and avoid recompute of slot info from key. * remove unnecessary bitwise and operation while retrieving cursor. * updated test for data verification after cross slot write operation from lua scripts/functions. [madelyn's list of pendings items copied from the top level comment] - [x] : for scripts and module commands, we need to be invalidating the client sends multiple sub-commands across multiple slots that are local to the node. the original script code was just designed for tracking invalid uses, and is not always set correctly. modules has no such tracking. both modules and scripts can access cross slots within their execution. (ref: [a link] we are missing tests here too. es#r1226137086 seems to have been forgotten. - [x] : defrag code generally seems unoptimized for slot tracking. (ref [a link] is this something we think we need to optimize. (assuming yes) - addressed this particular feedback however the time spent is still higher compared to standalone setup. - [x] : defrag later step could avoid slot recomputation if done serially. (ref [a link] - [x] : fix the naming of dictslots to be dictbuckets (ref [a link] either should be a separate commit or a followup pr. - [x] : cross slot operations from scripts (ref [a link] updated some test for validating data integrity with cross slot operations via scripts/functions in `tests/unit/cluster/scripting.tcl`.",0,0,1,0.5763750076293945,0.7411189675331116,0.9570293426513672,0.0,accept,majority_agreement
1745872485,11695,i didn't follow this. is it a possibility we've discussed somewhere or in general we want to improve the perf ?,0,0,0,0.7752640843391418,0.9015682339668274,0.9715102314949036,0.0,accept,unanimous_agreement
1746137883,11695,"right, i forgot, let me move the checklist down to your comment.",0,0,0,0.9676560163497924,0.983626425266266,0.9820755124092102,0.0,accept,unanimous_agreement
1746152545,11695,"it's somewhere in this thread. i think the idea was that instead of repeatedly calling `cumulativekeycountread` to do a binary search, we can instead do an efficient traversal by starting at the root and doing the following: 1. scan the children from largest to smallest until we've found a child node that is smaller than the target. 2. subtract the value of that child node from the target, and return to 1) but operating on that found child node. the result should give you the same target slot as the current implementation and in log(n) time.",0,0,0,0.9771191477775574,0.9810836911201476,0.9863253235816956,0.0,accept,unanimous_agreement
1749300162,11695,"i have done some redis benchmark tests and performance analysis. here are the results: **methodology** methodology is similar to what it was done before [a link]: gets/sets with cluster mode yes/no, additionally also for sets under the load of evictions , 50m keys and 100 bytes of data per entry using redis-benchmark, exact commands on screenshots below. tests were ran on `c5.12xlarge` x86_64 linux cloud machine. for testing under the load of evictions the `maxmemory` was set to `100mb`. cme - cluster mode enabled cmd - cluster mode disabled (standalone) **results.** **cluster mode enabled**: - sets are ~5% faster on this branch. - gets are ~4-5% faster on this branch. ![a link] flamegraphs for sets in cluster mode: this branch: ![a link] unstable: ![a link] flamegraphs for gets in cluster mode: this branch: ![a link] unstable: ![a link] **cluster mode enabled - with eviction**: - sets are ~2-3% faster on this branch. ![a link] flamegraphs for sets in cluster mode - with eviction: this branch: ![a link] unstable: ![a link] **cluster mode disabled**: - sets are ~1-2% faster on this branch. - gets have a very similar performance ~0.5% faster on this branch. ![a link] flamegraphs for sets in cluster disabled: this branch: ![a link] unstable: ![a link] flamegraphs for gets in cluster disabled: this branch: ![a link] unstable: ![a link] **cluster mode disabled - with eviction**: - sets are similar on this branch. ![a link] flamegraphs for sets in cluster disabled - with eviction: this branch: ![a link] unstable: ![a link] let me know if you have any questions or any suggestions/recommendation over the method of carrying out performance benchmark.",0,0,0,0.939392387866974,0.9848414659500122,0.6618648767471313,0.0,accept,unanimous_agreement
1749313581,11695,did we ran the test multiple times? there could be also some deviation across runs. ideally standalone (cluster-enabled false) shouldn't have any performance variance from unstable.,0,0,0,0.9831490516662598,0.9855389595031738,0.9886544942855836,0.0,accept,unanimous_agreement
1749316823,11695,"yes, but i can run it few more times to confirm. edited the above comment with the results",0,0,0,0.983000636100769,0.9832822680473328,0.9895288348197936,0.0,accept,unanimous_agreement
1749319023,11695,there are possibly two minor optimizations (see [a link] left out which i'm not sure if there is a lot of major gain. could we iron out if there is anything else left to be addressed or if it's in a good state to be merged ?,0,0,0,0.9786540865898132,0.9615871906280518,0.984014332294464,0.0,accept,unanimous_agreement
1749828644,11695,"gets have a very similar performance ~0.5% faster on this branch. i don't get this. we expect no performance change. i suppose ultimately it's okay though, since it's just net faster. i'm okay creating follow up for those two things. they don't seem essential to the implementation and we are still seeing a new improvement without them.",0,1,0,0.749377965927124,0.5395101308822632,0.8401834964752197,0.0,accept,majority_agreement
1750075805,11695,"those are minor deviation (+/-) seen across runs. i wouldn't read much into it. overall, it's similar in performance to unstable with 16 bytes of memory saving per key.",0,0,0,0.9750467538833618,0.8873703479766846,0.957194983959198,0.0,accept,unanimous_agreement
1756822198,11695,"i believe this got translated into the pending item about evaluating `multi performance`. i left a comment that we can benchmark and evaluate it independently. i think it would reduce readability too much and it's not clear it would have a major benefit, but it would be easier to benchmark it independently. [code block]",0,0,0,0.9241841435432434,0.978101909160614,0.9860306978225708,0.0,accept,unanimous_agreement
1756834647,11695,"i ran some tests for benchmarking the performance of mset in cluster enabled. here i executed 1m mset commands to fill data across different slot on each mset command, 10 keys at a time. the script that i ran: [code block] for unstable i got these results: `cmdstat_mset:calls=1000000,usec=13701672,usec_per_call=13.70,rejected_calls=0,failed_calls=0` `cmdstat_mset:calls=1000000,usec=13555544,usec_per_call=13.56,rejected_calls=0,failed_calls=0` for this pr: `cmdstat_mset:calls=1000000,usec=10463197,usec_per_call=10.46,rejected_calls=0,failed_calls=0` `cmdstat_mset:calls=1000000,usec=10466570,usec_per_call=10.47,rejected_calls=0,failed_calls=0` its ~20% faster performance of mset for the pr as compared to current unstable.",0,0,0,0.966069221496582,0.9804701209068298,0.9623537063598632,0.0,accept,unanimous_agreement
1756838681,11695,this seems compelling to me such that at least right now i don't think we need to pursue more optimizations.,0,0,0,0.915089190006256,0.8790265321731567,0.9771862030029296,0.0,accept,unanimous_agreement
1757559694,11695,"ok, so i'd ask to add mset and fix the link: :smile:",1,1,1,0.9792530536651612,0.9887545108795166,0.9913340210914612,1.0,accept,unanimous_agreement
1757563906,11695,"i think my request was to check how much more such an optimization can help (update the bit only once), not to fix any regression this pr has. i agree it's not a blocker, and should be left for later.",0,0,0,0.9607162475585938,0.9077540636062622,0.9732229709625244,0.0,accept,unanimous_agreement
1758051123,11695,/ i've updated the tracking comment to further look into mset performance improvement independently. [a link],0,0,0,0.98111492395401,0.9717490077018738,0.992973804473877,0.0,accept,unanimous_agreement
1758566426,11695,"from my perspective the only thing that seems pending is your thread [a link] i'm not familiar enough with the code, so assuming we can merge it once we have a decision there.",0,0,0,0.7837371826171875,0.9003891348838806,0.9855504035949708,0.0,accept,unanimous_agreement
1758567295,11695,"we reviewed this in the core team, and no-one else wanted to take a look. so once all outstanding comments have been addressed this should be good to merge.",0,0,0,0.5608912110328674,0.7467262744903564,0.9398524761199952,0.0,accept,unanimous_agreement
1759098441,11695,"the defrag discussion is resolved, from my perspective we can proceed. please try to do a quick skim through the code, and make sure the top comment (and quash-merge commit comment) has all the details it should have. i.e. the justification for the change, what was changed, any specific areas that should be described in more detail, and any other side effects. thanks.",1,1,1,0.8974533081054688,0.9552772641181946,0.954534649848938,1.0,accept,unanimous_agreement
1763206897,11695,full test run [a link],0,0,0,0.9862272143363952,0.9024476408958436,0.9940800070762634,0.0,accept,unanimous_agreement
1763253811,11695,new test run: [a link],0,0,0,0.9872499108314514,0.9780752658843994,0.9945346117019652,0.0,accept,unanimous_agreement
1763298243,11695,"the new memory efficiency test does seem a bit flakey (see [a link] valgrind makes it worse. didn't seem worth continuing to iterate here, but let's try to make it more stable.",-1,-1,0,0.6674148440361023,0.876271665096283,0.971333146095276,-1.0,accept,majority_agreement
1763480051,11695,"it's merged! thank you all for helping push this through! i think it's a very good improvement to redis cluster. regarding the follow up item mentioned in the top comment... didn't we end up storing the slot in the 14 lsb of the cursor, meaning an old cursor can map to any slot, not just slot 0?",1,1,1,0.9906882047653198,0.9956331849098206,0.9969229102134703,1.0,accept,unanimous_agreement
1765298314,11695,"yes, i think the `important changes` section has the correct statement. we could remove this.",0,0,0,0.9879315495491028,0.9825093746185304,0.9863075613975524,0.0,accept,unanimous_agreement
1765980175,11695,"i see that in addition to the defrag test, the `expire scan should skip dictionaries with lot's of empty buckets` test is also unstable. can you look into these and make a pr to fix them?",0,0,0,0.9802733659744264,0.929746389389038,0.991460919380188,0.0,accept,unanimous_agreement
970329168,9788,"just out of curiosity, does this create a foundation to extend [a link] to aof-only and not only rdb-only setups? thank you",1,1,1,0.9268375039100648,0.8719361424446106,0.9165900945663452,1.0,accept,unanimous_agreement
971143029,9788,"sure, you can see some followup pr plans mentioned by oranagra [a link].",0,0,0,0.9779047966003418,0.9801079630851746,0.9935380816459656,0.0,accept,unanimous_agreement
971228464,9788,"i've just created a few new issues to track these ideas (don't like them sitting around in a comment in a closed pr) i moved one note of that post to a todo bullet in the top comment of this pr. the first one of the other issues should be done for 7.0 imho: * [a link] the other ones might not make it for this release, but we can at least start discussion and mark them for next release: * [a link] * [a link]",-1,0,0,0.5983340740203857,0.9426525235176086,0.8570597767829895,0.0,accept,majority_agreement
971323580,9788,"about upgrades from old redis versions, i think we can't directly rename the old aof to the new name, because we still can't solve the atomic problem of rename and update manifest. we can write the old aof name directly into the manifest, so that we can start loading normally. even if the process crashes after writing the manifest, that is no problem, because we can find the aof from the manifest. once aofrw is executed once, all files will have new file names. (current code is implemented according to this idea)",0,0,0,0.976872444152832,0.9814181923866272,0.97396719455719,0.0,accept,unanimous_agreement
971328700,9788,"ok, sounds good to me. so the aof file (possibly with preamble content) will have it's old name, and will be listed in the manifest. once a rewrite happens it'll be tagged as history and later deleted. let's be sure to add a test for it (storing an old aof preamble file in the assets folder)",1,1,0,0.9592888951301576,0.7200866937637329,0.8512277603149414,1.0,accept,majority_agreement
979149935,9788,"please avoid force-push (`git commit --amend` and `git rebase`), it make it harder for me to track the changes. we're gonna squash merge the pr eventually, so it doesn't matter if it has ton of incremental changes.",0,0,0,0.9606105089187622,0.9729308485984802,0.9488558769226074,0.0,accept,unanimous_agreement
979171438,9788,"sorry for that... today i encountered a merge conflict, so i rebase directly...",-1,-1,-1,0.9900942444801332,0.9933386445045472,0.9943723678588868,-1.0,accept,unanimous_agreement
981725242,9788,"regarding yossi's proposal for using a folder with the name specified in `appendfilename`, i support that. i think it'll make things easier, specifically maybe to detect if we're upgrading from an earlier deployment. it does cause some inconvenience on startup, since we'll need to [create a folder, create manifest, move a file], and maybe handle power failures during this sequence, but i think it worth it. i wanna mention a few different cases for upgrade: 1. a deployment which has a modified redis.conf and an aof file on the disk (e.g. named ""mydb.aof"") 2. a deployment that uses a default config that's taken from the distro, and an aof file on the disk named (e.g. named ""appendonly.aof"") in that light, **i think my previous advise to change the default value for that config, was probably wrong**. i.e. in case 2, we may be looking for the wrong file, since the config file has changed, but the existing file on the disk was not. one more note however, i think we may still want to apply some user configured prefix on the files themselves, not just rely on the folder name, so that if a user decides to copy them somewhere without the folder he'll still know who they belong to. i.e. imagine the folders are named `redis-1`, `redis-2`, etc. it might be nice that the files themselves are also prefixed with that. but on the other hand, if by default we keep the name `appendonly.aof`, then it'll look odd that the files have that prefix.",1,0,1,0.6771135330200195,0.9098302125930786,0.7504949569702148,1.0,accept,majority_agreement
982233425,9788,"regarding `appendfilename` as a directory, it is indeed convenient for users to copy and backup aof files, which i also agree with. but i have a problem: if the current user has a deployment and uses the default configuration (`appendfilename` is `appendonly.aof`), then there will be a file `appendonly.aof` in the dir directory. if the user starts with the binary `redis-server` of the multi part aof version, what do we need to do? first create a directory named `appendonly.aof`? this obviously doesn't work, because it conflicts with the `appendonly.aof` file. create the directory under another name or rename `appendonly.aof` to another temporary name first? this does not work either, we may lose this file forever. [code block] so, do you have any more detailed suggestions? in addition, i have another idea. regarding backup and copying, manifest is the best reference. can we provide a tool binary `redis-package-aof` (similar to `redis-check-aof` and `redis-check-rdb`), which receives the manifest file name and target directory name, and according to the instructions of the manifest, all aof files (including manifest itself) are packaged in a target directory. in addition, regarding the issue that the files we generate may conflict with the user's scripts or tool files, i don't mind that we can add timestamp information to our file names. i think this is the limit we can do. [code block] and we have realized that we cannot change the default configuration of `appendfilename` (it will always be `appendonly.aof`) . do we need to reconsider the following naming method, intuitively i think it looks better. [code block] or even a more simplified version: [code block]",0,0,0,0.857279360294342,0.8901585936546326,0.9533438682556152,0.0,accept,unanimous_agreement
982743389,9788,"i have a feeling that we may be able to make it work by something like this: 1. create a temp dir named "" .tmp"" 2. create a manifest in that dir (points to a yet non-existing file) 3. move the original aof file into that dir 4. rename the dir. the only problematic point of failure is if there was a crash between 3 and 4, in which case when we recover from that crash we can look for "" .tmp"" and resume that operation. another idea is to decide from the get go that the dir is always named "" .files"" or alike. i.e. everything we create is based on that base file name, but always with some suffix. and then the upgrade procedure only has one rename in it and not two. regarding the `weird` thing of having rdb files with an intermediate `aof` suffix (i.e. `appendonly.aof.1.rdb`), i don't like it, but i'm wiling to accept it. i don't like the timestamps in the file name though. regarding tooling, that's a nice idea to add tools, but i feel they should not be binary tools, but rather scripts. the advantage is that they're maybe easier for users to adjust or mimic to create something similar of their own. i.e. as long as we don't need to parse binary files (e.g. rdb.c) or match things against a complicated format and maybe the redis command table (aof.c), maybe we better not write it in c?",0,0,0,0.6311602592468262,0.8734095692634583,0.855501651763916,0.0,accept,unanimous_agreement
983142615,9788,"the method you mentioned seems to work, but there are many problems. remember the redo solution we discussed earlier? the idea you mentioned involves `createdir`, `movefile`, and `renamedir`. if any one of them fails, we may be in an intermediate state. if there is no redo, redis will start complex and ugly if-else judgments, and this is not completely to solve the problem, we have been demonstrating this for a long time before. the lesson we learned is not to modify the original aof file (even move or rename) before ensuring that the user’s data has been loaded correctly. let’s review the current implementation: [code block] as you can see, there is no modification to the user's original file, and the user does not even feel the change (the difference is that there are a few more files, that is, the manifest file and an incr aof file), even if he wants to use the old version of redis now(rollback), it’s okay, because the original aof is still there. the base file will also use our new naming rule after aofrw. so i think the current upgrade method is fine. secondly, why do we want to do this, are we just to put all aof files into a dir for easy copying and backup? what i want to say is that in 99% of the cases, we will only have three aof-related files: `appendonly.aof.manifest`, `appendonly.aof_1.rdb`, `appendonly.aof_1.aof` (it's just two more files than only `appendonly.aof` before), and they all have a common prefix, i think anyone knows how to quickly copy these files: `cp appendonly.aof* targetdir`. in addition, for the remaining 1% case, frequent failures of aofrw resulted in a lot of incr aofs (fortunately we have aofrw limit, so it won't be too many). like the above, they still have a common prefix, and regular copy can still work. even, we can provide an aof tool (indeed, scripting is enough). we can provide the following convenient functions: [code block] regarding the issue of file name conflicts, i think this probability exists but is not critical. now `bgaofrewrite` will generate a file name similar to `temp-rewriteaof-bg-pid.aof`. this file name changes every time. who can guarantee that it will not conflict with other file names in the dir directory (yes, from the probability it exists). but i think that an online business that really uses redis will not name their tools or scripts that way. and under normal circumstances, users do not put their scripts in the working directory of redis (remember that `dir` is redis's own working directory), they usually put them in their working directory. therefore, this should not be the reason why we forcibly put aofs into a directory. wdyt?looking forward to your feedback.",0,-1,0,0.861526370048523,0.5249400734901428,0.949767768383026,0.0,accept,majority_agreement
983373880,9788,"i don't think it'll be that bad. looking at my 1..4, i think in all cases except 3, we can just restart the process from scratch, and the only one that needs special handling is a crash between 3. and 4. yes, but: 1. that's exactly what yossi was concerned about... that an old backup script will keep working since the file is still there, and it'll take time till the user realizes his script is broken. 2. after startup we'll start writing new commands to that old aof file, these commands could be ones that didn't exist in the previous versions, so the user is not allowed to downgrade anyway. (p.s. even if we start a new part at startup, that's even worse, since a downgrade will only look at ""part 1""). so anyway, downgrade is out of the scope, it's not an argument for this discussion. we don't need to design things according to 99% of the cases, that's exactly what could lead users to write a script that backs up only 3 files, and be broken on the 1% (without user ever realizing it before it's too late). i agree that if we don't change or mess with the default of `appendfilename` in any way, then `cp appendonly.aof* targetdir` is sufficient. but then i breaks yossi's concern about backup script after upgrade failing silently. so we're just trying to find a something better. these two concerns are actually concerns are actually contradicting each other (one require no rename, and the other requires not to use the old name), so we must decide which one is more important.",0,0,0,0.513941764831543,0.7011285424232483,0.6330769658088684,0.0,accept,unanimous_agreement
983403537,9788,"yes, i very much agree that we should work hard for that 1%, which is why we have been discussing here, otherwise things will be very simple. regarding the problem you mentioned (the backup script found that they were backing up a wrong aof for a long time), i think this is easy to solve. once we find that we are starting from an old redis when we start, we can do aofrw immediately after loading this aof (this is easy to do: `server.aof_rewrite_scheduled = 1`), so that the old aof will become our new naming rule in a short time.",0,0,0,0.6356621980667114,0.7735698819160461,0.7025434970855713,0.0,accept,unanimous_agreement
983410653,9788,"this will come at a cost... (time, cpu, latency, cow). yossi suggested to create use hard links, which i didn't like since i'm afraid of unexpected platform specific issues, and for users to accidentally copy both files. but maybe it's not that bad to only temporarily use them for a short period? i.e. what you're suggesting about an aofrw at upgrade / startup, is in some way similar to making a temporary hard link, and then delete the original one (i.e. you create another copy of the data, so you can later delete the old one).",0,-1,-1,0.7023100852966309,0.6865188479423523,0.990298867225647,-1.0,accept,majority_agreement
983495609,9788,"we all know that upgrading redis online does not directly restart the master, but first upgrade the slave and do ha again, and then upgrade the previous master. therefore, i think that the aofrw after loading will not affect the real online requests. at the same time, what we are doing now (multi part aof) is to reduce the overhead caused by aofrw. isn't it ? if the user directly stops the service and restarts the master, then the overhead caused by aofrw does not need to be concerned. moreover, this upgrade is a one-time thing, i don't think we need to introduce such a complicated logic for this one-time thing (compared to the current implementation). below is a piece of pseudo-code i tried to understand and write, we can compare it(if redis crashes after the 3 step, the user will find that his original aof file is gone (actually it was moved to .tmp)): [code block] below is the current implementation: [code block] finally, i want to declare that i don't care about writing a few more lines of code, but once this code is added, it will always remain in the redis code (because the version upgrade is a long-term thing). but the upgrade operation is an instant and one-time thing (we do not support rollback), at least for now, i think the problems you mentioned can be solved at present, unless there is something that cannot be solved now. i like this discussion very much. it makes our views on things clearer. i look forward to your feedback, thank you",0,0,0,0.8654512763023376,0.9739869832992554,0.9580489993095398,0.0,accept,unanimous_agreement
983622957,9788,"i think your pseudo code can be simplified, we don't need explicit code to re-do steps 2,3,4. just delete the tmp dir and let the process start again normally (i.e. fall back to the code below). the only part that needs special handling is in case the temp dir exists, and the aof file is already in it. i think that upgrading a standalone server (no replication) is a common thing. introducing an additional aofrw will also add some complexity (and a special state, and testing), and considering the performance overhead, i personally rather have a few lines of code and comments with fast execution than a long rewrite process. maybe the hard link solution is a valid one too (will make the recovery simpler, but it's harder for me to see where it could backfire). anyway, i think this is a viable solution to upgrade if we choose using a folder, but iirc there are other concerns about this approach (like having a folder named `appendonly.aof` or maybe `redis-13.aof`), and possibly the files in it will also have the prefix (e.g. `redis-13.aof.incr.1234.aof`). or other concerns i already forgot?",0,0,0,0.9454033970832824,0.9517988562583924,0.9003223180770874,0.0,accept,unanimous_agreement
983643111,9788,"as i said earlier, to upgrade an standaone server, the user can only restart the `redis-server` in place. does he still care about the cost of aofrw? no request to come in at this time. aofrw is only triggered when upgrading from the old version redis, not after every loading, it will not bring any complexity (including test), you can see my latest commit. i really haven't found any benefits (just pack the aof file in advance) that using a directory can bring us, and what are the problems with the current solution. regarding the issue of backup, i suddenly remembered that we all made a mistake in order. normally, users will not upgrade the redis-server first without thinking of upgrading its backup script in advance, which is difficult to understand(instead of waiting here for the backup script to report an error to know that we have upgraded the redis-server). i believe that redis 7.0 will definitely introduce some other major features, which must be prominently reminded in the release notes and let users know. we introduce the multi part aof mechanism, and the surrounding operation and maintenance tools will definitely need to be changed and adapted. this is unavoidable. in addition, if we use `appendonly.aof` as the directory name, we don't need to use it as the prefix of the aof file, otherwise it will be a very redundant design(let us free ourselves from this annoying naming convention). ithink it should look like this: [code block] the directory name will be used to distinguish different aof sets. as for how to distinguish aof files when backing up, i think this is a matter of the backup script, not a matter that redis should care about. the backup script should directly type the entire directory into a zip or other format compressed package, so that it will truly become a file, and subsequent backups and downloads will use this file, so there will be no question of who belongs to whom. i think we have been discussing this issue for a long time, but i feel that we are now thinking about the design of the solution for the 1% possibility (the backup script is not adapted in advance). i think my point of view has been expressed very clearly, at least so far i still think that there is no need to introduce so much code in order to upgrade from the old version redis, which just one-time thing. but if we have been holding our own opinions, this pr will be blocked for a long time (i think this is the last question of this pr, right? exclude tests). so, i think this decision is left to your core team. if you all still insist on doing this, then i will start to change the code.",0,0,0,0.5789710283279419,0.9586944580078124,0.9550176858901978,0.0,accept,unanimous_agreement
987724649,9788,"we discussed this in a core-team meeting. we decided to proceed with keeping the old config and it's default, and creating a folder with that name. some of the reasons: * we want to implicitly handle upgrades properly for the wide range of ways redis is used (without failing on startup, or doing something that will result in data loss) * we want it to be very easy for users to realize which files needs to be copied or managed * we want old scripts that look for the old file to either keep working, or immediately fail * we're ok to have a few extra lines of code to handle complicated upgrade, or even fail and require manual intervention in the very rare case of a crash during the upgrade process.",0,0,0,0.7766172289848328,0.973042130470276,0.9427616000175476,0.0,accept,unanimous_agreement
988472067,9788,"i don't think we reached an agreement as you described above. using the old config `appendfilename` to create a folder is ambiguous and hard to understand, and maybe we have to spend more time and words in release note to explain the odd design to most users. you give me an example that maybe some users manage some redis instances in same `dir` but using different `appendfilename` to distinguish their data. and then after upgrade to redis 7, it may lead to clash because of the new version have same default manifest and other file name. but i want to say these users are so smart they give redis instances different `appendfilename`, i believe they could be smart again to give redis 7 different `dir`. and upgrading needs restart redis, it's easy to modify config file, i don't think it's a big work. and actually our control plane system is doing that everyday, upgrade old version to new version (we already implement the multi part aof in the manifest way). btw, some other popular oss project like rocksdb manages its files like that, i think it's easy to understand and maintain.",0,0,0,0.8417713046073914,0.6631852388381958,0.7428888082504272,0.0,accept,unanimous_agreement
988510692,9788,"well, it seems we still haven't reached an agreement. things have progressed here, i think this is beyond the ability of this pr. i am curious, when we are designing, we should give priority to satisfying most common scenarios, or should satisfy some very marginalized scenarios. i believe most people will choose the former, because there is no one thing in the world that can be good at everything, and `redis` is the same. for example, `redis` is an in-memory database, but many users use it as a persistent database. we cannot change `redis` to synchronous replication and synchronous disk writing (or add lsm tree like rocksdb) just because of these users. i think this is the design principle of `redis`. you have been opposed to my current implementation (i think it is simple and effective), because many users will deploy many `redis` on one machine, or they all use the same `dir`. first of all, i think this usage is surprising, i don't know if you have seen such users in reality. assuming they do exist, it must be the edge scene i mentioned above. think about it, all `redis` running on one host (and possibly using the same `dir`) itself is a very big security risk. do they still care about the overhead caused by aofrw when `redis` is upgraded? i don't think it will. there is also the problem of the backup script you mentioned. this is even more puzzling. don't users know that they want to upgrade `redis 7.0`, and upgrade or stop its backup script in advance? if we always use the 1% edge scene to guide our design, then we will lose 99% of the user experience. in private, i have implemented the code that uses `appendfilename` as a directory, but i think it is ugly for the following reasons: 1. `appendfilename` configuration is literally a file name, but in fact it is indeed a directory name, which is very confusing 2. compared with the flat directory structure with only `dir`, the two-layer directory structure will cause many path joins in the `redis` code and tests. 3. a directory named `appendonly.aof` looks very strange current implementation (please allow me to list it again for comparison): 1. `appendfilename` as base name, name rules: `appendonly.aof_manifest`、`appendonly.aof_1.base`、`appendonly.aof_1.incr`, i think it's simple and beautiful 2. can smoothly support users to upgrade from the old version `redis`, user files will not be tampered with during the upgrade process, any crashes can be recovered again 3. the code is clearer and more concise, as you can see regarding what you mentioned: this completely misunderstood my implementation, even if all files are in the same `dir` directory, they will be distinguished by `appendfilename` prefix,declare the naming rules again: `appendfilename_seq.type` for unreasonable `redis` usage, we should try our best to guide them to the correct method, rather than cater to them. at the same time, in terms of functions, we still support this unreasonable usage, but these users should stop complaining about why the cpu of the machine is so high, the reason is because he put all the `redis` on the same machine (when `redis` is doing aofrw at the same time, which doesn't just happen when upgrade)",0,-1,0,0.588965654373169,0.6438407301902771,0.5284698009490967,0.0,accept,majority_agreement
988594773,9788,"i think the same can be said about using this as a prefix (it's hard to understand and we'll have to explain it, i.e. the prefix part). that whole change about multiple files and a manifest is hard to explain. maybe using a folder will make it easier to explain, i.e. we don't need to give detail about the individual files suffixes. it's right that the config name is odd since it has the word `file` in it. had it been just `appendname` or `aof_name` it might look a bit cleaner. my thinking that users might have been smart or not when the initially created some configuration that worked for them, and since then they moved away and forgot what they did.. now they might just be updating their binary to get fixes and improvements, without re-thinking their configuration, and i don't want it to break. some users have fancy control plain and modify it when integrating new version (have an r&d department), others may have something simpler, but still test it in a staging environment before upgrading their production environment. some may just place the new binary and restart their production environment, or type `apt-get upgrade`. some may not even be aware they have redis, they use some other tool that relies on redis internally. my point is, i imagine redis is used in a very wide array of configurations, as we've seen by the popular abuse of `protected-mode`, initially salvatore didn't want to create that feature, [a link] that uses should know that they're doing, later it became a burden telling people they're doing the wrong thing, and he added this feature, but today we see that even that wasn't enough... some people just disable it without reading, and then open an [a link] here to ask why they lost their data (without realizing they were also hacked). i don't see the security risk, and yes, i do think the excessive aofrw can cause an overhead, and even oom kills due to cow. yes, i think many who upgrade don't even realize they're doing an upgrade, and wont read the release notes. they have something they've set up in the past, it works for them, and they just replace the binary or do `apt-get upgrade` without reading the release notes. we should aim for the 99%, but even for the 1% we don't want to break them too hard, or inflict data loss or security issues on people who are careless or have odd configurations. i can understand why you think flat is elegant, and folder name i'm proposing is odd (due to the name of the config and the name of the folder, and due to the complications with rename on upgrades). but i think you also have to admit that having all files related to a certain mechanism sit in a folder is also elegant and less confusing. i.e. imaging we didn't have any backwards compatibility concerns. if we would be creating redis 1.0 today. wouldn't it be cleaner to put all these aof related files in a folder? isn't it easier for the user to realize that's where all the content for the aof mechanism sits, instead of having to pick files scattered in another folder and mixed with other files? mentioned rocksdb and other systems, rocksdb indeed creates many files of various formats and puts them in that folder, but iirc it isn't mixing other files in there (like config?), it's just data. and you have to admit that it is better to put files serving a certain purpose (when you have more than one) in a folder. for example a navigation app, will store the maps in one folder and the voice recordings in another. same as we store the tests separated from the code and the docs. i think storing the aof related files in a folder will means our documentation can be simpler! (we don't have to explain the individual file names / purposes and structure), and considering users are likely to not read the docs anyway, and just maybe write a backup script backing up what they empirically see, they might write a script that copies the 3 files they normally see, and completely miss the rare cases where we create a 4th file. putting it in a folder makes it easier to understand without reading anything. this example was about a discussion on a scenario in which we **do** create a folder, but use a new config to control its name (and deprecate the old config), in which case, users who just upgrade the binary of redis without changing their config files, will use the default name of the folder, and the files will clash. another proposal that was made in discussion was that if we find that the user didn't update his config file, we'll fail to start, demanding that the user reads the docs and updates the config properly. i don't like that approach at all.",0,0,0,0.9459456205368042,0.8814820647239685,0.8836140632629395,0.0,accept,unanimous_agreement
988599827,9788,"i have no words about this scenario, schrodinger's user?",0,0,-1,0.5029967427253723,0.5569947957992554,0.9585578441619872,0.0,accept,majority_agreement
992736972,9788,"one use case to consider, that was not brought up, is redis that is deployed and managed by an upstream linux distro (as a deb/rpm). typically, these package systems will handle replacement of binaries and restarting of services easily, and may even replace a config file if it was not modified. however, if a config file is modified then it may or may not be replaced - depending on user choice or system settings. i realize you have a very firm opinion about the design details, but are we in agreement on the goals we try to achieve? to me, the *must-have* goals are: * upgrade from previous versions should be as simple as replacing the binary and restarting, no additional / manual steps involved * redis 7 should behave in a reasonable way when loaded with a config file of a previous version * we should minimize the cost of backwards compatibility (i.e. avoid redundant code, duplicate mechanism, ugly config in the future due to the past) * we must minimize the chances for data loss due to users not being aware that aof data is no longer stored in just `appendfilename` before diving into the design details, do you have any thoughts about these goals?",0,0,0,0.6732313632965088,0.9374249577522278,0.9785603880882264,0.0,accept,unanimous_agreement
992801279,9788,"i agree with these goals, the first two you mentioned are also ""mast-haves"" in my eyes too. the other two are prefixed by ""minimize"" so we can always argue about how badly these goals are compromised and their priority, so maybe they're not mast-haves. in that case, here are other (non-must-have) goals: * we wish the new behavior (file names) to make sense (i.e. we don't want new deployments to look awkward). * we wish to make it easy for users to conclude where their data is located and easy to copy it (probably falls into your last bullet, but i think you meant existing deployments in yours).",0,0,0,0.7594523429870605,0.948420226573944,0.8033790588378906,0.0,accept,unanimous_agreement
993021347,9788,"i also agree with this goal. i also know that putting aof files in a separate folder is easier to manage and copy. my only entanglement is to use `appendfilename` as a directory name (frankly speaking, i don’t recommend adding a new configuration such as `appenddirname`. maybe this will increase the burden of configuration compatibility when upgrading). so, i want to summarize the things we need to agree on now: 1. whether to put aofs and manifest in a separate directory? 2. whether to use `appendfilename` as the directory name? 3. do we need to use `appendfilename` as the prefix of aof and manifest file names? (such as: `appendonly.aof_manifest、appendonly.aof_1.base、appendonly.aof_1.incr`) 4. do we need to use the encoding format (aof or rdb) as the suffix of the file name? (such as: `appendonly.aof_manifest、appendonly.aof_1.base.rdb、appendonly.aof_1.incr.aof`) or do you have any other specific things to add, i hope we can reach an agreement on the above issues asap.",0,0,0,0.7749007940292358,0.9004279375076294,0.8948342204093933,0.0,accept,unanimous_agreement
993155026,9788,"trying to catch up on this thread, sorry if i say something addressed earlier, but there is a lot to read. i also agree with those goals. as a new user to redis, i would probably prefer to store all my data in a directory. motivation being it's easier for my to tar and upload it, which is what i do with redis aof. the esoteric case of what if someone is running multiple redis instances seems slightly better for directories as well, as there is more structure. (although that does seem like a major anti-pattern) i don't have a great grasp on the added complexity of moving everything in to directories the first time. i was also going to suggest hard links (seems like yossi mentioned that previously) for handling atomically moving the files around. that is a common pattern in aws deployments. the user might accidentally upload both versions while we're moving the data, but that should only be for a short time. people seem to have really strong opinions one way or the other here, but i don't think either approach would be unexpected. given my previous preference to store all the data in a single directory, i suppose i would be okay using it as the directory name. we could also introduce an alias so that either `appendfilename` or `appenddirname` both work. if they are in a folder, i don't see that as a requirement i must have missed the conversation about this, why would we omit the suffix? it's useful for identifying the file type. i'll also give the disclaimer that i probably know the least about aof here. i do use aof for a pet project, and checked to see what happened if i were to upgrade, and found that my automation just recursively uploads the entire dir folder to s3, i might have been paranoid though.",-1,-1,-1,0.9640586376190186,0.9645146727561952,0.958750307559967,-1.0,accept,unanimous_agreement
993340859,9788,"we discussed this again in a core-team meeting. the concrete decisions are: * we wanna put all the aof files in a folder * we wanna move the existing aof file into the folder on upgrades at startup. the moving of the file on startup might be done in a few different ways: 1. it doesn't have to be something that needs to be done right away (in can be done in the background, but not by using an aofrw, or waiting for the next one), i.e. it may be ok to do that in cron / thread, but i don't see the advantage in that. 2. it may be ok to do that using hard links (although there is a concern some on some deployments that's an issue) 3. it may be ok to resort to manual intervention in case there's a failure in between two renames (extremely unlikely scenario). regarding the configuration we didn't reach a decision. the options are: 1. use the original appendfilename config 2. use an alias (same default) 3. introduce a new config, and keep the old one just in order to find the old file on upgrades. options 1 and 2 may be confusing for users, and option 3 may look clean, but there might be some deployments in which it'll not behave correctly. the only one i can think of at the moment, is multiple instances of redis writing to the same `dir`, and using a different `appendfilename`, in which case on upgrade without modifying the config, they'll all write to the same folder. i think we can explore the possibility of using a new config directive for for the folder, if we can handle that case in some way that will not lead to a catastrophic disaster (like data loss), even if it'll require manual intervention (assuming this case is the only problematic one). so one solution could be to use the `appendfilename` also as a prefix for the files we create in the folder. another solution is to somehow detect that problematic configuration and fail to start redis with some error message (if that's just this one odd case, then maybe that's a valid outcome). in any case, i think this means we can now proceed with the implementation (use folders, move the file, and use a new config), while we discuss the remaining details (won't be too hard to change these details later).",0,0,0,0.9380634427070618,0.9819451570510864,0.949688732624054,0.0,accept,unanimous_agreement
994407872,9788,"okay, it seems that we have reached an agreement on the use a directory. i will implement it and make a commit asap, thanks.",1,1,1,0.9122491478919984,0.8131633996963501,0.9631683826446532,1.0,accept,unanimous_agreement
998365902,9788,"add a new configuration, we will encounter the following problems (for the scenario you mentioned, that is, multiple redis share the same dir): suppose there are three `redis` processes, they use different aof file names: `redis1.aof, redis2.aof, redis3.aof`, and they share the same dir. [code block] if we add a new config named `appendonlydir`, which use `appendonlydir` as defualt value. when they upgrade, suppose `redis1` completes the upgrade first, then we will have the following dir structure: [code block] when `redis2` or `redis3` starts to upgrade, because they both use the same `appendonlydir` default configuration, our upgrade process will not work at this time.",0,0,0,0.9807751178741456,0.993062436580658,0.9884431958198548,0.0,accept,unanimous_agreement
998683032,9788,"since our upgrade isn't atomic anyway (we have a recovery procedure), what if we slightly change our upgrade procedure to be like so: 1. create the new dir if missing. 2. create a manifest in the new dir pointing to a missing file 3. move the file into the dir. in that case: - we don't have a temp dir (named with a unique suffix) - if we upgrade multiple servers at the same time, one will create the dir, and others should detect the creation fails and use the existing dir - no other problem of any conflicts between servers during upgrade. if the upgrade of any server fails half way, we can recover like this: 1. failure before 1, nothing special, we just retry the whole thing. 2. failure between steps 1 and 2 above, is no concern (the dir exists and that's fine) 3. failure between 2 and 3 above, we can detect that the manifest exists but it refers to a missing file, and at the same time detect that the original aof is still in the root dir, and do the move again. * the upside is that users have a cleaner config (no folder that's named after a config that refers to a file). * normally it all looks very clean, and the only complication is during upgrades, and we can handle it properly (i think). * since all the files in the folder are still using the old config name, there's no clashing in case multiple servers are using the same dir. * the old config is still being used and documented, not only for backwards compatibility concerns. * all scenarios in which user has an existing config file which he may or may not have modified are handled. maybe the only down side is that if a user has a script that used to copy the old aof file, he'll need to update that script to use the dir (different config), but it's likely that his script didn't use `-r` so it would have failed anyway.",0,0,0,0.9729275107383728,0.9858574867248536,0.9813427329063416,0.0,accept,unanimous_agreement
999321239,9788,"yes, in fact, i think the upgrade process will be simpler in this way. i have modified the code according to this idea and ensured that there is a corresponding tcl to test it. plz review this again, thanks.",1,1,1,0.9417290687561036,0.6872227787971497,0.975288450717926,1.0,accept,unanimous_agreement
1001177341,9788,full ci: [a link] valgrind complains on some `fatal aof manifest file error` when it looks for leaks and finds an unrecognized error). freebsd complain: [code block],0,0,0,0.8761698603630066,0.9582130312919616,0.947736918926239,0.0,accept,unanimous_agreement
1001178617,9788,"/core-team please read the top comment and approve. in my eyes, the only part that's still debatable and i think i'd like to change is the deletion of the `aof_rewrite_buffer_length` info field, see [a link]",0,0,0,0.962490737438202,0.9377864003181458,0.98320871591568,0.0,accept,unanimous_agreement
1001183295,9788,add `#include `,0,0,0,0.98529052734375,0.992189645767212,0.9900874495506288,0.0,accept,unanimous_agreement
1002230058,9788,there are some valgrind warnings please look into them,0,0,0,0.9818421602249146,0.9827854037284852,0.9913740754127502,0.0,accept,unanimous_agreement
1002355800,9788,"yes,i find it. just a variable (`line` in aofloadmanifestfromdisk) is not initialized. fixed. and i tested (runtest with valgrind) in my linux machine, it work.",0,0,0,0.9710733890533448,0.8296859860420227,0.9228240251541138,0.0,accept,unanimous_agreement
1004037676,9788,"i've added another commit with a few updates to comments/redis.conf text and removing trailing whitespace. please ack, thanks!",1,1,1,0.9771840572357178,0.9916993379592896,0.9742454290390016,1.0,accept,unanimous_agreement
1004070163,9788,"lgtm, thanks.",1,1,1,0.8687224388122559,0.8991513252258301,0.8075713515281677,1.0,accept,unanimous_agreement
1004114535,9788,"almost forgot, one more comment - the manifest file format will break if `appendfilename` includes spaces or newlines, which are otherwise valid today (just an sds). we need to address that either by rejecting such filenames (slightly not backwards compatible) or escaping such chars in the manifest file.",0,0,0,0.9736164212226868,0.8709102272987366,0.9661299586296082,0.0,accept,unanimous_agreement
1004118393,9788,"i tend to ban it directly. after all, i believe that no one configures it like this in reality.",-1,-1,0,0.8543799519538879,0.7865993976593018,0.933186650276184,-1.0,accept,majority_agreement
1004133640,9788,"hi, i think we need to consider updating _redis-check-aof_ to be able to parse the manifest file and figure out on its own which aof needs to be checked. perhaps even pass the `appendonlydir` to it as an argument. any chance you can look into this and create a separate pr once this is merged?",0,0,0,0.9826071858406068,0.9902297854423524,0.985827088356018,0.0,accept,unanimous_agreement
1004145545,9788,"okay, thank you for your reminder.",1,1,1,0.7599358558654785,0.5816544890403748,0.7505162954330444,1.0,accept,unanimous_agreement
1004158655,9788,i'd like to merge this big-boy and postpone these improvements to a followup prs. i suppose they won't induce any massive change in the existing code. is that ok?,0,0,0,0.9582433700561525,0.6982733607292175,0.9533289074897766,0.0,accept,unanimous_agreement
1004240664,9788,":tada: more than 400 comments 2500 loc, and some 3 months (counting form #9539) thank you for all the time you put in.",1,-1,1,0.9446941018104552,0.625018835067749,0.9769135117530824,1.0,accept,majority_agreement
1004431811,9788,"thank you, now i am going to deal with the next pr.",1,1,1,0.9092336893081664,0.8558986783027649,0.8962216973304749,1.0,accept,unanimous_agreement
1019428620,9788,can you please look into this failure: [a link] [code block] another one: [a link],0,0,0,0.9744614958763124,0.9910077452659608,0.9910147786140442,0.0,accept,unanimous_agreement
1019443025,9788,"i also saw it in my daily. `db 9: 13640 keys ` i guess `r config set auto-aof-rewrite-min-size 1mb`, in freebsd, sometimes it was so slow, and there are not enought key to tigger the rewrite. so maybe change it to 1kb, or do a for loop and insert some keys (maybe at least 20000 keys)",0,0,0,0.9265316128730774,0.9840811491012572,0.9874616265296936,0.0,accept,unanimous_agreement
1019444550,9788,"maybe.. maybe we just need a longer timeout for this check. maybe instead of using `start_write_load` we rather just add just enough data manually, and only then start the wait_for_condition. i suppose someone needs to add more prints and reproduce this so that whatever fix we add we can have some certainty it's gonna solve the problem.",0,0,0,0.9622567296028136,0.982418954372406,0.943384289741516,0.0,accept,unanimous_agreement
1019480825,9788,"it is easy to reproduce if we delete all the `start_write_load` (in case they generate enough keys), and replace it with some `for loop set k v`, but not let it reach 20000 keys. like in this commit ([a link] if we change the 20000 to be smaller, it will fail because not able to touch 1mb. (i can trigger a ci in freebsd and see the result, but i think this is the right fix, the `start_write_load` seems a bit fragile). it also speed up the test (total: 32s -> 25s, in my machine. centos7. but the time it should be similar in freebsd, because we need to generate that many keys all the time (successful use case too)) ci: [a link] btw, i think it is not enought, the `start_write_load` will last 10s, in this case, i think in thoes case of failures none of them produce enough keys.",0,0,0,0.9565547108650208,0.9028241038322448,0.9501820206642152,0.0,accept,unanimous_agreement
1019883342,9788,i think we can change `1mb` to `1kb` and then manually construct a 1kb more (which is easy) data to get a definite test status. -binbin wdyt?,0,0,0,0.9866052269935608,0.9814352989196776,0.9908938407897948,0.0,accept,unanimous_agreement
1188920772,11012,"/ this is following the issue [a link] lots of small changes in this pr, but i think it brings some order onto things. in case the change of bpop --> bstate is problematic i can remake it, but it killed me to look at it :)",1,1,1,0.4560518562793731,0.9768421053886414,0.9787228107452391,1.0,accept,unanimous_agreement
1188926970,11012,just to report that a week ago i run this change on the daily ci: [a link] some errors but i am still not sure it is related to this change,-1,0,0,0.5446336269378662,0.8056555390357971,0.9394829869270324,0.0,accept,majority_agreement
1188987447,11012,"thanks (it'll take some time till i'll get to review this). for the record, i do support renaming `bpop`, i think it's about time..",1,1,1,0.8697145581245422,0.7904382348060608,0.964963674545288,1.0,accept,unanimous_agreement
1265331774,11012,", , -binbin i forced push in order to rebase my version on top of the updated repo (merge would have made some salad of commits) i was unable to accommodate all comments, but most of them have been fixed.",0,0,0,0.8790988922119141,0.9505094289779664,0.983437955379486,0.0,accept,unanimous_agreement
1265571008,11012,"i don't understand why a merge commit would be a problem. i'd prefer you avoid that next time. anyway, water under the bridge now, so please tell me which commits are new? or is the above force-push only pushing an rebase, and no actual new content?",0,0,0,0.6814090609550476,0.9587159156799316,0.5957711935043335,0.0,accept,unanimous_agreement
1265645545,11012,[a link] and [a link] are the new fixes.,0,0,0,0.9888260364532472,0.9910683631896972,0.9920852780342102,0.0,accept,unanimous_agreement
1268142945,11012,"i've reviewed some of this pr but in order to continue please undo some code moving (db.c, blocked.c, remove blocked.h) so the diff won't be as big",0,0,0,0.9734210968017578,0.9891523122787476,0.9908026456832886,0.0,accept,unanimous_agreement
1270456026,11012,i have read the new comments and will address all of them. however due to some work related obligations and the holidays i will only be able to work on it next week. i am sorry for the delay.,-1,-1,-1,0.9873435497283936,0.990794837474823,0.988214373588562,-1.0,accept,unanimous_agreement
1276340048,11012,i made some effort to make the diff cleaner. tell me what you think,0,0,0,0.957677125930786,0.8173052668571472,0.9142650961875916,0.0,accept,unanimous_agreement
1282799135,11012,"the other pr was merged, please merge unstable into this one, p.s. discussed it and conceptually approved it in a core-team meeting.",0,0,0,0.9801660180091858,0.9901021718978882,0.9910985827445984,0.0,accept,unanimous_agreement
1290461004,11012,i have committed the merge and some followup commit to fix a small issue from [a link] please review and tell me what you think.,0,0,0,0.9770500659942628,0.9689143896102904,0.960943043231964,0.0,accept,unanimous_agreement
1296935784,11012,"not sure if it was already discussed, but do we plan to let modules enjoy the new logic? i.e. instead of a module having to provide a `reply_callback`, just reprocess the module command",0,0,0,0.9849334359169006,0.988205075263977,0.9837928414344788,0.0,accept,unanimous_agreement
1296966277,11012,indeed i discussed it internally with before i issued this pr (seems like ages ago) and we thought at first stage we will exclude the modules refactor in order to reduce the blast radius. i plan to invest more in refactoring but it is getting hard to hold back this current phase :),1,1,1,0.8769682049751282,0.9637869596481324,0.9644185304641724,1.0,accept,unanimous_agreement
1321089119,11012,what's the status here? i'm afraid that it'll go stale and be hard to pick back up.,-1,-1,-1,0.9407846331596376,0.9255324006080629,0.9640887975692748,-1.0,accept,unanimous_agreement
1321090865,11012,"i do not think there is a new status to report on my side. suggested to consider aligning the modules blocking infra to this change, but i do not think we should do that at this point given the magnitude of the change as it is. aside for that i will go over the open comments, but i do not think there are any open issues to fix.",0,0,0,0.8691945672035217,0.8542895317077637,0.9746816158294678,0.0,accept,unanimous_agreement
1321659856,11012,i merged unstable and checked that local tests are passing. can we progress with this pr or do you identify something still missing?,0,0,0,0.987514317035675,0.8217546343803406,0.9938737154006958,0.0,accept,unanimous_agreement
1321927344,11012,triggered a full ci [a link] some failures seem unrelated.,0,0,0,0.9541728496551514,0.911703646183014,0.9899818301200868,0.0,accept,unanimous_agreement
1322005194,11012,took your small fixes (aside from one comment i did not understand) as well as place some more points to the top comment. from my pov this is ready to be merged.,0,0,0,0.9632413387298584,0.9468257427215576,0.9656991958618164,0.0,accept,unanimous_agreement
1322086194,11012,"/core-team please take a look and approve (read the top comment). specifically the first 3 sections (include interface and behavior changes) other than the major cleanup, note that it (""blindly"") deletes some big chunks of code that used to process the commands in the unblocked flow. i hope there are no critical differences between these flows and the main ones use when the command didn't had to block.",0,0,0,0.9404468536376952,0.9730049967765808,0.9449286460876464,0.0,accept,unanimous_agreement
1322230354,11012,- please note the last commit to handle the race condition in `blocking xreadgroup for stream key that has clients blocked on list`,0,0,0,0.9881977438926696,0.9938714504241944,0.994248867034912,0.0,accept,unanimous_agreement
1324702689,11012,"reprocess command by re-call `processcommand`, so the new call may meet errors like `oom`, acl rule changed, replication stop, disk errors, we should flag it as breaking change i think.",0,0,0,0.986718475818634,0.9899221062660216,0.9918749928474426,0.0,accept,unanimous_agreement
1324824587,11012,"good point, please mention it in the top comment as well. however, i don't think it's really a breaking change, since the client can't really distinguish between a race that results in the command getting blocked and then getting into an error state, or a command that got processed after the error state started. so in any case the client should expect that error and handle it.",0,0,1,0.7367400527000427,0.875205934047699,0.948246955871582,0.0,accept,majority_agreement
1367100614,11012,i updated the top comment. i think we are ready to go (unless you find some unclosed issue i missed),0,0,0,0.8470069766044617,0.7534288763999939,0.9383074045181274,0.0,accept,unanimous_agreement
1367232353,11012,triggered a full ci: [a link],0,0,0,0.9884634613990784,0.9514199495315552,0.9954967498779296,0.0,accept,unanimous_agreement
1367459136,11012,i made some small fix to prevent the race condition in the valgrind tests.,0,0,0,0.9848628640174866,0.9776586890220642,0.9894677996635436,0.0,accept,unanimous_agreement
1367565363,11012,new valgrind ci: [a link],0,0,0,0.9872406125068665,0.9681469798088074,0.9940754175186156,0.0,accept,unanimous_agreement
1368463023,11012,. done,0,0,0,0.9428805112838744,0.8266977667808533,0.9164353013038636,0.0,accept,unanimous_agreement
1368541932,11012,merged! thank you for your patients.,1,1,1,0.9729068279266356,0.9891563653945924,0.9895042181015016,1.0,accept,unanimous_agreement
1396442627,11012,"one small difference i noticed from before this commit and after this commit. before this commit, when a client is killed in the middle of a blocking command (example: brpop), we were still tracking the command in info command stats as attempted. [code block] after this commit, when a client is killed in the middle of a blocking command, we are not tracking it in the command stats. this difference in behavior may be fine, but it is probably worth documenting.",0,0,0,0.9749249219894408,0.98666650056839,0.8704511523246765,0.0,accept,unanimous_agreement
1396566618,11012,"we're aware of that (command stats are updated when the command actually runs / executes it's logic). the notes at the top include added: let me know if i'm missing something, and feel free to re-edit.",0,0,0,0.9617873430252076,0.8994649648666382,0.9917850494384766,0.0,accept,unanimous_agreement
1441482323,11012,"found a possible bug in this when blocking on keys within a module we always set the flag `c->flags |= client_pending_command;` even when blocked on module, which causes redis to reissue the command even after the module has returned data to the user and has unblocked it. per conversation with this seems like a bug, and that line maybe should be protected with a `if btype != blocked_module {}`",0,0,0,0.776081919670105,0.9758063554763794,0.9824582934379578,0.0,accept,unanimous_agreement
1441493241,11012,can you please describe the bug in some more details? this flag is cleared one we call moduleunblockclientonkey. do you mean that it is not cleared in some other route?,0,0,0,0.9888546466827391,0.9943682551383972,0.9917076230049132,0.0,accept,unanimous_agreement
1441500960,11012,"perhaps, need more investigation, just saw it not being reissued when i commented it out (which isn't the right general fix, but pointed in that direction). will investigate more and get back",0,0,0,0.9678189158439636,0.9243341684341432,0.9476296305656432,0.0,accept,unanimous_agreement
1441519295,11012,"this is what it does is moduleunblockclientonkey [code block] this assumes that that the module replied in the reply_callback. i'm not, i'm using a thread safe context to reply outside the reply_callback (my reply_callback always returns err, and i handle unblocking external to it). that's probably why.",0,0,0,0.9790725111961364,0.9818634390830994,0.9917069673538208,0.0,accept,unanimous_agreement
1441521355,11012,"so the followup q is, for modules should this ever be set in the first place? my guess is that instict was correct. if its auto unset when replied from reply_callback, there's no point to set it in the first place as just breaks people like me who reply/unblock manually elsewhere",0,0,0,0.9326725602149964,0.9852761030197144,0.9871236085891724,0.0,accept,unanimous_agreement
1441540374,11012,", . interesting. so for the simple fix would just be cleaning this flag before/after we issue moduletryserveclientblockedonkey any case (i do not see any problem with clearing this flag as it does not serve modules anyway) a larger refactor (imo) would be to make sure this flag is set on ly in the cases the command itself was blocked on keys and require reprocessing. im am currently writing an issue on how we should better improve the blocking infra - so will make sure this idea will get there.",0,1,1,0.5787522792816162,0.7476646900177002,0.5299370884895325,1.0,accept,majority_agreement
1441547150,11012,btw - i am just wondering regarding your implementation: so when is the module becomes unblocked? or it simply doesn't? as you do not need extra processing? sounds like a fragile implementation imo,-1,-1,-1,0.6109618544578552,0.9519068598747252,0.8711563348770142,-1.0,accept,unanimous_agreement
1441574493,11012,"think thread safe contexts. i.e. when processing outside of main thread. currently, just working on a poc and wanted a module ""signal"" when specific keys became ready, so leveraged blockclietonkeys, in reality, i'd really want a regular callback that will be called when a specific key is signaled as ready, as blockclientonkeys needs a timeout_callback and the current timeout callback callers do not play nice for out of main thread usage, as can be called while processing and unlike reply_callback, it doesn't respect an rm_err response (it ignores all return values). doesn't significantly harm my poc, just want a more flexible api",0,0,0,0.7416422367095947,0.9741244316101074,0.9767336845397948,0.0,accept,unanimous_agreement
1441602619,11012,"thank you. that is exactly one of the scenarios my upcoming issue is meant to deal with. i can push a pr for that, but testing it is something else... probably blockedclient shuld have some way of failing the reply_callback",1,1,1,0.97041916847229,0.8313232064247131,0.978725254535675,1.0,accept,unanimous_agreement
1441608650,11012,"you can push a pr to start a discussion, even if it's far from being ready. but if there are bugs in the current code (for certain module use cases) that are easy to fix, let's do that before 7.2 goes out.",0,0,0,0.9851877689361572,0.980783760547638,0.9904388785362244,0.0,accept,unanimous_agreement
1441662232,11012,"another thing to note in the modules rm_blockcientonkeys is that if you unblock the client manually, like i am doing, it forces timeout_callback to be called (and actually error out if not provided) [code block] i'd note that the comment for abortblock isn't quite accurate in this case, as callbacks are called [code block]",0,0,0,0.9753462672233582,0.9935353994369508,0.9930096864700316,0.0,accept,unanimous_agreement
1441885275,11012,"i do not think this is something that was introduced by this pr. this was the way it was handled before. i can provide a fix , but that usecase is basically wrong imo. returning redismodule_err from the reply_callback is basically saying you wish to continue blocking.",-1,0,0,0.5901488065719604,0.528337299823761,0.6338794827461243,0.0,accept,majority_agreement
1442297521,11012,- please take a look at: [a link],0,0,0,0.9688063263893129,0.9744165539741516,0.9778712391853333,0.0,accept,unanimous_agreement
1446178570,11012,"i think the way you use `rm_blockclientonkeys` - can you please explain why don't you unblock the client from the reply_callback? iiuc you use this api not to block a client, but rather to know when a certain key is ready... which is not the right way to use it. why don't you use the ""new"" keyspace notification? does the fix in #11832 only handles the case where one unblocks a blocked-on-key by using rm_unblcokclient? or does it handle more common use-case like client unblock?",0,0,0,0.9779604077339172,0.9639080166816713,0.8507554531097412,0.0,accept,unanimous_agreement
1446215902,11012,afaik this only applies to client unblocking by module when blocked on keys.,0,0,0,0.9878897070884703,0.98945552110672,0.9902108907699584,0.0,accept,unanimous_agreement
1461742686,11012,"found another bug with blocking commands in a module, when reprocessing them. [a link]",0,0,0,0.9685347080230712,0.9287431836128236,0.9877490401268004,0.0,accept,unanimous_agreement
1914002903,11012,"it looks like we have this problem #12998 when reprocessing the xreadgroup block, we will reset the timeout in blockforkeys (since we will get a new timeout when we unblock a key): [code block] in this case, should we first determine whether c->bstate.timeout has a value? [code block]",0,0,0,0.9823249578475952,0.9926763772964478,0.9917888045310974,0.0,accept,unanimous_agreement
1914058975,11012,"if we do that, we'll need to make sure to reset `bstate.timeout` in `resetclient()` and `createclient()`. but maybe it's a better idea to add a client flag and explicitly let the command know that it is being re-processed?",0,0,0,0.9855255484580994,0.9951624274253844,0.9871395230293274,0.0,accept,unanimous_agreement
1914080236,11012,"that's a good idea, i've thought about it before. but i haven't figured out where to check this flag, something like this? [code block]",1,1,1,0.738278865814209,0.6985551714897156,0.9810559153556824,1.0,accept,unanimous_agreement
1914169133,11012,there's a reprocessing flag in `call()`. set and clear the client flag there,0,0,0,0.9893681406974792,0.9949064254760742,0.9953180551528932,0.0,accept,unanimous_agreement
896805203,9357,"in this pr i refactored the internal implementation of redis list in order to add elements larger then 4gb. curretnly redis list is built on quicklist data structure, every node in the quicklist is a ziplist, after my refactor if the the entry is bigger then 4gb it will be added to the quicklist node as is.",0,0,0,0.9848576784133912,0.9874760508537292,0.9934666752815248,0.0,accept,unanimous_agreement
896837837,9357,"i took a cursory look at the code. 1) i'm not sure if we really need to support 4g nodes, such large data should be more suitable for files. 2) in order to support 4g, i think it would be more appropriate to add a new container type for quicklsit, rather than just using `quicklist_node_container_none`, the way you implemented it makes the code confusing.",0,0,0,0.8724389672279358,0.885759711265564,0.8251795172691345,0.0,accept,unanimous_agreement
896866734,9357,"you mean add `quicklist_node_container_sds` rather than use `quicklist_node_container_none`? so what does none stands for? they way i looked at it, sds is not a container of multiple records (unlike ziplist / listpack) i think we do want to support large nodes in a list, same as we support large ones in hash.",0,0,0,0.9843299984931946,0.9859407544136048,0.9917919635772704,0.0,accept,unanimous_agreement
896899896,9357,"yes, i think quicklist_node_container_sds would be more appropriate. quicklist_node_container_none should only be for future (for now) multi-container support (as you said to me once, and i removed it), just like having quicklist support both ziplist and listpack. we can dynamically convert its container type in a node, converting the container to sds when it is larger than a certain threshold.",0,0,0,0.9661662578582764,0.9901012182235718,0.9857054948806764,0.0,accept,unanimous_agreement
896928039,9357,"i don't follow you. iirc you wanted to completely drop the container field (i.e. `unsigned int container : 2; /* none==1 or ziplist==2 */`), and i said that we better keep is so we can represent a case where one quicklist node is `zliplist` and another one is `listpack`, in that sense `sds` is the 3rd. and note that either one of these can be raw, or lzf (the `int encoding` field). but i don't see how this discussion applies on whatever sds is considered an `sds` ""container"" or a `none` ""container"". anyway, it's just an enum.. we can change it at any point. (unless i'm completely missing your point)",0,0,0,0.9670002460479736,0.9169071316719056,0.9487946033477784,0.0,accept,unanimous_agreement
897286952,9357,"yes, unrelated to naming, i left out my ultimate intention. because if we modify the code directly with `quicklist_node_container_none` (or `quicklist_node_container_sds`), we might end up with a lot of ifs in the quicklist, which would make the quicklist unmaintainable. i would like to add a layer of adapters, such as `quicklistcontainersds` or `quciklistcontainerziplist`, to reduce the number of ifs.",0,0,0,0.9767560958862304,0.990921139717102,0.9903417825698853,0.0,accept,unanimous_agreement
897375047,9357,"i'm not sure i follow you again. are you referring to a common interface (struct with pointers) to reduce the `if`s? maybe you wanna draft a quick pseudo code example? i don't think that's really that important, there aren't that many `if`s here. i think the main complexity of quicklist is about splitting, joining and iterating, inserting and deleting elements inside its containers. but in this case the node is not a container, ti's a plain one element in one node, so the modifications and ""early exit"" `if`s are not reaching the complex parts.",0,0,0,0.9254412055015564,0.6484658718109131,0.8821237683296204,0.0,accept,unanimous_agreement
897382485,9357,"this is what i mean. i tried it during the qucilist ziplist->listpack migration, maybe we can wait for the finished version of . , this is a version of my previous implementation. [a link] [a link]",0,0,0,0.9729598760604858,0.9713616371154784,0.9883995056152344,0.0,accept,unanimous_agreement
897386954,9357,"yeah, that's what i imagined (`quicklistcontainertype`), i think that case is very different because the work with the container is a lot more complicated (insertbefore, insertafter, merge, etc). ironically, this also gives us a good hint that `quicklist_node_container_none` is not a bad name after all...",0,0,0,0.7081848382949829,0.8955714106559753,0.5228514075279236,0.0,accept,unanimous_agreement
950288034,9357,/core-team please approve (see the top comment for details),0,0,0,0.9815946817398072,0.9804152250289916,0.9854238629341124,0.0,accept,unanimous_agreement
950290368,9357,triggered full daily ci: [a link],0,0,0,0.9852478504180908,0.8278221487998962,0.9950826168060304,0.0,accept,unanimous_agreement
950338877,9357,"there are a few leaks, please have a look at [a link] and fix.",0,0,0,0.9874682426452636,0.9205474257469176,0.959882915019989,0.0,accept,unanimous_agreement
951519847,9357,missing copy `quicklistnode->container` in `quicklistdup`. maybe we should make up for this test. [a link],0,0,0,0.9865182042121888,0.9940186738967896,0.9934386610984802,0.0,accept,unanimous_agreement
951592259,9357,"i ran a daily ci and found some valgrind issues, and they were reproduced locally, but i haven't found out what's wrong yet. [a link] the following command can be used to reproduce: [code block]",0,0,0,0.9430038928985596,0.9778195023536682,0.9573606848716736,0.0,accept,unanimous_agreement
951599897,9357,i will take a look on the valgrind issues. thanks !,1,1,1,0.966462016105652,0.9855421781539916,0.9642452597618104,1.0,accept,unanimous_agreement
951631153,9357,"not sure if you saw my comment (it was hidden). the problem is caused by the truncation of `size*` to `unsigned int*` in `ziplistget(p, &tmp, (unsigned int*) &sz, &longval);`.",0,0,0,0.973694622516632,0.9919421672821044,0.9712843298912048,0.0,accept,unanimous_agreement
958921341,9357,what's the status now? i see all comments are addressed.. let me know when this is ready to be merged.,0,0,0,0.9778602123260498,0.973147451877594,0.973818063735962,0.0,accept,unanimous_agreement
958923658,9357,full ci: [a link],0,0,0,0.9845400452613832,0.8597871661186218,0.9959890246391296,0.0,accept,unanimous_agreement
958944711,9357,lgtm. i have no more comments.,-1,0,0,0.6686052680015564,0.7808669209480286,0.9876043796539308,0.0,accept,majority_agreement
959367519,9357,there are some walgrind [a link] for the unit tests. please fix them and we can finally merge this,0,0,0,0.9793821573257446,0.9884118437767028,0.980481207370758,0.0,accept,unanimous_agreement
1036817946,10285,"i think module developers will all love this feature, thanks.",1,1,1,0.9823094010353088,0.9917539358139038,0.9892149567604064,1.0,accept,unanimous_agreement
1036968056,10285,"/core-team please take a look at the top comment and provide feedback about the implementation. there are still a couple of checks failing in the ci, so once those are cleaned up and we have consensus on the finalized implementation i'll ping the group again.",0,0,0,0.981238842010498,0.9850153923034668,0.9630904197692872,0.0,accept,unanimous_agreement
1039955402,10285,"core group meeting decisions: 1. we should add a new api to register an apply callback that is executed after all of values have been set during `config set`. this allows simple modules to define just set callbacks, but more advanced used cases should be able to register this apply config. the apply callback should provide at least a list of config names. 2. the current apply api is likely okay, it makes it straight forward to reason about the values. 3. we didn't fully align on default values. the major points are: * we would prefer defining default values once, perhaps during registration. this value is what is compared against when doing rewrites. * we aren't sure if configs should be able to force being included or excluded from a rewrite.",0,0,0,0.8629575371742249,0.9826412796974182,0.9212326407432556,0.0,accept,unanimous_agreement
1039998689,10285,"few notes: i'm not certain the new apply callback must get names of all configs that were changed (it's a complexity i think we can leave out, and let modules that care handle on their own. i see two options here: one is to have just one global apply callback per module, and the other is to have an optional apply callback per config (in which case redis can detect that several modified configs had the same apply callback, and call it only once, like we do for our internal configs). regarding defaults, the important requirements in my eyes are: first that we avoid making something explicit in rewrite so that we can later change default and still distinguish between explicit changes and implicit ones. and secondly that if we do let the module provide a default value, we must avoid a case where the real default (for startup) could be different than the one used for rewrite, so i think it must be at registration time, and then the module will always get a callback at startup, and can't distinguish between an implicit callback, and a case where the user actually provided a config (i think this is acceptable) just to mention that we agree that enums need better handling, not just an array of valid strings.",0,0,0,0.9717302322387696,0.95696622133255,0.97293221950531,0.0,accept,unanimous_agreement
1042680283,10285,"one thing that has come up with our work on redisraft is that we want to base some redisraft configuration based on redis configuration (tls configuration in this case). it could be nice if one could get the callback as well on general redis ""config set"" commands. thoughts?",0,0,0,0.9706881046295166,0.9785917401313782,0.9479554891586304,0.0,accept,unanimous_agreement
1042738668,10285,"i don't think this should be part of the configuration infrastructure. i think that maybe similarly to `rm_getserverinfo` and `rm_getcommandkeyswithflags`, we should add some api to access configuration (i.e. as an alternative to getting it with `rm_call(""config"")`. and we could add add an event / hook like `redismoduleevent_replicationrolechanged`, notifying modules about configuration changes. i.e. not an event per config, but rather a generic `redismoduleevent_configchanged` that carries the config name.",0,0,0,0.9755659699440002,0.9941283464431764,0.9831491708755492,0.0,accept,unanimous_agreement
1045063189,10285,"few updates here. args is no longer required as an argument to loadex. there's now also an optional apply callback, and a default value that can be specified during registration. enums now work based off ints (i went with the implicit index implementation.) renamed rm_applyconfigs to rm_loadconfigs to avoid confusion with new apply callbacks. also a few documentation updates, and removed '.' restriction in config names. module configs always have a '.', but standardconfigs in the future can as well.",0,0,0,0.9867793321609496,0.9943749904632568,0.8150677680969238,0.0,accept,unanimous_agreement
1045254608,10285,note the forcepush was to clean up a bunch of small commits that made up the new features,0,0,0,0.9859877228736876,0.98496812582016,0.990997552871704,0.0,accept,unanimous_agreement
1047528709,10285,"notes from core meeting i took ( keep me honest): * we want to try to merge in the common code from module.c into config.c, specifically all the code that does parsing and decoding. the ideal state is that the only difference between module configs and regular configs is the bit of code that ""fetches"" the value out of a config. this will likely require some refactoring of config.c, since the abstraction is too opaque. perhaps some of this refactoring can be done independently to speed up the pr. the goal is to minimize the differences between module and redis configs in the long run. * basically allow all the config options to be shared between module and redis except for debug and multi_arg * we ideally want to move the config to a dict for the pr. i think we can do that as a separate pr since it should be small. * we should officially support binary strings. we should theoretically support them since everything is being exchanged in a redisstring, but we should codify that and make sure it works with rewrite and configs. * enums should be able to support explicit values? since we want to share infrastructure, we decided to also pass in a list of enum values. * loadconfigs probably doesn't need to do the apply, instead applying can be called manually. * redismoduleconfigapplyfunc should also take in a redismodulectx and private data. there isn't a clear consensus if we should add guardrails around the context right now. for now assume we don't need them and it can be documented.",0,0,0,0.7432941198348999,0.9747090339660645,0.8970938324928284,0.0,accept,unanimous_agreement
1065818550,10285,"did an absolute ton of refactoring here, there's a leak in register enum that i'm still trying to track down, but wanted to get this out.",0,-1,0,0.792069137096405,0.5296084880828857,0.8137531280517578,0.0,accept,majority_agreement
1066276586,10285,"i think most of the high level comments have been addressed at this point, feel free to take another look if you have time (although there are still some outstanding comments, so you can wait for those to be addressed as well).",0,0,0,0.9005846977233887,0.93784099817276,0.8589954972267151,0.0,accept,unanimous_agreement
1079852597,10285,triggered daily ci for this branch: [a link] passed (errors are unrelated). started one for valgrind with modules: [a link] passed!,0,0,0,0.9831503033638,0.9193613529205322,0.692802369594574,0.0,accept,unanimous_agreement
1082295785,10285,/core-team please approve the api. note the unresolved comment above (currently using char* as error). please update the top comment about recent changes.,0,0,0,0.983625590801239,0.9733276963233948,0.9915887713432312,0.0,accept,unanimous_agreement
1082647548,10285,"i also prefer `char*`, using a heap allocated string is more burden for the common use case (static errors). if we stick with `char*` only the ones that need to format a string have more burden. please look at the last commit and state if you agree to revert it. (if not, we should at least unify some common code)",0,0,0,0.9639230966567992,0.9920679330825806,0.9905808568000792,0.0,accept,unanimous_agreement
1082808322,10285,"as discussed, i don't feel strongly about it but do think the `redismodulestring` option is uglier/more boiler plate but also more correct from an api perspective. pushed a commit to reduce duplication and a small test fix.",-1,0,0,0.7910410165786743,0.94192373752594,0.8694114685058594,0.0,accept,majority_agreement
1083097315,10285,merged :tada: thank you and everyone involved.,1,1,1,0.9412644505500792,0.9940352439880372,0.9932770133018494,1.0,accept,unanimous_agreement
1083994080,10285,ci failed on alpine. [a link] [a link],0,0,0,0.9370065331459044,0.9164673686027528,0.9832872152328492,0.0,accept,unanimous_agreement
1084074781,10285,^,0,0,0,0.4896445572376251,0.947323441505432,0.970213770866394,0.0,accept,unanimous_agreement
1576523944,12109,"replication buffer memory during bgsave with rdb-channel on vs rdb-channel off. the upper graph shows primary's (in blue) and replica's replication buffer size during bgsave, with `repl-rdb-channel` off. ![a link] ## explanation - the rdb channel (on the lower graph) allows us to transfer most of the incremental data memory load to the replica. - after about 15sec, the primary replication buffer start growing, this is the point where the replica start synchronies loading the snapshot into db. ## test details - initialized the primary database with 3gb. - i used redis-benchmark to continuously set a single key with a random value of 8 bytes. - the primary's buffer size is measured by mem_total_replication_buffers, and the replica' buffer size by replicas_replication_buffers.",0,0,0,0.6922214031219482,0.9386783838272096,0.985850155353546,0.0,accept,unanimous_agreement
1609319456,12109,"have you tried using multiplexing to achieve it? the current method of using two channels is a bit too complicated, i think multiplexing would be much simpler and can also solve the problem with ping.",0,0,0,0.7939772009849548,0.9420613646507264,0.9919772744178772,0.0,accept,unanimous_agreement
1616490663,12109,"hi , i have considered multiplexing. although multiplexing also allows the replication data and rdb to be sent simultaneously, the key point is that using another channel, the child process can write directly to the replica. that completely eliminates the need for a pipeline to redis main process. in addition to removing a lot of pipeline complex code, this will increase the responsiveness of the main process during synchronization. the design is better explained in [a link]",0,0,0,0.9436161518096924,0.914094626903534,0.9673502445220948,0.0,accept,unanimous_agreement
1873314337,12109,"hi , , sorry for the delay. i'm still working on previous comments. at the meantime i have some exciting results i want to share. i worked on reviving connset structure and handlers, in order to directly stream online changes from the child process to the replica, without pipeline to main process. here are some of the results. ## data ## explanation these graphs demonstrate performance improvements during full sync sessions using rdb-channel + streaming rdb directly from the background process to the replica. first graph- with at most 50 clients and light weight commands, we saw 5%-7.5% improvement in write latency during sync session. two graphs below- full sync was tested during heavy read commands from the primary (such as sdiff, sunion on large sets). in that case, the child process writes to the replica without sharing cpu with the loaded main process. as a result, this not only improves client response time, but may also shorten sync time by about 50%. the shorter sync time results in less memory being used to store replication diffs (>60% in some of the tested cases). ## test setup both primary and replica in the performance tests ran on the same machine. rdb size in all tests is 3.7gb. i generated write load using redis-benchmark ` ./redis-benchmark -r 100000 -n 6000000 lpush my_list __rand_int__`. ---- i will soon create a second pr for this change (on top of this pr), to avoid making this pr more complex then it already is.",-1,-1,-1,0.9911304712295532,0.9884325861930848,0.981255292892456,-1.0,accept,unanimous_agreement
1873376624,12109,"nice results. i imagine that we can improve the latency spikes (p99) with the old approach too (split some memory copying to smaller bulks). and the huge improvement in time is probably due to having redis fully utilizing it's core, and another core is completely free (which may not always be the case, either because redis isn't busy, or because all other core are). it'll still be an improvement even if these conditions were not true, but probably not that noticeable. anyway, regardless of this being a 100% improvement or an 10% improvement, it is a good one, and now that it's possible to do (due to the separate channel which i mainly wanted to move the memory to the other end), i'd like to proceed. but what are the benefits of a second pr? before we merge this one, the diff will show both of them. i think we can incrementally review and merge both of them in this one.",1,1,1,0.7410082817077637,0.9588040113449096,0.98499596118927,1.0,accept,unanimous_agreement
1873386977,12109,i wanted second pr just because this change can live without pipeline removal. beside making this pr shorter there is no benefit. lets continue on this pr then.,0,0,0,0.9390907883644104,0.8976291418075562,0.9743312001228333,0.0,accept,unanimous_agreement
1918660775,12109,"main changes: - background process will directly stream rdb into the replicas sockets. - revived connection set mechanism. - fixed most of the comments. still working on: - peer replicas connections at the master end. - remove feature flag and simplify replica's state machine. - test coverage for sync failure scenarios. - refactor replication.c. tbd: - avoid copping master query buffer. in my opinion, this will improve performance, but we can push it into a later patch. - send estimated rdb size to replica, and use it to determine buffer limits. since the estimate is not very accurate, we agreed that this feature is at most nice to have.",0,0,0,0.9446840286254884,0.9894608855247498,0.8626384139060974,0.0,accept,unanimous_agreement
2016980828,12109,"[a link] thank you for your submission! we really appreciate it. like many open source projects, we ask that you all sign our [a link] before we can accept your contribution. **0** out of **2** committers have signed the cla. :x: amitnagl :x: naglera you have signed the cla already but the status is still pending? let us [a link] it.",1,1,1,0.9896504878997804,0.9919703006744384,0.9961565136909484,1.0,accept,unanimous_agreement
893596364,9323,relates to [a link] resolving it in some setups,0,0,0,0.981564462184906,0.9936331510543824,0.9928911924362184,0.0,accept,unanimous_agreement
915928959,9323,are you gonna finish this? any major issues? iirc it's only renaming / doc and some minor changes. let me know if you want me to pick it up instead.,0,0,0,0.9846952557563782,0.7029449939727783,0.9243924021720886,0.0,accept,unanimous_agreement
915946186,9323,"i can move on, was just waiting for some answer on the questions you raised, so i can finish in one seating. i’ll apply the suggestions soon. thanks",1,1,1,0.9728076457977296,0.9814262986183168,0.9908726811408995,1.0,accept,unanimous_agreement
917631348,9323,"things got quite complicated due to many conflicting changes especially related to [a link] and [a link] so i decided to squash everything then rebase to reduce my cognitive load. the rebase was commited in 2 steps due to the huge amount of changes (5676f0c and ca922aa). in relation to ""delay to discard cached master when full synchronization #9398"", at replication.c `readsyncbulkpayload()`, i repositioned this new block and run it in a specific situation for swapdb mode, and another specific situation otherwise (please review). it could be wrapped in a method for reuse, but how to call a routine like this?: [code block] in relation to ""slot-to-keys using dict entry metadata #9356"", it hit this pr in some points and even required moving this to server.h: [code block] additionally, raised question about a missing bit: naming for pre and post versions of some module events",0,0,0,0.4976983368396759,0.9072377681732178,0.8174753785133362,0.0,accept,unanimous_agreement
917784414,9323,"for #9356, if you have uncertain things, we can ask for suggestions, he is a friendly man!",1,1,1,0.9847662448883056,0.9683322310447692,0.9904965162277222,1.0,accept,unanimous_agreement
917957805,9323,"i'm here to help. :smile: i tried to move everything cluster-related to `cluster.{c,h}`. isn't it enough to add `#include ""cluster.h""` in some files? i can't see exactly why you had to move them. maybe create some new `.h` file (replication.h) which includes cluster.h and defines some new structs..?",1,1,1,0.9005905389785767,0.9865931868553162,0.9932356476783752,1.0,accept,unanimous_agreement
918484591,9323,"thank you ;) the reason it was moved is that tempdb struct needs clusterslotstokeysdata (which is not a cluster specific concept) tempdb is a broad concept and is used by - db.c (depends on cluster) - replication.c (depends on cluster) if it lived in one of these 2, one of them wouldn't be able to access it. so keeping in server.h was a better choice. i'm afraid i'm still not familiar with the code base as a whole and c best practices to know if that was a decent solution though. i believe that can be also a separated commit after the storm of changes and rebases done here if necessary, what do you think? but most importantly, i believe it would be great to have your review on the correctness of this change after i integrated your pr, for example: i noticed your change includes this cleanup in db.c `restoredbbackup`: [code block] into [code block] which inlining, is: `memcpy(server.cluster->slots_to_keys, backup, sizeof(server.cluster->slots_to_keys));` and that's what i follow here after swapping tempdb with main db: [a link] does it make sense, is the part removed now unnecessary?",1,1,1,0.9764337539672852,0.993647277355194,0.9954468607902528,1.0,accept,unanimous_agreement
945356226,9323,/core-team please approve or comment. see top comment for details,0,0,0,0.9761540293693542,0.978460729122162,0.9752764105796814,0.0,accept,unanimous_agreement
950334226,9323,"good catch. in `emptydb` we call `signalflusheddb` which does both `touchallwatchedkeysindb` and `trackinginvalidatekeysonflush`. but in `swapmaindbwithtempdb` we call `touchallwatchedkeysindb` directly (since we need to pass another argument, and we need to call `trackinginvalidatekeysonflush` the same way as `dbswapdatabases` does.",1,0,1,0.8999047875404358,0.8060193657875061,0.9461578726768494,1.0,accept,majority_agreement
950371730,9323,"oh, actually `dbswapdatabases` also doesn't have a call to invalidate (but i see in documentation that it's on purpose db numbers are ignored in server-assisted client side caching, so no need for that call there). did you mean we need a single call to `trackinginvalidatekeysonflush(1)` at the end of `swapmaindbwithtempdb`?",0,0,0,0.9875447750091552,0.9943431615829468,0.9910646677017212,0.0,accept,unanimous_agreement
950376407,9323,"i'm sorry.. i must be either blind or drunk. let me try again 8-) signalflusheddb which we skip in this mode calls touchallwatchedkeysindb and trackinginvalidatekeysonflush, we can't call signalflusheddb because we need to pass different arguments to touchallwatchedkeysindb, but we should mimic the other things it does, and maybe even add a cross reference note, so that if someone ever updates it, they won't forget to update swapmaindbwithtempdb too. another thing i see that emptydb does is flushslavekeyswithexpirelist. i suppose we need to do that as well. (and drop a similar comment in emptydb). sounds right?",-1,-1,-1,0.9860748648643494,0.9915685057640076,0.967140257358551,-1.0,accept,unanimous_agreement
951897982,9323,"i noticed that cluster tests cascade server config from one test file to another, and this change affects the test ""replica in loading state is hidden"" in the sense that it will run in swap mode (even though 22-replica-in-sync.tcl doesn't specify that). so, is the configuration leaking though tests on purpose and the fix is to explicitly change ""replica in loading state is hidden"" to disable swapdb, or the configuration leaking is a bug in the test suite?",0,0,0,0.9710156321525574,0.9857985973358154,0.9908291101455688,0.0,accept,unanimous_agreement
951935933,9323,"i'm not sure if that's on purpose, but it is indeed a fact. the cluster test infra spins up a bunch of serves and runs all test on them sequentially one by one. you can add a line to the `cluster nodes hard reset` ""test"" in `tests/cluster/tests/includes/init-tests.tcl` and that will clear that config to the default before each unit. or alternatively you can make the new `tests/cluster/tests/17-diskless-load-swapdb.tcl` unit clean up after it (maybe this is slightly better since it's not a common config we expect other tests to mess with)",0,0,0,0.9624300599098206,0.9805201888084412,0.975167453289032,0.0,accept,unanimous_agreement
954642214,9323,i would like to change the description in redis.conf to something like this. please let me know if it's ok: [code block],0,0,0,0.9759154319763184,0.981599986553192,0.9627906084060668,0.0,accept,unanimous_agreement
954687590,9323,"the description for redis.conf seems good, i'd suggest two changes: 1. since this is redis.conf that ships with redis 7, i don't think we need to mention what this meant before. for example for new configs we add in which version they where added. 2. it seems odd to me that the text implies that the master is the one that makes the decision if the replica serves reads during loading.",0,0,0,0.925021767616272,0.9700160026550292,0.8392764925956726,0.0,accept,unanimous_agreement
954723391,9323,"thanks, then maybe: [code block]",1,1,0,0.7041453123092651,0.6617511510848999,0.8302766680717468,1.0,accept,majority_agreement
954805959,9323,"lgtm, please add a commit.",0,0,0,0.989225685596466,0.9867300987243652,0.9949289560317992,0.0,accept,unanimous_agreement
957257038,9323,triggered full ci on the new tests: [a link] [a link],0,0,0,0.9878215193748474,0.9870545268058776,0.9947267174720764,0.0,accept,unanimous_agreement
957314621,9323,"looks like we have a few possible issues. 1. `diskless load swapdb (different replid): new database is exposed after swapping` fails to terminate one of the servers, and this ends up reporting an [a link] report after doing `forcing process 7292 to exit` 2. some [a link] in the module test (possibly timing issues) see below: [code block]",0,0,0,0.9841697216033936,0.9922142624855042,0.9881651401519777,0.0,accept,unanimous_agreement
958104160,9323,"what i could think of, delaying server termination is the async flushing of temp db after the test on successful replication. i reduced the replica amount of keys here to accelerate that. do you think we could have any issues terminating servers, or the valgrind test is just very slow? btw, got this on last run, seems unrelated to the pr: [code block] [a link]",0,0,0,0.8850141167640686,0.9687445759773254,0.9864044189453124,0.0,accept,unanimous_agreement
958108818,9323,"that failing tracking test is unrelated (unstable is broken, still trying to figure it out). regarding the other failures i reported, maybe you can try to reproduce them first and then try to figure out what's wrong (i haven't looked myself yet)",0,0,0,0.8601382970809937,0.7648494839668274,0.9756693243980408,0.0,accept,unanimous_agreement
958112700,9323,"i triggered a re-run of the tests (accessible in the same links i posted above) p.s. valgrind can be very slow, up to 100x slower. but the server doesn't wait for the lazy free queue when terminating.",0,0,0,0.9609869718551636,0.9551175236701964,0.9913982152938844,0.0,accept,unanimous_agreement
958157596,9323,second run came back clean (the valgrind errors it has are ones already solved in unstable),0,0,0,0.9830062985420228,0.988421618938446,0.990643322467804,0.0,accept,unanimous_agreement
958162337,9323,"though i could reproduce in my machine that the actual time it's taking to terminate the server is the rdb being flushed to disk. changing keys to 200 ""fixed"" because it's faster, but i'll making a commit with $replica config set save """" for no delay.",0,0,0,0.986539900302887,0.9733967781066896,0.9895220398902892,0.0,accept,unanimous_agreement
960559883,9323,merged :tada: thank you for your patience.,1,1,1,0.9516010880470276,0.962158977985382,0.9822956323623656,1.0,accept,unanimous_agreement
960560741,9323,thank you for the patience :beaming_face_with_smiling_eyes:,1,1,1,0.9696630239486694,0.9352364540100098,0.9957787990570068,1.0,accept,unanimous_agreement
962561824,9323,"i noticed some sporadic timeout errors in the valgrind runs [a link] [code block] and [code block] they did not reproduce they day before / after so it might be nothing. maybe you wanna look into that, or we can wait to gather more evidence.",0,0,0,0.9576578736305236,0.96358323097229,0.9855898022651672,0.0,accept,unanimous_agreement
964834630,9323,another timeout in the test under valgrind: [a link] [code block],0,0,0,0.9891380667686462,0.9910477995872498,0.995054006576538,0.0,accept,unanimous_agreement
965508778,9323,"i think the timeout is not a fault of the tests, hope it'll get solved by [a link]",0,0,1,0.7886096239089966,0.8206595778465271,0.6016030311584473,0.0,accept,majority_agreement
874587207,9166,"thank you for this pr. i've briefly reviewed the code, trying to sum up what i understand: 1. the server struct has a mechanism similar to the reply list that exists in every client, this one has a refcount in each node indicating how many replicas are sharing that node. 2. a replica client can also have a normal (private) reply list, which is normally used on successful psync. 3. each replica client tracks how many bytes in the shared reply list are meaningful for it (in addition to tracking it's own reply buffers size). 4. when considering the obuf limits disconnection, we look at both the private and shared memory (similar total as before this pr). 5. when considering the total memory used for replication in the server (e.g. for eviction and info), we count the shared memory just once (counting actual memory usage). my thoughts: we have a long term plan to (almost) completely eliminate the slave buffers, see: [a link] had this pr provide a significant improvement with a small price of complexity, i would have merged it right away.. but considering the added complexity, and considering the long term roadmap, i think we may wanna avoid it. note that the above mentioned roadmap plan doesn't indeed solve a case where a sudden traffic spike on the master accumulate a large replica output buffer on multiple repliacs (your pr will mitigate that), but i'm not sure that justifies the complexity price. note that other cases (slow replicas that can't catch up with the master traffic) can just be considered bad configuration. /core-team -steinberg feel free to argue or share additional thoughts.",1,1,1,0.9685359597206116,0.9368045330047609,0.9663848280906676,1.0,accept,unanimous_agreement
874641846,9166,"thanks for your review i want to argue for my pr:winking_face_with_tongue: for your plan, i ever tried to design, there are some problems i encountered, please correct me if i am wrong. - replica stores output buffers during the rdb transfer, we need to reserve this big size memory for developing every instance, because the buffer still exist when finishing loading entire rdb, and master may change into replica, replica also may change into master. so i think we can use this reserved memory only on master. - on full synchronization with multiple replicas, every replica uses one output buffer, for total memory used in one shard, this method still cost much memory. - you already said, your plan still has this bad case, actually, we often encounter it, because some replicas may be thousands of kilometers away from replicas, network sometime is not unstable. - send replication stream during transferring rdb, for diskless replication, there may be more cow memory use because child process exists more time. - multiplexing replication packet is much complicated, replicas need to distinguish different packet types and install different read/write handlers. especially for diskless load solution, i think it is more complicated. moreover, we need to concern compatibility for syncing with old version instance. - it will be not easy to many replicas share one `rdb` on disk-based replication, because we send replication output buffer asap for every replication, new full sync replicas can't copy old replicas' output buffer. for this pr, i have another idea if we allow, we can discard replication backlog memory and use global shared replication buffer to implement replication backlog mechanism, replication backlog just is a consumer of replication buffer, this also may save some memory, and saving memory copy because we just increase refcount of some node when one replica hits replication backlog content. moreover, replication backlog size may be the biggest size of replication buffer that is kept by slow replicas.",1,1,1,0.9359452724456788,0.7177857756614685,0.9661874771118164,1.0,accept,unanimous_agreement
874830933,9166,"i don't have answers to everything, i'll try to respond to the parts i can.. * regarding reservation of memory on the replica machine, i think this is better than having to reserve that memory on the master machine. one case that's easy to reason with is a case where the machine hosting redis (master) is running low of memory (because the master grew), and now you wanna migrate that master to another machine, so we set up a bigger machine, but we can't get the data out of the old machine, since there's not enough memory on it to host the replica buffers and cow. if we move the replica buffers to be a problem of the replica machine, the admin can allocate enough memory and save the day. * for multiple replicas, they're usually on multiple machines, and each should have enough spare, it's right that if we had hosted the replica buffers on the master and share them we can save the total memory, but when the replica is promoted it'll have to have that spare too, and considering a replica per machine, that's the same total. * i think the admin should make sure there's enough memory on the machine to support these spikes, or we can throttle them. i agree it's better to share memory if we can, it's just a matter of complexity vs gain. * yes indeed it can slow down replication and cause more cow.. but on the bright side it save the replica buffers, not sure if that's not overall beneficial. * first, we don't really need to support old replicas, we don't do that in the rdb format either.. a replica must never be of an older version than the master. however, we may still want to use some `replconf capa` mechanism to switch this off for compatibility with other tools that pose as a redis replica. * yes, disk-based replicas will not be able to join an ongoing fork in that mode.. same as we have with diskless. i also thought about that idea of using the shared output buffers for replication backlog.. basically saying that refcount of 0 is still not freed in some conditions (up to a certain size). overall, this pr is certainly useful... my concern is about the extra complexity it adds and whether of not it is needed if we had the other solution in place.. i.e. imagine a case where the other solution is already implemented, then the problem this pr comes to solve is not really that painful. i'll try to give it another look, maybe we can simplify it, or maybe it isn't as complex as i think it is..",0,0,0,0.8111576437950134,0.5312761664390564,0.654592752456665,0.0,accept,unanimous_agreement
891135723,9166,"yes, actually, this mechanism is not complex, i also lightly refactor `writetoclient` which is a bit long. if no tests and refactor, there are less changes. i must acknowledge that reservation of memory on the replica machine is better than on the master because master data safety is much more important, especially, cow expense also is on master. in fact, i think my this pr doesn't conflict with your solution(sending rdb and replication buffer by multiplexing), your solution may mitigate the risk of oom on full synchronization. my pr aims to reduce total memory for all replicas' output buffer, when there are many replicas and output buffer is accumulated since of slow network or waiting rdb finished. we always hope the consuming memory of redis is predictable and controllable after setting maxmemory. we general set memory quota of redis as 1.5~2 multiple of maxmemory (cow, replication backlog, replicas output buffer) in our deployment, but redis may eat too much memory when more replicas and worse network. reserving more memory is costly, but not adding memory is risky in this case. in some other disk storage services, such as mysql, there is only one binlog for replication whatever how many replicas, actually, copying writing commands to every replica output buffer also is writing amplification. in future, for replication backlog, i want we could regard the shared replication buffer as replication backlog if replication backlog is less than shared replication buffer because copying replication buffer is very light, i.e. if replication backlog is 100mb but replicas output buffer limit is 1gb, due to that one replica is slow and keep the replication buffer to 1gb, so another replica could start partially synchronization when reconnecting even its offset gap is more than 100mb with master, of course, offset gap should be not more than 1gb.",0,0,0,0.8841769695281982,0.7880633473396301,0.9587877988815308,0.0,accept,unanimous_agreement
900175350,9166,"we discussed this pr in a core-team meeting and decided we wanna proceed. i didn't review it in detail, and i'm not certain what's the current state (i did take a brief look to realize the concepts). i suppose we'd want to also proceed with using this for the replication backlog too (unless you think it's a bad idea to add it now, and prefer to add it in a followup pr). please let me know what you think the next step is... if you want to do some refresh and add the replication backlog into the mix, or you think it's ready for a detailed review. thanks.",0,0,0,0.8269043564796448,0.7925105094909668,0.6387979984283447,0.0,accept,unanimous_agreement
901225681,9166,"thanks wow! this branch has some conflicts with unstable branch, i need to resolve them, i think current code already implemented the function that all replicas use one global shared replication buffer. i want to apply this mechanism to the replication backlog too, but initially, to make it easy to review, i prefer to make another pr to do that, in fact, i didn't implement it yet. if you want me implement them in this one pr, i also feel fine.",1,1,1,0.9841247797012328,0.9926623106002808,0.9946788549423218,1.0,accept,unanimous_agreement
901272123,9166,"my hunch is that this is better done together, it'll probably impose a few changes on the current mechanisms. but if you feel that it'll be too complicated to review and reason with in one go, i'm ok to do it later.",0,0,0,0.8704466819763184,0.9588651657104492,0.9577538967132568,0.0,accept,unanimous_agreement
915168325,9166,generally it is ready to review and update top comment /core-team do you have any thought?,0,0,0,0.9502111077308656,0.9077901244163512,0.9849318265914916,0.0,accept,unanimous_agreement
926420330,9166,"there are more 150 conversation, it is not easy to review, so i tried to resolved conversation asap, maybe i want to do it later. if i forget and it is important, please let me know.",0,0,0,0.7624862194061279,0.8043690323829651,0.7242177128791809,0.0,accept,unanimous_agreement
926459415,9166,"think it's ok to mark comments as resolved as soon as you address them on your local working copy (or even put a todo in the code). specifically if the comment is trivial suggestion. i think this is important for you to be able to keep track of what you handled and what's still pending. i think it would also be a good idea to comment or add an emoji reaction indicating that you decided to take it or skip it. since emoji don't trigger notification, that's a good idea to use them when the suggestion is trivial or non-critical. but if you decide to reject a more complicated one, it's best to comment back as to why, so we can have a discussion and make sure there's no misunderstanding.. thanks for the great work on this pr. i wanna hammer it and merge it quickly before it gets outdated or we all lose focus and go elsewhere. it was pending for quite a while, but now that we picked it up, let's keep pushing to be merged soon.",1,0,0,0.4956923723220825,0.9403864145278932,0.7963791489601135,0.0,accept,majority_agreement
943085727,9166,triggered full ci to see that there are no leaks and flaky tests: [a link],0,0,0,0.9820806980133056,0.9801106452941896,0.9936696887016296,0.0,accept,unanimous_agreement
943197172,9166,there are many failures in the triggered ci that are unrelated. but this one (happens in valgrind) is a timing issue in the new tests: [code block],0,0,0,0.9600005745887756,0.9772250056266784,0.989232897758484,0.0,accept,unanimous_agreement
943211073,9166,"yes, i found that, and i noticed `replication-buffer` costed 188s, so i suspected `replica2` finished full synchronization. `assert_equal [$master debug digest] [$replica1 debug digest]` is slow. let me increase `rdb-key-save-delay` to make dumping rdb longer. i will do that asap.",0,0,0,0.978173553943634,0.9626787304878236,0.9884268641471864,0.0,accept,unanimous_agreement
945438580,9166,"/core-team this one is finally ready for merge, please approve. see full details about the design, and implications in the top comment.",0,0,0,0.9580228328704834,0.8670811653137207,0.9291858673095704,0.0,accept,unanimous_agreement
950570406,9166,"finally merged.. thank you for proposing, coding, and following my comments.",1,1,1,0.9467451572418212,0.9405246376991272,0.9842952489852904,1.0,accept,unanimous_agreement
953506603,9166,can you please have a look at this test failure (valgrind / timing): [a link] [code block],0,0,0,0.989129900932312,0.983819842338562,0.9944791793823242,0.0,accept,unanimous_agreement
1088062118,10517,"right now slot migration is entirely administrator driven, where they execute a lot of commands and handles reconciliation of failures. this pr definitely does a lot to make this more ""managed"", however i still believe the fundamental gap is that slot migration should just be exposed as an atomic operation to admins. replicating ""setslot"" also introduces the awkward situation where there are now two competing sources of truth for slot ownership, the first from the clusterbus and the second from the replication stream. it might make some sense to have a shard own its slot, and then passively observe the state changes from other shards.",0,0,0,0.7718014717102051,0.9368057250976562,0.923120379447937,0.0,accept,unanimous_agreement
1088216640,10517,"the ""atomicity"" (for lack of a better term) is a nice thing to have but i am not sure if it is truly the core problem or it could be dealt with separately. in other words, i wonder if there is a way to solve the two key issues in slot migration (no replication of migration states and the fragility in the slot finalization logic) without changing the current slot migration protocol. i am guessing ""setslot node"" is the one causing some uneasiness and i agree ""setslot node"" is not strictly required. ""setslot migrating"" and ""setslot importing"" on the other hand should be fine. i added you to the main thread for #6339. i will also think some more about your proposal on #2807.",0,0,0,0.8744858503341675,0.9440385103225708,0.8978235721588135,0.0,accept,unanimous_agreement
1093605831,10517,"i updated the pr to enforce the ordering of `setslot node` as follows 1. client `c` issues `setslot n node b` against node `b` 2. node `b` replicates `setslot n node b` to all of its replicas, such as `b'`, `b''`, etc 3. on replication completion, node `b` executes `setslot n node b` and returns control back to client `c` 4. the following steps can happen in parallel - client `c` issues `setslot n node b` against node `a` - node `b` gossips its new slot ownership to the cluster including `a`, `a'`, etc where `a` is the source primary and `b` is the destination primary. this pr is now complete and it handles the two core issues as we discussed earlier (no replication of migration states and the fragility in the slot finalization logic). this pr should be compatible with the existing slot migration protocol. thoughts?",0,0,0,0.9819299578666688,0.9918305277824402,0.992606520652771,0.0,accept,unanimous_agreement
1097762893,10517,"i like this change and i think it can be merged regardless of the other plans for cluster migrate, because it improves the current migration process, which we can't deprecate any time soon anyway. the change is backwards compatible with old client applications and old redis nodes. old replicas would simply error (ignore) the replicated setslot. what it doesn't solve is a migration interrupted by a failover. the migration doesn't continue automatically. it can be handled by `redis-cli --cluster fix` which can continue the migration (which it can do even without this pr with some guesswork and using cluster countkeysinslot, but now with less guesswork when there is one importing and one migrating node). the problem is that there's still manual administration required in this case. if we can do anything to automate this scenario? for example, can we make `redis-cli --cluster rebalance` detect a failover during migration and automatically continue the migration after a failover?",1,1,1,0.9231740236282348,0.7914133071899414,0.9593688249588012,1.0,accept,unanimous_agreement
1098289333,10517,i think both `cluster fix` and `cluster rebalance` are good callouts. i can certainly help improve redis-cli after this pr is merged.,1,0,1,0.774006187915802,0.8750765323638916,0.9576991200447084,1.0,accept,majority_agreement
1111435763,10517,"just noticed: when `cluster setslot ... migrating|importing` is replicated, until now this was backward compatible, because replicase used to ignore failing commands, but with #10504 included in redis 7, replicas can (optionally, configurable) panic on replication errors. this means that this feature is not compatible with a replica running redis 7.0 if it's configured to panic on replication errors.",0,0,0,0.9474599957466124,0.9901131391525269,0.9892616271972656,0.0,accept,unanimous_agreement
1111760933,10517,"it looks to me that replicas would panic on write commands only (if there was a disk error). i turned on `may_replicate` for `setslot` but left `write` out so i think the change should still be compatible? ps, now that 7.0 has ga'd, i will get my shard id change in shape next. there are a few places in this pr where i need to detect the same-shard relationship and i think they could benefit from the shard id change.",0,0,0,0.9415616393089294,0.9859098196029664,0.985483944416046,0.0,accept,unanimous_agreement
1114315327,10517,fyi. the latest push includes shard id changes from #10536. i will rebase once #10536 is merged.,0,0,0,0.9823116064071656,0.9889659285545348,0.9916951060295104,0.0,accept,unanimous_agreement
1114550639,10517,fyi - with this change both 20\* and 21\* cluster migration tests pass consistently without further modifications. also i believe there is a way to extend this solution to achieve a forkless solution for #2807 as well though i haven't started yet. curious to hear your thoughts on this approach. let me know if there is anything i can help to make some progress on this pr.,1,1,1,0.8365839719772339,0.5657697319984436,0.9013538956642152,1.0,accept,unanimous_agreement
1239175688,10517,"i'd like to see these incremental improvements to slot migration in 7.2, since i don't expect atomic slot migration to be ready soon enough. /core-team wdyt?",0,0,0,0.971635103225708,0.9136777520179749,0.9701687693595886,0.0,accept,unanimous_agreement
1241111383,10517,"i agree about getting these improvements in 7.2, we don't really have much of a concrete plan for when 7.2 is going to launch.",0,0,0,0.9335110783576964,0.9493310451507568,0.9608027338981628,0.0,accept,unanimous_agreement
1242413656,10517,"please hold off on the review of this pr. #10536 is the prerequisite to this pr and it has gone through some churns recently. as soon as #10536 gets merged, i will rebase this pr and ping the thread for a review. stay tuned...",0,0,1,0.8801305890083313,0.96828430891037,0.7281725406646729,0.0,accept,majority_agreement
1328357194,10517,pr is ready for review,0,0,0,0.9579511880874634,0.9766886830329896,0.9734271764755248,0.0,accept,unanimous_agreement
1422297639,10517,"ping /core-team to make sure this isn't forgotten. during slot migration, a master is a spof, so a master crash during slot migration is quite bad. this pr solves that in a fairly simple way. two test suites that were previously disabled and wrapped in `if (false) {...}` are now stabilized and can be enabled again.",0,0,0,0.7607594728469849,0.5470249652862549,0.9725546836853028,0.0,accept,unanimous_agreement
1423080666,10517,"i pinged offline to see if he was around to address feedback now. otherwise i was focusing on other prs. i've lost context, so would rather get this done and merged quickly.",0,0,0,0.7174368500709534,0.918404757976532,0.9826632738113404,0.0,accept,unanimous_agreement
1423100417,10517,hey sorry i missed your offline message. i am around to discuss this pr. i will also need to page in the context as well :-),-1,-1,-1,0.9724215269088744,0.9724374413490297,0.9530414342880248,-1.0,accept,unanimous_agreement
1424985397,10517,"thanks for the review, . let me resolve the merge conflicts and revert all white space changes first and then start addressing your comments. i think i also need to fix my editor settings too to not automatically stripping the trailing white spaces.",1,1,1,0.9617967009544371,0.971727192401886,0.970832884311676,1.0,accept,unanimous_agreement
1449132068,10517,"so, we discussed in the core meeting, and we are all generally not super happy with the fake-synchronous replication. so we would prefer one of the two following alternatives: 1. allow set slot to support a mode where it is not applied locally but only replicated. the redis-cli can then send this command during finalization. it can then wait after it. once the command the wait is returned, the normal set-slot can be sent. 2. implement the logic i suggested, where if the node is a primary with an importing slot, and the source that it was supposed to be importing from is no longer importing the data, finalize the slot and take ownership of it. this may have transient periods where the slot isn't being served if the source was unavailable, but as soon as the source shard is up again the slot will finalize itself.",0,0,0,0.866217315196991,0.9663083553314208,0.91470205783844,0.0,accept,unanimous_agreement
1455297802,10517,"between the two, my preference is option 1 for its (more) determinism. i need to think a bit on how to achieve it but at a high level, i am imagining that i would introduce a new command to replicate the state to the replicas and then a second command to finalize the slot ownership on the primary. the second command would be the command that we are using today, for the backcompat reason. if the caller misses the first command, they are still susceptible to the spof that brought us here in the first place but functionality wise it should still work. does this sound good to everyone?",1,0,0,0.7749171853065491,0.8788490295410156,0.8466457724571228,0.0,accept,majority_agreement
1456844572,10517,"sounds good to me. what would this command be? cluster setslot replicas? you call this on the dst node, then wait, then cluster setslot node? first, i was going to write that i like option 2 because it requires no changes to the admin procedure (not everyone is using redis-cli), but it not perfect that it can leave the slot without owner for a while. btw, are 1 and 2 mutually exclusive? if we can have both, even better. with 2, a cluster can self-heal if it gets in this state (for example due to admin not using the new command).",1,1,1,0.9701660871505736,0.7859006524085999,0.9487958550453186,1.0,accept,unanimous_agreement
1456853190,10517,"yep - something along that line. i don't have a name yet. i will certainly consider this name but also open to other suggestions. that is a really great point, ! will definitely give a try next.",1,1,1,0.9923170804977416,0.9721070528030396,0.9961358904838562,1.0,accept,unanimous_agreement
1506098802,10517,"i have updated the pr to include both recommendations from the core team 1. introduced a new variant of `cluster setslot` , i.e., `cluster setslot node replicaonly` that finalizes the slot ownership on the replicas only and returns the number of replicas on which the finalization succeeds. 2. added logic to automatically take over a slot that is in the importing state but is no longer owned by the original primary. also made sure that the logic can handle a case where the slot is explicitly assigned to another shard. let me know what you think. i'd like to close on the high level idea and implementation first before addressing the remaining comments in the test code (i.e., rewriting them using the new test framework) and patching up redis-cli.",0,0,1,0.7664538025856018,0.6265513896942139,0.7629258632659912,0.0,accept,majority_agreement
1513623267,10517,i am hoping to get a quick review on the engine changes first and make sure we are aligned directionally. i will make the changes in redis-cli next.,0,0,0,0.9616020917892456,0.9541358947753906,0.9758895635604858,0.0,accept,unanimous_agreement
1517326665,10517,i have patched up redis-cli. i would like to request a review from you guys on the production code. i will be refactoring the test code in parallel and my plan is to move 29-*.tcl over to the new cluster test framework under tests/unit/cluster. please let know if this is aligned with your thoughts as well. thanks!,1,1,1,0.9611185193061828,0.9912886619567872,0.9911863207817078,1.0,accept,unanimous_agreement
1521173689,10517,"fyi. i have removed 29-*.tcl and rewritten the tests in `tests/unit/cluster/slot-migration.tcl`. all open comments should've been addressed by now. btw, ci/build-debian-old has been failing outside this pr as well. is this a known issue? [code block]",0,0,0,0.9348506331443788,0.985582709312439,0.971659541130066,0.0,accept,unanimous_agreement
1521251173,10517,"we are aware of the debian ci issue (unrelated to this pr). madelyn is currently off the grid, i suppose she'll review it when she's back.",0,0,0,0.981877326965332,0.9715033769607544,0.990805447101593,0.0,accept,unanimous_agreement
1559665412,10517,"we discussed this in a core-team meeting, we're afraid to merge this now to 7.2 for fear of causing stability issues. it should get merged right after 7.2 is decoupled from the unstable branch.",0,-1,-1,0.9180287718772888,0.657223641872406,0.6659708023071289,-1.0,accept,majority_agreement
1561766885,10517,"it should get merged right after 7.2 is decoupled from the unstable branch. i think we have more time than we originally thought to merge this, so i think we should at least try. 8.0 might be quite a bit aways.",0,0,0,0.9580533504486084,0.9790557026863098,0.980475127696991,0.0,accept,unanimous_agreement
1562272371,10517,"if you feel comfortable, go ahead...",0,0,0,0.8737426400184631,0.9363065361976624,0.817620575428009,0.0,accept,unanimous_agreement
1562765856,10517,"this isn't really a new feature. it solves some stability and consistency issues in the slot migration which can cause data loss, so imo it is better to merge it than to wait.",0,0,0,0.9672434329986572,0.967442512512207,0.945376992225647,0.0,accept,unanimous_agreement
1582421264,10517,will you try? :),1,1,1,0.9771904349327089,0.9880937933921814,0.9833009243011476,1.0,accept,unanimous_agreement
1606595817,10517,and i sync'ed up offline. i will look into replicating the open slots in the rdb itself next.,0,0,0,0.9830145835876464,0.98423969745636,0.9952657222747804,0.0,accept,unanimous_agreement
1608924434,10517,we had one more discussion and agreed that it is a correct solution to replicate the open slot states via bursting of commands. replicating the states via the rdb would be a bigger change with higher risks. i have incorporated all other feedback. it is a go from my end. fyi,1,0,0,0.8264522552490234,0.497755229473114,0.6176750659942627,0.0,accept,majority_agreement
1609787384,10517,conceptually approved during the core meeting if we can get it merged this week. will also take a look.,0,0,0,0.9679843187332152,0.9746626019477844,0.9868199229240416,0.0,accept,unanimous_agreement
1610649738,10517,"didn't look deep into the codes, but to be honest, i don't like using the replication stream to propagate migrating/importing status, this makes the replication stream muddier. iirc, we said that we need to eliminate unnecessary replication and separate non-data propagation from the data replication stream, so that the replication stream only contains data, or in other words, we can distinguish which commands are data and which are non-data via multiplexing. therefore, before that, we should avoid adding more may_replicate type commands. and, i don't see a strong reason that we have to use the replication stream, i think the cluster bus can achieve it well, we can send `clustermsg_type_ping`(by adding a new extended `clustermsgdata`) to replicas to notify them of the change in migrating/importing status, and replicas can reply with `clustermsg_type_pong` to ack.",-1,-1,-1,0.7219840884208679,0.669813334941864,0.6224304437637329,-1.0,accept,unanimous_agreement
1610921787,10517,"can you please provide some more context on the ""multiplexing"" part? i would like to better understand your concern on mixing data with non-data in replication. is it more about the amount of non-data replication or the mere existence of non-data replication? if it is the former, it's worth noting that this replication is only necessary when a slot migration is initiated. there are a few reasons why it is not a good idea to route this to the cluster bus: first, the cluster message size will increase significantly since we need to include the to/from node-ids. additionally, imagine that there are tens or hundreds of slots being moved at any given time on different shards. we are also unnecessarily asking nodes that don't need the information to receive/process it, only to later drop it. while an ack mechanism helps reduce overhead by allowing the node to stop broadcasting when it receives confirmation, it introduces more ha policy questions related. for example, how many replicas should a primary require acks from? the trickier part is that there is likely no one-size-fits-all answer. this is why i prefer the current solution, as it essentially converges on the wait semantics. it is not very intuitive to couple ha with the cluster. another complexity arise when a new replica joins the shard. we need to ramp it up about the slot migration happening in its primary. this means that we can't stop broadcasting just because existing replicas have acknowledged the states. instead, we should continue broadcasting the slot migration states until the migration is complete. this prolonged broadcasting would significantly burden the cluster. the fact that the slot migration states only concern the primary and replicas in question is a good indicator that it is a replication problem in my opinion.",0,0,0,0.9576411247253418,0.986779808998108,0.9816142916679382,0.0,accept,unanimous_agreement
1610936078,10517,"it is a little odd that we replicate that open slots command to all replicas, every time one becomes online. when we'll have the multiplexing feature we'll be able to send it to just the desired replica, and since it's not gonna affect the repl offset, maybe we can also send it earlier (rather than at the tail of the pending command stream). i think the questions we wanna focus on now are: 1. is merging this as is, something that we're gonna regret and can cause complications in the future? 2. do we have another simple solution that can be implemented right away instead? 3. is it important to fix this asap or we can leave it unsolved for another lengthy period of time? i suppose the answers are: 1. doesn't look like something that will cause complications, and we can easily drop it later (or better yet, move it to another virtual command stream when muxing is introduced) 2. looking at the last response, i understand that doing it via the cluster bus isn't easy. 3. although it is already broken for many years (indicating that it's not urgent), it is quite annoying, and can cause quite some damage, right? if the above is correct, i'm in favor of merging it for now.",0,0,-1,0.783896803855896,0.8673542737960815,0.8297931551933289,0.0,accept,majority_agreement
1610967445,10517,"we don't need to broadcast, we just need to send messages(without gossip part) to the replicas of the current master node, just like the spublish command. and the cluster setslot command only modifies one slot at a time, we only need to indicate the change in state for this one slot in the message, so the message size shouldn't be too large. as i mentioned earlier, we don't need to broadcast. we can just send these messages within a single shard. and about the number of acks, we can keep it the same as the number of replicas specified in the wait command. for newly added replicas, we can transmit the state of all slots when they perform the handshake with the master. besides, currently, adding cluster setslot command to the replication stream won't enable new replicas to perceive migrating/importing status since this information is not included in the rdb during full synchronization.",0,0,0,0.9601327776908876,0.979629635810852,0.9714704155921936,0.0,accept,unanimous_agreement
1610990980,10517,"""it is already broken for many years (indicating that it's not urgent)"" and i believe we will drop it later (imho, it's very strange to replicate an admin command). so, i suggest we consider using the cluster bus to implement it, if we don't have enough time, we can put it into 8.0.",0,0,0,0.8671649694442749,0.9142606258392334,0.9369885325431824,0.0,accept,unanimous_agreement
1611676840,10517,"we suffer from these problems when scaling, when many slots are moved. we have seen slots that get two or zero owners. and there is potential data loss. the long term solution would be either atomic slot migration or the cluster v2 ""flotilla"". (is there any decision about that?) in the mean time, i though this pr was supposed to be a quick backwards compatible fix, yet it has been reviewed since before 7.0 was released. how can reliability issues like this be not urgent? aws has their own slot migration implementation. why? can it be that open source slot migration has never really been reliable...",-1,0,-1,0.6267433762550354,0.8179826140403748,0.7854762077331543,-1.0,accept,majority_agreement
1612042388,10517,"the cynic in me is that most people are using managed providers that either do cluster different or wrote around it (like aws as you mentioned). you're right though, this should be a higher priority than it is. i think there is a good debate here, and i have mixed feelings as well. i think it could be reduced into two different ways of thinking: 1. slot ownership is data. whether or not a slot is currently owned and can be served by a node can and should come from the replication stream. a replica will receive the slot state during load and will receive the state changes in the stream. this prevents subtle divergences which can happen, such as the replica thinking it can serve data that was deleted. 2. slot ownership is independent of data. this is more of the current scheme, which is to say the replica will eventually learn what is going on from the primary based off the cluster state. i think ultimately one is the more correct way to think about the data transfer, which may be divergent from the way flotilla is thinking about it and how we do it today. it's also divergent from zhao's ""so, i suggest we consider using the cluster bus to implement it"". i don't agree with that. we discussed this and i agreed with the approach you are suggesting originally, but ping made the case that it was a lot more complexity. i just want to say i don't think this is all that fair of an assessment. i still believe there are structural issues with the way slot migration is broken today, and we need more effort to fix it. there are other issues as well such as the epoch after bump issue.",-1,-1,0,0.946083664894104,0.8476067185401917,0.6646772027015686,-1.0,accept,majority_agreement
1612486886,10517,"of course, i support fixing this problem. however, but the key point is not whether it is urgent, but whether the method of fixing it is correct. as far as i am concerned, using the replication stream method is wrong. have you tried the method that i suggested in [a link] iiuc, the discussion before was mainly about using the cluster bus to broadcast. my suggestion is to spread it only to the affected replica, just like how the replication affects the nodes.",0,0,0,0.833563506603241,0.9735562205314636,0.9320435523986816,0.0,accept,unanimous_agreement
1612624269,10517,"i'm not convinced about replication being the wrong approach, and i'd like to understand your concerns regarding the challenges with ""multiplexing"" that you mentioned earlier. it would be really helpful if you can expand on it some more or maybe share some pointers. fwiw, we currently have a few commands marked for replication. while they may not be categorized as admin commands, the argument of data vs non-data can be quite blurry (for example, `evalsha`). on the other hand, one could argue that these are exceptional cases, which would be acceptable if we can clearly articulate the differences. [code block] i think it's worth diving deeper into this discussion of replication vs broadcasting, data vs non-data, etc here on github, even if it's more philosophical in nature. the knowledge sharing would be extremely valuable for the community as well as for me personally. i think the idea of sharded broadcast is quite interesting (thanks for the suggestion!). i'm definitely open to giving it a shot and seeing how it plays out. however, i don't think it is a guaranteed solution. until i spend some quality time exploring the idea, i can't be sure what other issues might arise. note that the current pr has already gone through multiple peer reviews and lots of testing, including our internal testing. if we consider getting a good grasp of this idea as a prerequisite for making a decision on whether to fix the reliability issue in 7.2, then the answer seems pretty clear to me: we won't be fixing it in 7.2. so, here's an alternative approach i'd like to suggest. how about we focus on addressing the remaining feedback on the existing implementation and merge it into 7.2? at the same time, we can start evaluating the sharded broadcast idea for the future versions. i do have one concern, though. the current design, which relies on replication, gives the caller (redis-cli in this case) explicit control over the number of replicas that need to acknowledge the receipt of slot migration states using the `wait` command. this is an important aspect of the user/admin experience that ideally should be maintained. if we believe `wait` is the right approach (orthogonal to the replication vs broadcasting discussion) or we see no concern of dropping it, then i don't foresee any issues with replacing the internal implementation in the future.",0,0,1,0.6936734914779663,0.8014549612998962,0.5857171416282654,0.0,accept,majority_agreement
1613772962,10517,"indeed, the root cause of all these problems can be traced back to the absence of a consensus-driven epoch bump. until this fundamental aspect is addressed, i think we are just playing a never-ending game of ""whack-a-mole.""",-1,0,0,0.6172226071357727,0.8858100771903992,0.5354712009429932,0.0,accept,majority_agreement
1615932552,10517,"i don't think multiplexing is right here. the point of multiplexing is to support out of band messages, in which case we would have used the cluster-bus as was mentioned. we still can run into issues when a replica fails before receiving a message that it's primary has started importing or migrating a slot. the current implementation relies on the cli to send a wait, but we could have also changed the cli to wait for the replica to acknowledge the new state. i still have a preference for the current replication based implementation.",0,0,0,0.8861119151115417,0.9609981775283812,0.8939989805221558,0.0,accept,unanimous_agreement
1617601220,10517,"one more point for having setslot in the replication stream is consistency for clients that are reading from replicas. when a key is migrated, it is deleted from replicas. if the replica knows the slot is being migrated, it can return an ask-redirect even if del comes immediately after setslot migrating. (#11312 is not yet supported but it's easy to implement after this.) the cluster bus is not synchronized with the replication stream. if we use the cluster bus for setslot, the admin would need to wait for all replicas to ack the new state before starting to move keys, if we want replica read consistency.",0,0,0,0.980558454990387,0.9887726902961732,0.9889222383499146,0.0,accept,unanimous_agreement
1619584910,10517,"i still believe cluster bus is the right way. but, maybe we can change our perspective. why do we have to rely on replication or cluster bus to propagate the metadata? isn't it simpler and more explicit to execute ""cluster setslot with master node id"" on the replica nodes directly? after all, ""replicate"" also needs to be explicitly specified by the administrator. for the old version of the cluster manager, this is a breaking change. it's better to let the administrator be responsible for making sure that the replica nodes are aware of the migration status.",0,0,0,0.8664163947105408,0.9816486835479736,0.9206578135490416,0.0,accept,unanimous_agreement
1620292208,10517,"although your proposal is simpler, it is adding failure points into the system and also requires all existing systems to adopt the new api calls. i don't think we should be pushing too much onto administrators, especially with flotilla wanting to be more of a stateless controller. i think the main decision point is this though, how do we want the migrating and importing state to be recognized on the replica. although we haven't implemented it yet, i also want to discuss how it may relate to cluster v2 (flotilla) as well as an atomic slot migration. ## option 1: have the slot state be transferred through the replication state. ideal state is a rdb full sync includes slot information + replicating set slot commands. - will make sure the replica is serving replication data with snapshot consistency. - clients can use the wait command to make sure that data is replicated. clients are required to make a change to get the benefit of data being replicated though. - atomic slot migration will need a way to indicate to clients that they have the full sync of a slot, to indicate they can successfully serve the data. - flotilla will set importing and migrating state and rely on the server to coordinate transferring the data. ## option 2: have the slot state be transferred through the cluster state - replica may serve a slot is empty or has stale data for since the data has already been fully migrated to another slot. the most serious places i've seen this is right have a sync, the replica might believe it ""owns"" a slot, but have partial data for it. - clients would need to implement logic to check to see that all replicas have acknowledged the new state. - atomic slot migrations would need to wait for replicas to acknowledge the importing/migrating state through cluster messages. - flotilla would replace the importing/migrating state messages sent by the client.",0,0,0,0.9186631441116332,0.9817843437194824,0.9679347276687622,0.0,accept,unanimous_agreement
1621124228,10517,"i haven't had a chance to evaluate the cluster bus solution in depth, but one thing i'm pretty certain about is that we'll need to establish a new mechanism parallel to 'wait'. this will be essential to attain parity with the replication solution, which enables the caller to explicitly synchronize with replicas regarding the slot migration states. that's why i still think that this issue fundamentally revolves around replication. philosophically speaking, i beg to differ with this statement. if we were to adopt the approach of ""admin handling everything,"" i believe there would be minimal need to address any reliability concerns since, ultimately, the admin would be responsible. imo, the main objective here is to reduce human intervention, or the involvement of the control plane to a greater extent, allowing the redis service (managed or not) to have an opportunity to achieve 3/4/5 9s of slos by itself.",-1,0,0,0.539277195930481,0.9398373961448668,0.8702660202980042,0.0,accept,majority_agreement
1621254208,10517,"seems you misunderstood me, my point is that the administrator is responsible for all metadata, such as passwords, common configuration items such as ""appendonly"", and cluster configuration items such as ""cluster-allow-reads-when-down"". it is the responsibility of the administrator to ensure that these are set correctly and consistently on both the master and replicas. in my opinion, slot information is also metadata or a configuration item, and it is the right choice for the administrator to ensure consistency between the master and replicas. even when using the ""wait"" command, it is not transparent to the administrator, and the administrator is still responsible for making the final consistent judgment.",0,0,0,0.885460376739502,0.9680793285369872,0.899095356464386,0.0,accept,unanimous_agreement
1621399866,10517,"requiring manual intervention is quite bad imo. it's too easy to mess up. a cluster does guarantee some things by itself, like making sure there is exactly one master per shard. guaranteeing slot ownership consistency is one such thing it should do imo. if some configuration (or acl, etc.) can be inconsistent between different nodes, that's a design flaw imo and we better fix that; propagate that config to make sure all nodes have consistent configuration. that's a different discussion though.",-1,-1,-1,0.9739921689033508,0.9802446365356444,0.9052779674530028,-1.0,accept,unanimous_agreement
1621467683,10517,"flotilla orchestrates all slot changes through the topology director (the admin never needs to communicate directly with the data nodes), and guarantees that a newly elected primary node is at least as up to date on slot ownership than any of its replicas. (since a replica won't be promoted before it acks the latest topology known by the failover coordinator). there's a potential for stale reads when read-from-replica is enabled -- but that's inherent in any read from replica usecase....",0,0,0,0.966067671775818,0.9858531951904296,0.988399624824524,0.0,accept,unanimous_agreement
1621517422,10517,"i strongly agree with this approach, and i also want to point out that in this case, the topology director is the real administrator, as it is doing the job of an administrator.",0,0,0,0.9004113078117371,0.7473514676094055,0.7856044173240662,0.0,accept,unanimous_agreement
1621524755,10517,"as far as i know, in areas outside of redis, there are many systems that can achieve consistent reads on replica databases, such as polardb. however, this is another interesting topic.",0,0,0,0.9392725229263306,0.9803855419158936,0.9850797057151794,0.0,accept,unanimous_agreement
1621580719,10517,"that's very bad analogy imo. the topology director is a raft cluster, which achieves consensus among shards. the consistency between master and replicas is handled by failover coordinator, which is another raft cluster per shard. these are part of the cluster, not admins. an administrator is a user, typically a human, orchestrating the whole thing. an admin should not be able to induce inconsistent slot ownerships by sending setslot differently to different nodes.",-1,-1,-1,0.978182017803192,0.9882377982139589,0.9620116353034972,-1.0,accept,unanimous_agreement
1621612581,10517,"i don't think so, an administrator is not a specific person, it is a role, as you said, responsible for resource orchestration, metadata management, and other tasks.",0,0,0,0.9657354354858398,0.970373272895813,0.97335284948349,0.0,accept,unanimous_agreement
1621677689,10517,sure -- i meant in the redis replica sense.... not as a global statement for all dist systems :) fwiw i'd argue that a paxos learner (from what i see polardb uses that for read-only members) isn't really a 'replica'....,1,1,1,0.9605547189712524,0.8816500306129456,0.9199829697608948,1.0,accept,unanimous_agreement
1827160862,10517,"hi and team, revisiting our conversation on this pr, i've been considering the cluster bus method. my preference for the replication approach primarily stems from how the `wait` command for replica acknowledgment naturally aligns with it. the proven functionality of this approach and the extensive time invested in its development are also key factors. moving to the cluster bus method would mean developing a new mechanism for acknowledgment. while there are often multiple viable solutions to a problem, i think it is crucial to understand the concrete advantages of each. , your insights on its benefits would be invaluable.",1,1,1,0.4861163198947906,0.9190844893455504,0.9487931132316588,1.0,accept,unanimous_agreement
1830395001,10517,"i'm still waiting for a merge. :) it's a good improvement. a bonus point for making replicas aware of migrations: it opens for replicas to return -ask redirects. currently, they don't know about ongoing migration so they just return null for already migrated keys.",1,1,1,0.9912885427474976,0.9957417249679564,0.996908724308014,1.0,accept,unanimous_agreement
1849513293,10517,"hey team, can we bring this pr to the table at your next core team meeting? it feels like the ideal time to reach a decision and keep things moving. i'm ready to walk you through my thought process and am keen to hear your perspectives.",1,1,1,0.9663387537002563,0.964651346206665,0.949299693107605,1.0,accept,unanimous_agreement
1974667161,10517,all tests have passed. this pr is ready for merge.,0,0,0,0.9645220041275024,0.8848594427108765,0.9717007279396056,0.0,accept,unanimous_agreement
2010963060,13157,![a link],0,0,1,0.9475814700126648,0.9432068467140198,0.5592244863510132,0.0,accept,majority_agreement
2011033559,13157,![a link],0,0,1,0.9475814700126648,0.9432068467140198,0.5592244863510132,0.0,accept,majority_agreement
2011297105,13157,booooo,0,-1,-1,0.39738729596138,0.8844950199127197,0.985837996006012,-1.0,accept,majority_agreement
2011311146,13157,it would be nice to list the new forks in this pull-request along with the ones without cla and shared copyrights with developer certificate of origin. this would avoid this exact situation.,0,0,0,0.9764852523803712,0.9845147132873536,0.9873928427696228,0.0,accept,unanimous_agreement
2011733104,13157,my contributions are made open source under the bsd license. you are not allowed to redistribute nor use them in source or binary forms without the original copyright notice and the bsd license. you must remove my contributions.,0,0,0,0.972489058971405,0.9788862466812134,0.9561330080032348,0.0,accept,unanimous_agreement
2011836479,13157,"are you aware that you have just removed redis from the vast majority of linux distributions? :unamused_face: this is a very bad decision that will not end well for you in the end. look at elasticsearch vs opensearch, mysql vs mariadb, oracle jdk vs openjdk, openoffice vs libreoffice etc. there are some precedents with terraform, elasticsearch, red hat, and a few other big players now dealing with a lot of their target users and potential customers depending on open source forks. as a business strategy alienating future users like that seems misguided. ([a link] if you want an example of how to do it better, check out timescaledb.",-1,-1,-1,0.9729565978050232,0.9269274473190308,0.9924072623252868,-1.0,accept,unanimous_agreement
2011837681,13157,time for a libredis fork!,0,0,0,0.8245976567268372,0.8220582008361816,0.8090935945510864,0.0,accept,unanimous_agreement
2012079375,13157,"that'd be a way out, yes, but it would be so much less wasteful if we could convince you to take this change back. sure, making money producing open source software is tricky, but given the fact that this is carried out with such obvious mistakes as the unlawful removal of bsd headers, are you sure you have done your research and established this won't backfire? i am working for a commercial reddis user, and i am certainly going to look out for forks now. btw good reputation comes from reconsidering and fixing mistakes, not doubling down on them! :)",1,1,1,0.9657436609268188,0.9944783449172974,0.9882081151008606,1.0,accept,unanimous_agreement
2012153631,13157,"edit: please use sircwncmp's redict: ~~really hope someone could take care of either or .~~ ~~however, since a new redis fork isn't backed by any experienced players in the database field (at least for now), i'm not sure will it last long.~~ ~~that being said, if anyone is interested in taking over librekv please contact me :)~~",1,1,1,0.9679329991340636,0.9843378663063048,0.9941614270210266,1.0,accept,unanimous_agreement
2012360667,13157,as a copyright holder you might consider pursuing a dmca process towards the files with mangled license headers.,0,0,0,0.9559087753295898,0.9864308834075928,0.9868170619010924,0.0,accept,unanimous_agreement
2012764160,13157,5.5 years it took for that to become untrue.,-1,0,0,0.5268605947494507,0.5009016394615173,0.8169313669204712,0.0,accept,majority_agreement
2012958877,13157,fork by drew devault: [a link],0,0,0,0.9841278195381165,0.967870593070984,0.993988871574402,0.0,accept,unanimous_agreement
2012998974,13157,"freedis, here we come.",0,0,0,0.94470477104187,0.9659337401390076,0.5420576930046082,0.0,accept,unanimous_agreement
2013219637,13157,"this change is probably uncalled for since it has been claimed that aws has funded one or more redis developers for years. the relevant forks of redis i see are: * [a link] * [a link] * [a link] (perhaps just for record-keeping) alternatively, the legacy of redis may be the resp protocol rather than redis itself. here is a list of projects that claim to support the redis protocol: * [a link] * [a link] * [a link]",0,0,0,0.9780554175376892,0.9920896291732788,0.9873356223106384,0.0,accept,unanimous_agreement
2013282230,13157,wow this is dumb,-1,-1,-1,0.9871894717216492,0.9916160702705384,0.9936646223068236,-1.0,accept,unanimous_agreement
2013692730,13157,no,0,0,0,0.922592043876648,0.8846297860145569,0.9063705205917358,0.0,accept,unanimous_agreement
2013741227,13157,"my commit 11cd983d5819 to allow redis modules to function with modern compilers was also made under the bsd license and permission is **not** granted for it to be used under any other license. copyright for changes in this commit is held by my employer, red hat.",0,0,0,0.9785889387130736,0.9821882843971252,0.9887630343437196,0.0,accept,unanimous_agreement
2014071938,13157,no.,0,0,0,0.8795708417892456,0.9381861686706544,0.9730188846588136,0.0,accept,unanimous_agreement
2014100831,13157,:thumbs_down:,-1,0,0,0.5444921851158142,0.9822263717651368,0.5241621136665344,0.0,accept,majority_agreement
2014186987,13157,"it is not allowed to use the contributions of cloud vendors to run against them by simply changing a famous ""anti-cloud"" license on it. there's no cla signed. you should have better explanations and we'd better be impressed.",0,0,0,0.9232215285301208,0.7357020974159241,0.5604177713394165,0.0,accept,unanimous_agreement
2014336681,13157,obligatory go fuck yourselves :3,-1,-1,-1,0.9793384671211244,0.993860960006714,0.994617998600006,-1.0,accept,unanimous_agreement
2014340803,13157,obligatory fuck redis. i cant wait for you guys to get sued into the ground by everyone who has ever committed that disagrees with this change.,-1,-1,-1,0.987500786781311,0.9916975498199464,0.9948154091835022,-1.0,accept,unanimous_agreement
2014355620,13157,"i appreciate that people claim i exist. i'm moving to development here, [a link] not a great name, but trying to get a lot of the old contributors here to help resume developing where we left off. trying to keep as much the same as possible for now, but i'm sure we'll want to change.",1,1,1,0.971424400806427,0.9909634590148926,0.9768378734588624,1.0,accept,unanimous_agreement
2014361960,13157,"going forward, best option for users downstream is to pin version or switch upstream to a fork depending on use-case (preferably the latter). contributors especially under contract have had their terms breached and this relicense is unwelcome especially after the assurance bsd would remain. fuck off redis, you backstabbers.",-1,-1,-1,0.981190860271454,0.970565915107727,0.9796253442764282,-1.0,accept,unanimous_agreement
2014389264,13157,just go closed-source ffs.,0,0,0,0.98507821559906,0.9758033752441406,0.993672251701355,0.0,accept,unanimous_agreement
2014462566,13157,wording to consider in your copyright violation demand letters: where patch-1 is replaced with the problematic branch.,0,0,0,0.9668422937393188,0.9590421319007874,0.991360604763031,0.0,accept,unanimous_agreement
2014465395,13157,"understand that there is no significant change in terms of individual usability. however, there are some concerns in terms of the opensource ecosystem, so that it would rather hinder, not improve. others have already pointed out problems with bsd deletion. i would like to ask maintainers if there is any possibility of this decision being rollback. thanks a lot. (impressed by what **fisx** said above, so quoting it.)",1,1,1,0.9829469919204712,0.9736088514328004,0.9840829968452454,1.0,accept,unanimous_agreement
2014600274,13157,"a lot of people in here acting like suddenly they're being asked to pay for redis or they're being told they can't do something. considering this new licenses are to restrict competitors, seems silly to be mad that your competitor stopped you taking their work. and i believe that commit is not significant enough to warrant copyright.",-1,-1,-1,0.985832691192627,0.9676477313041688,0.9689873456954956,-1.0,accept,unanimous_agreement
2014627803,13157,"open source licenses are supposed to enable and foster competitors, not restrict them. if this seems a problem to you, you don't understand open source.",0,0,0,0.8720263838768005,0.7903354167938232,0.7110399603843689,0.0,accept,unanimous_agreement
2014678928,13157,"here comes aws elasticache for ""red'ish""",0,0,-1,0.959593951702118,0.9822134971618652,0.839556097984314,0.0,accept,majority_agreement
2014708862,13157,"the community is right to be irritated. non-free licenses are the enemy of foss no matter how you shake it. oss is fundamentally altruistic, so none of us care how many competitors redis has. copying is not theft but relicensing open source contributions without permission is. doing so stands in grave contrast to the philosophy of sharing under which these third-party developers contributed in the first place. ""don't steal from me what i have stolen"" just isn't a very convincing defense.",-1,-1,-1,0.9711776375770568,0.9641382098197936,0.938751459121704,-1.0,accept,unanimous_agreement
2014709005,13157,you forgot that those competitors like aws contributed to redis development (see [a link]. so in my eyes it looks like redis is now backstabbing after it profited from the community which includes cloud service providers.,0,0,-1,0.7681671380996704,0.9705631136894226,0.7955492734909058,0.0,accept,majority_agreement
2014716686,13157,"also i'd be curious to know if, legally, this clause of the bsd licence: aren't new versions included in ""redistributions""? because, in a sense, you are modifying and distributing a new source code. if that is the case, not even redis labs (or whoever) is allowed to change the licence of the code. :)",1,1,1,0.9650723934173584,0.8926281929016113,0.9854834079742432,1.0,accept,unanimous_agreement
2014720805,13157,"[a link] oh, that's explained then",0,0,0,0.9830321073532104,0.9483597278594972,0.9935719966888428,0.0,accept,unanimous_agreement
2014737480,13157,the contributions from alibaba cloud: [a link] [a link] [a link] [a link] [a link] [a link],0,0,0,0.9827909469604492,0.9884802103042604,0.9917913675308228,0.0,accept,unanimous_agreement
2014742606,13157,you can't just hijack work from 700+ contributors.,0,0,0,0.9382604956626892,0.9437399506568908,0.7937144041061401,0.0,accept,unanimous_agreement
2014831737,13157,"also funny how no dev wanted to do this(?), so it had to be a project manager ahah (not a direct attack at the specific person who did this, just at the corporate structure itself)",-1,-1,0,0.7105523943901062,0.8190410733222961,0.3505648672580719,-1.0,accept,majority_agreement
2014840953,13157,congratulations on destroying your reputation,0,1,-1,0.6987636089324951,0.943192720413208,0.9919564723968506,,review,no_majority_disagreement
2014873851,13157,"how to kill your product in 1 easy step.... what do people recommend as alternatives? fill the thread with that. also, contributors should consider a class-action, this is theft of intellectual property.",0,-1,-1,0.7804251909255981,0.5699718594551086,0.8352641463279724,-1.0,accept,majority_agreement
2014943582,13157,[a link] being able to do things like what redis is doing is *the point* of bsd licenses. don't contribute to bsd-licensed projects if you don't want a corporation to grab your code and start selling it.,0,0,0,0.9698629975318908,0.953212320804596,0.926684021949768,0.0,accept,unanimous_agreement
2015084965,13157,"that's not the actual point here, but it's that redis project itself, completely moved to a proprietary license model which restricts users freedom. every free software/open source license gives freedom to sell the copies of the software, with a clause that you'd also distribute the source under a free license. bsd* licenses aren't something new in this. i think many gnu/linux distributions will remove redis from the package database, unless it's critical enough to support a few more years before they purge it off. it would be great if few active contributors can fork redis (the previous version before it got relicensed) and relicensed (keeping the previous bsd license) to a copyleft license, preferably friends of gpl or itself.",0,0,0,0.7196202874183655,0.905543088912964,0.906526267528534,0.0,accept,unanimous_agreement
2015098673,13157,"no, you're thinking of *copyleft* licenses, which the bsd licenses aren't. the bsd license text is still in the repo here: [a link] the notice is still in the repo - they're compliant. i can't imagine they're going to suddenly stop doing something so simple, so they're going to comply with this. they weren't doing this before, they aren't doing this now - they're compliant. that is *all the bsd license requires*. there are no other requirements, no ""you must put your code under an open license"" or even ""you must release code"". yes, certainly, the new licenses aren't free. but what they're doing is legal, even if it's annoying/not foss/will make them unpopular/etc. they have not violated the bsd license in any way.",0,0,0,0.9326339960098268,0.9500638842582704,0.8174871206283569,0.0,accept,unanimous_agreement
2015110649,13157,"you didn't interpret it correctly. although i didn't explicitly said there but i'm well aware of this. what i'm saying is ""every free software/open source license gives freedom to sell the copies of the software"" and so bsd licenses aren't new in this. bsd licenses are source of confusion when you'll use them in larger projects. there are a lot of variants exists of this license (originated from berkley). indeed. permissive licenses allow software to go under proprietary control, it just that redis took the advantage of oss and now they go into deep silence. edit: forgot to mention but i think some distributions may continue distributing redis. for example, ubuntu maintains snap and they allow proprietary product to be install on the system as snap. also, distributions such as rhel... may also keep redis. fedora (very likely) has a non-free repository, so they can keep it there as well, if community wishes to do so.",-1,0,0,0.7090117931365967,0.6356239914894104,0.6582393646240234,0.0,accept,majority_agreement
2015179373,13157,blink twice if the private equity folks have you in an awkward position.,-1,-1,0,0.7472468018531799,0.6101555824279785,0.5175434947013855,-1.0,accept,majority_agreement
2015217903,13157,apparently that's already being done :) by: above i like redict especially :),1,1,1,0.9894840121269226,0.99598628282547,0.996966540813446,1.0,accept,unanimous_agreement
2015244132,13157,very good move if you want to kill your project. congrats,1,1,1,0.9859431982040404,0.9957224130630492,0.9948257207870485,1.0,accept,unanimous_agreement
2015266561,13157,new redis logo ![a link],0,0,1,0.9539429545402528,0.5717988014221191,0.9843391180038452,0.0,accept,majority_agreement
2015278240,13157,nice :) thanks for sorting it out.,1,1,1,0.9832995533943176,0.995852828025818,0.9968388080596924,1.0,accept,unanimous_agreement
2015476486,13157,"hey all. quick disclaimer - i am a redis employee, not a dev though. we worked with various open source experts on this. the license.txt file is the first-party notice for redis 7.4 and subsequent versions. [a link]is the file that contains the 3bsd license (a third-party notice for the applicable portions of the project). we would never violate the licenses and do not seek to alter the licenses of any existing (or in-flight) contributions. any contributions that are still in process will fall under 3bsd.",0,1,1,0.6258578300476074,0.9636845588684082,0.7369321584701538,1.0,accept,majority_agreement
2015547447,13157,"but- you did. you removed the header, from all of the code contributed under the bsd license. none of the previous contributions, under bsd, are labeled as bsd anymore. instead, everything is labeled with the new license.",0,0,0,0.9871773719787598,0.9906693696975708,0.9915775060653688,0.0,accept,unanimous_agreement
2015645044,13157,"am i missing something? if the 3bsd license was on a file, we left it in place. example: [a link]",0,0,0,0.9757921695709229,0.9937307834625244,0.9837659001350404,0.0,accept,unanimous_agreement
2015676832,13157,"ok, let me pick a random file. [a link] this file, has commits from [a link]. many of those users, contributed code under bsd terms. did, you ask all of those users if they wish to change the license? is the bsd license mentioned anywhere in that file? how about this [a link] from [a link], originally committed under bsd terms. did- you ask him if he agreed to remove the bsd license from his commits? no. you internally agreed to remove the license. you did not consult the community. you did not ask previous contributors if they agreed to this change. as a result, as we speak, redis is being actively pinned to older versions, or just outright removed from repositories, linux distributions, etc, because you internally decided you no longer want to do open source. open source doesn't mean you get to own the code, and just take everyone's contributions for free. no. open source means, open source. we can argue about this all day, and in the end, that won't change a single thing. however, i would urge your leadership to go evaluate the ramifications which are currently occurring. * [a link] * [a link] * [a link] know what is going to happen when a cve gets released for version 7.2? the remaining user base you have, is going to look elsewhere, as you cannot back-port the fix, due to your proprietary license. i'd urge you to go review the history of a few other projects, with similar fates. * owncloud / nextcloud. * pfsense / opnsense. * mysql / mariadb * openoffice / libreoffice although, i suppose in the end, it doesn't matter. you have already lost the trust of your user base. [a link]",0,0,0,0.9668200016021729,0.9734691977500916,0.9543219208717346,0.0,accept,unanimous_agreement
2015712860,13157,"is there anything in the 3-clause bsd license that says it needs to be included in every file? it doesn't look like it, but maybe i'm misreading it. the parts where it reads ""redistributions of source code must retain the above copyright notice"" wording sure seems like keeping a copy of it in the repository (which they've done) meets the requirement. they mentioned that the change only applies to new code, not existing code, so it seems like this change, although perhaps distasteful meets the requirements of the bsd license. i'm not saying this to be a jerk. i think it's reasonable to be upset about this kind of unexpected change. but i also don't believe that trying to make issues out of non-issues is helpful.",-1,-1,-1,0.7354563474655151,0.7057790756225586,0.8447650074958801,-1.0,accept,unanimous_agreement
2015727491,13157,"likely not. but, it's not going to stop me from getting on my soapbox, and fussing about a decision to commercialize the many contributions of others, committed under bsd terms. i am sure they have lawyers who went over the fine print, to double-check everything. but, in the end, a few people will give a thumbs up to my rant, and likely nothing will change at all.... and 6 months from now, everyone will have swapped over to redict, or whatever popular fork emerges from this... and redis can join the dozens of services which took similar steps like this in the past. i should also add- mysql / mariadb to my list.",-1,-1,-1,0.9155125617980956,0.9784414172172546,0.8396838307380676,-1.0,accept,unanimous_agreement
2015802209,13157,ianal but i think that relicensing the bsd-3 source code under the terms of a different license is possible even without the explicit approval of the contributors as long the terms of the bsd license are still fulfilled under the new license (see topic license compatibility). otherwise the redict fork would also be illegal since drew devault relicensed the bsd-3 code under the terms of lgplv3. but relicensing a community involved project under a proprietary or source available license is still a massive d***move.,-1,0,-1,0.9089404940605164,0.8101531863212585,0.9115037322044371,-1.0,accept,majority_agreement
2015965802,13157,the bsd license text is [a link] and they're still complying with its requirements. it does not require license headers in files.,0,0,0,0.9857009649276732,0.99026358127594,0.9948322772979736,0.0,accept,unanimous_agreement
2015968626,13157,oh yeah i completely overlooked that it was licensed under a [a link].,0,0,0,0.8408275246620178,0.686259925365448,0.9251222610473632,0.0,accept,unanimous_agreement
2015993737,13157,"well, this makes it harder to suggest redis to clients in the future, at least with a good conscience. best of luck, thanks for all the fish.",1,1,1,0.94320946931839,0.9894981384277344,0.9728055000305176,1.0,accept,unanimous_agreement
2016249492,13157,"no it doesn't, and had they stopped at removing the license header they would probably be fine, but they added a new license header, and that likely crosses a line.",0,0,0,0.9866820573806764,0.9834451079368592,0.9424634575843812,0.0,accept,unanimous_agreement
2016435846,13157,"### ""hope our contributions don't end up funding someone's yacht"" always think of this before pouring hours into open source projects, imo github (cc: , ) should enforce or have a rule of some sort where you can't change licenses until all contributors signs it off. not just licenses but major changes in general.",1,0,1,0.8889040350914001,0.8294008374214172,0.6053867340087891,1.0,accept,majority_agreement
2016446328,13157,"industry keeps evolving and shape-shifting in weird ways, and in this case, beats me _to thinking if olympus that bad that it went south enough_ to kill a fantastic software and community?",-1,-1,-1,0.980305016040802,0.9739582538604736,0.9876201152801514,-1.0,accept,unanimous_agreement
2016475934,13157,"to clarify, they removed the text of the license from _most_ headers, although it remains in about ~26 files under `src/` (mostly files originally contributed elsewhere). as for the attribution statements in the headers, this patch almost exclusively removes those referring the original author salvatore sanfilippo, while leaving other names alone (for example in [a link]. some of the names remaining in headers, whose contributions are seemingly still distributed under bsd3, are: * joyent * harish mallipeddi * georges menie * matt stancliff * amazon web services / ""amazon.com, inc. or its affiliates"" * yinqiwen * pieter noordhuis * marc alexander lehmann * makoto matsumoto and takuji nishimura * the regents of the university of california since so many files are distributed under a license different from the project's default, it would be useful to provide granular machine-readable statements as recommended e.g. in [a link]",0,0,0,0.962794840335846,0.9821329712867736,0.9910935759544371,0.0,accept,unanimous_agreement
2016508771,13157,clear as water: the name redis is being used to endorse products derived from this software without specific prior written permission. ![a link],0,0,0,0.8431215286254883,0.8948190212249756,0.9497320652008056,0.0,accept,unanimous_agreement
2016523357,13157,"i have done a deep analysis of the commits to this project. see [a link] for the details. here's a list of all the contributors who have more than 1,000 active lines of code (not counting whitespace changes). if any or all of them haven't approved / given over their copyright, they have good rights to fight this: [code block]",0,0,0,0.6879518628120422,0.916702926158905,0.9548267126083374,0.0,accept,unanimous_agreement
2016524624,13157,"[code block] active lines of code: [code block] 1.8% of the active code base. you, sir, have good standing in a court of law.",1,1,0,0.6943541169166565,0.9154658913612366,0.6850025653839111,1.0,accept,majority_agreement
2016529095,13157,"wrote: madolson's contributions: [code block] with 1.4% of the active source code, you and viktor have contributed 4.2% of the active source code, which has been relicensed without your consent. you have good standing in court.",1,1,0,0.5921976566314697,0.8563744425773621,0.9547755122184752,1.0,accept,majority_agreement
2016533469,13157,"this is covered by contract law as ajudicated in both the united states, uk, and several countries of the eu (notably france, germany, and switzerland). the courts have universally decreed that copyright and license clauses on top of files of a source code project act as additional explicit conveyances of rights and restrictions per the authors of those files. especially in the case of closed-source software, such clauses act as safeguards against unsupervised leaks or stealing of the code, especially when used in other projects or when inappropriately relicensed without 100% of contributors' with active source code consent... this particular contract law has been affirmatively adjudicated in the uk, us and germany in separate cases. every developer *should* add copyright clauses to the top of their files, such as this one from one of my projects: [code block] altering these clauses without 100% written consent or copyright assignment contracts from all contributors to that file (with active lines of code) is tanamount to breach of contract, and all rights to publishing / making a product are liable for damages from the rights holders. it technically no lnoger becomes ""legal"" software, as it is a breach of contract. because github is a us-based company, and this project is hosted there, the most direct line of legal action is for the contributors to submit dmca takedown requests for individual files they have active lines of code on. i have [a link], if anyone reaches out, i can run reports that show the exact lines they have rights to. [a link] since redis, ltd., is a uk company, you would need to file claims against the copyright, designs, and patents act 1988 (cdpa).",0,0,0,0.9163975715637208,0.9860857725143432,0.9870728254318236,0.0,accept,unanimous_agreement
2016537682,13157,[a link] so long redis. hello [a link],0,0,1,0.9818379878997804,0.8047913908958435,0.6825095415115356,0.0,accept,majority_agreement
2016570120,13157,![a link],0,0,1,0.9475814700126648,0.9432068467140198,0.5592244863510132,0.0,accept,majority_agreement
2016594843,13157,are there other links or discussions for legit communities on github that are willing to fork and maintain redis from its previous license version and create a solution like opensearch following elasticsearch's licensing changes? please share links here so we get notified!,0,0,0,0.9183483719825744,0.8962973952293396,0.9684798121452332,0.0,accept,unanimous_agreement
2016620957,13157,"and others looking for that kind of info: lots of links have been shared to forks like **redict.** read through the feed instead of commenting, it's just burying them further.",0,0,0,0.9141408205032348,0.6118015646934509,0.9405338168144226,0.0,accept,unanimous_agreement
2016620997,13157,"ianal i think there are two points that do not seem to be explained here: * if i get a shallow copy of latest redis source code, i am not able to determine which parts of the code are covered by the original license terms. * it is not clear if you can add additional terms to the parts of the code covered by the original license terms. if you sign a contract with someone, obviously you can't add new terms to the signed contract. it could be different from a country to another, though.",0,0,0,0.9376124143600464,0.9496538639068604,0.9480059742927552,0.0,accept,unanimous_agreement
2016624891,13157,"i'm a bit skeptical since it's not hosted on github, so i'm wondering why this fork wasn't hosted here instead of some other site i'm not familiar with. it makes me question the legitimacy of it.",-1,-1,-1,0.9237886667251588,0.930716335773468,0.9145281314849854,-1.0,accept,unanimous_agreement
2016658885,13157,"codeberg is a more open alternative to github, kinda like gitlab.",0,0,0,0.9789823293685912,0.874065637588501,0.9806846976280212,0.0,accept,unanimous_agreement
2016668327,13157,"in the public interest, i will note that gitlab and others are infrastructurally more fly-by-night, subject to its mba parasite induced revenue constraints that put their long-term operations at risk. gitlab already diminished their free offerings not too long ago, with a very small notice period. in contrast, microsoft is a behemoth whose existence is at this point tied to the national security of the united states. the point is that despite any free code by codeberg/gitea and gitlab, allowing for potential self-hosting, as much as i applaud their efforts, **too many people will not take any project seriously if it's not hosted on github**. if github is good enough for apache projects, and it increasingly is, then it's good enough for most projects. certainly any forks of redis do not merit any special infrastructural exemption like some ""base layer"" or ""small hobby"" projects might.",0,-1,0,0.4667177200317383,0.8966318964958191,0.675032913684845,0.0,accept,majority_agreement
2016698144,13157,"_it's pedantism time~!_ because of concerns with gitea ltd, codeberg currently uses their own fork of gitea that they've called forgejo - and has done so for a while now. it started as a soft fork, but [a link]",-1,-1,-1,0.9594652056694032,0.7041666507720947,0.7428896427154541,-1.0,accept,unanimous_agreement
2016776663,13157,"while codeberg could be a good open-source alternative, github has become the de facto standard platform for many developers and open-source projects. we all ""live"" here. migrating to a new platform would create friction and barriers for existing contributors and it will split the community and scatter the efforts. if there isn't a strong reason for the move, it raises a significant question mark for me. the decision to move away from github may not be worth the effort and potential loss of contributors. the new initiative should prioritize the long-term sustainability and growth of the project and its community. i would fully support the [a link] like [a link] if it remains entirely on github. :raising_hands:",-1,0,-1,0.7244873046875,0.7848500609397888,0.528378427028656,-1.0,accept,majority_agreement
2016820000,13157,"i see people calling this ""misguided"". i think the word you're all looking for is ""incredibly fucking stupid"".",-1,-1,-1,0.9892539978027344,0.9804004430770874,0.9773922562599182,-1.0,accept,unanimous_agreement
2016870631,13157,"it's funny that in three days we went from ""redis is changing licences"" to ""where is the best place to host the new redis fork""",-1,-1,-1,0.739372193813324,0.5471726655960083,0.9647058248519896,-1.0,accept,unanimous_agreement
2016939572,13157,"it's also would be nice to have a non copyleft fork of redis. preferably with the original bsd-3 license, since what's a point of changing the license at all.",0,0,0,0.9664262533187866,0.9861435294151306,0.9856045842170716,0.0,accept,unanimous_agreement
2016942127,13157,from what i've read [a link] by will most likely be sticking with the bsd-3 license.,0,0,0,0.9862138628959656,0.9800451397895812,0.9796804785728456,0.0,accept,unanimous_agreement
2016958057,13157,"what a strange way to kill a product. not bad for a first pull request! ;) i need to point out that you're a bit early for april fools though. you should probably change your profile to be more accurate. ""director of product mismanagement"" has a nice ring to it, doesn't it? ;) also, are you licensed to use that phrase from star trek?",1,1,1,0.9737246632575988,0.9547459483146667,0.9732776284217834,1.0,accept,unanimous_agreement
2016966845,13157,"also, please correct me if understand it wrong. according to their faq in question 21. if you create a product using redis, which start to earn money. redis can just look at it, see it commercially viable and then create a similar product. and you will be forced to fix redis version in order not to become a competitor and break their license. that's like the worst way to make redis appealing to anyone.",-1,-1,-1,0.961403489112854,0.9653319120407104,0.8871434926986694,-1.0,accept,unanimous_agreement
2017023944,13157,note that as redis labs executives have made explicit statements such as [a link] one may presumably continue to use future redis releases under the original bsd license terms in addition to any other listed licenses as a reasonable person would likely interpret this statement to be a perpetual irrevocable license grant to use any version of redis as distributed by redis labs under the original bsd license terms(ianal but i would presume written statements made by company executives regarding licensing to be legally enforceable). note that this likely would not apply to versions of redis distributed by entities other than redis labs as this implied perpetual irrevocable license grant was only explicitly made by redis labs.,0,0,0,0.9737435579299928,0.9877391457557678,0.9920697212219238,0.0,accept,unanimous_agreement
2017082556,13157,that's one way to kill a project,-1,0,-1,0.7145406007766724,0.9244530200958252,0.7606011033058167,-1.0,accept,majority_agreement
2017088097,13157,"yeah, our plan is to keep it the same. i'm trying my best to follow the will of the community there, and the general preference seems to be to keep the license the same.",0,0,0,0.9047155976295472,0.8627742528915405,0.9624674916267396,0.0,accept,unanimous_agreement
2017185715,13157,can everyone review my pr to fix this [a link] update: looks like they closed it :crying_face: long live the community fork [a link],-1,0,-1,0.952598512172699,0.8668009042739868,0.720493495464325,-1.0,accept,majority_agreement
2017548630,13157,"sound reasoning. being more familiar with swedish law myself i'm not confident that my analysis under that legal system would be the same. could you provide the references to the case law mentioned, it would be interesting to read up on at least a few of them :)",1,1,1,0.9530871510505676,0.973086714744568,0.9851689338684082,1.0,accept,unanimous_agreement
2017702376,13157,in case this is in any way useful:,0,0,0,0.9795246720314026,0.9805676341056824,0.9720488786697388,0.0,accept,unanimous_agreement
2019063472,13157,"legalities aside, i can't help but feel saddened by the gradual rise in instances of widely used previously open-source software being re-licensed to licenses that deviate from the core essence of open source. i also can't help but fear what the future of open source will look like if the norm becomes such that the moment a project becomes popular enough, it gets taken away from the community that helped build said project in the name of monetary gains. while i don't agree with the approach taken, i'm also not oblivious to the fact that it is true that large companies (e.g. cloud providers) stand to benefit a lot from leveraging existing open source solutions and selling it as their own without necessarily giving an equivalent amount of ""love"" back to the maintainers of the open source software they profit from, i wish we could all sit at a round table and think about the greater good of open source instead of each company focusing on their own profit margins.",-1,-1,-1,0.9657211303710938,0.966807186603546,0.9313771724700928,-1.0,accept,unanimous_agreement
2019537782,13157,"now each time i remember this phrase, it associates with this pr, and with the problem of re-licensing to source available licenses. it was such a good phrase, why did it have to be used in such a controversial context. i am in the middle of watching voyager, and it's just stuck with me.",-1,-1,-1,0.8406886458396912,0.9506930112838744,0.9323875904083252,-1.0,accept,unanimous_agreement
2019715484,13157,"this unilateral merge is a betrayal of how many, about 700 (bsd) contributors? :vomiting_face:",-1,-1,-1,0.985032320022583,0.8743332028388977,0.9946780204772948,-1.0,accept,unanimous_agreement
2019797807,13157,just articulated my thoughts clearly in few paragraphs.,0,0,0,0.9667378067970276,0.9516796469688416,0.9634050726890564,0.0,accept,unanimous_agreement
2019827864,13157,"i do not think that this path is gonna repeat itself for the next wave of companies, in fact it does not even for companies that started a few years ago. all the next-gen startups operating in infra space chose a license that protects them right from the start from such situations. i am talking about scylladb, redpanda, cockroachdb, dragonflydb. i can personally say, we chose to be upfront with dragonfly community, knowing the price we gonna pay for having fewer distribution options for dragonfly. redis is a bit different and unique - it started early, when oss movement was still naive, and it was founded by a single developer.",0,0,0,0.5667275190353394,0.813186764717102,0.7822220325469971,0.0,accept,unanimous_agreement
2019996298,13157,"definitely not all chose to go with ""source available"" licenses. and if we count ""source available"" vs open source, i have a feeling that open source will have more (though i didn't check). for example, we can look at new projects in linux foundation, cncf and apache foundation. although i don't have much experience in oss. to me, it looks like this path will lead back to where it started, with two opposite sides, proprietary and copyleft software. this doesn't look that optimistic.",-1,0,0,0.5150412321090698,0.7120503783226013,0.8758566975593567,0.0,accept,majority_agreement
2020032794,13157,so let me get this straight. a company that hosted redis [a link] a company which [a link]. a company who acquired the name redis in **2021**. this very company which only exists because redis was foss has the audacity to change the license.,0,0,0,0.8342647552490234,0.9790145754814148,0.786929726600647,0.0,accept,unanimous_agreement
2020673140,13157,"after analyzing this new redict fork, it's all bs... **the redict maintainer has illegally relicensed all of redis code to a much more restrictive license: the lgpl.** this is *blatant* breach of ethics, even worse than what redis, ltd., has done (since they arguably have permission to relicense at least 50% of the code).",-1,-1,-1,0.9667433500289916,0.7323694229125977,0.94207501411438,-1.0,accept,unanimous_agreement
2020715437,13157,"-ahlberg : the most defining case was decided by the united states supreme court, decided on 5 april 2021: google, llc, v. oracle america, inc. (185-96). [a link] the supreme court decided that 1. copyright clauses on top of files do, in fact, dictate the license terms of the individual file, as an additional safeguard against improper obtaining (e.g., via accidental plublishing, espionage, and/or theft). 2. they delineated 4 fair-use cases for using the class and function signatures (e.g., ""source code api"") and even ""binary api"". 3. they ruled in favor of google in all four cases. 4. they upheld that the license headers of oracle remained in the google android source code, unmodified, and thus google did not violate that pivotal part of now-firm contract law. what i got out of it, in revisiting it for this redis relicensing debacle is that redis, ltd's, lawyers who ok'd this license change need to immediately file a claim with their accidental damages, errors and omissions insurance, because they gave very, very bad legal advice. if google hadn't left the oracle copyright + licensing clauses in the java sdk code they used in android os (while they did add a couple lines of their own, such as dictating their own copyright claims), then they would have been in breach of the read-only oracle license and would have been infringing on both the license and oracle's copyrights. put in another way, that's exactly what redis has done on about 90% of its code base. they are deifnitely in violation. and that redict fork is even more egregious, because that guy has no claims whatsoever. if i were him, i'd both immediately delete this repo and hire an attorney. [a link]",0,0,0,0.9207133054733276,0.983242392539978,0.820782482624054,0.0,accept,unanimous_agreement
2020724192,13157,"how so? this kind of sublicense is permitted by the bsd license, and redict isn't removing the bsd license from existing code, as is clear if you look at the additions to the source files. it's described in more detail here: [a link] there seem to be a lot of folks confused about what's allowed and what's not. note that i'm not saying whether this is a good idea is not - that's a separate discussion.",0,0,0,0.9108368754386902,0.9459465742111206,0.8720216751098633,0.0,accept,unanimous_agreement
2020739350,13157,"you can't relicense code that you don't own rights to. that's the end of it. he has absolutely no right to relicense any line of code he doesn't own rights to. in this case, it's a form of outright copyright infringement, since he's making the license more restrictive, and thus infringing on their rights: we ran into this problem a lot in the mid-to-late 2000s with all the inappropriately gpl and lgpl code out there. the industry standardized on using mit / bsd licenses for almost everything. now, 20 years later, we're swinging back into proprietary source land. p.s. for the flipside of this, look at all the controversies in the 2010s of people dedicating their entire lives to building amazing wordpress and drupal plugins and themes, only to be forced and compelled to live by the gpl of wordpress, even tho they didn't use any of its code. inconsiderate leakers would continuously publish (completely legally) gpl forks of the project, for free, and the modders had no ability to sell it or fight back or anything. because unlike the bsdl and mit license, with gpl or lgpl or agpl, you as the author are not able to license new code in a more restrictive way to preserve your rights to sell and fight hostile forks. this si why i had to quit the project i dedicated my life to from 2002-2007, hostile forkers of a gpl project. there is a middle ground and that seems to be the [a link] (very permissive), coupled with the [a link] (that prohibits the project from being used by projects licensed under the gpl and agpl).",-1,-1,-1,0.9002965092658997,0.7689460515975952,0.6855053901672363,-1.0,accept,unanimous_agreement
2020948450,13157,"hello, i'm the one who started the redict project. a lot of people are struggling to understand how this sublicensing works, so i understand the confusion here. however, redict is perfectly in compliance with the redis oss license. permissive licenses like redis oss's 3-clause bsd license are, almost to a generalization, *defined* by the fact that they allow you to do this, in contrast to copyleft licenses. note that redis ltd *also* does not hold the copyright over the redis code (they never obliged contributors to agree to a copyright assignment or cla), and the sub-licensing that redis ltd has done with the sspl is dependent on the exact same legal framework that allows for redict to do so. redict has not unilaterally changed the license, rather we have applied the lgpl to any changes we have made to the project *on top* of the original bsd licensed codebase, and the resulting combined work uses the lgpl. the original 3-clause bsd license is still there and all of its terms (namely attribution and the reproduction of the copyright notice and license terms) are all present and in order. the bsd license can be found under `licenses/` and in every source file's copyright notice, and the repository as a whole has been brought into compliance with the [a link] standard to make the overall licensing situation clear. the [a link] goes into more detail, and i would be happy to answer specific questions here. --- anyway: i came here to mention that redict 7.3.0-rc1 is available and ready for general testing: [a link]",0,0,0,0.8428728580474854,0.5749675631523132,0.6347496509552002,0.0,accept,unanimous_agreement
2021615724,13157,could you provide more details on the decision to move away from github and its impact on the community?,0,0,0,0.9807783365249634,0.986944615840912,0.9906545281410216,0.0,accept,unanimous_agreement
2021876788,13157,"guys, we're almost at 1000 down votes. keep going! ![a link]",-1,1,1,0.7395402193069458,0.875616192817688,0.9920580983161926,1.0,accept,majority_agreement
2022090118,13157,"actually, i'd like to understand too :thumbs_up_medium-light_skin_tone:",0,0,1,0.6247972846031189,0.9805265069007874,0.970005452632904,0.0,accept,majority_agreement
2022163197,13157,[a link]: not sure this needs more explanation?,0,0,0,0.703149676322937,0.9527680277824402,0.8936344385147095,0.0,accept,unanimous_agreement
2022190405,13157,"yup, it's covered on the blog post: imo that's great news :red_heart:. i wish more floss projects would take this direction.",1,1,1,0.9862511157989502,0.9952364563941956,0.9959012866020204,1.0,accept,unanimous_agreement
2022380438,13157,"certainly. forgive me for answering at length. for context and full disclosure, i have a vested interest in projects making use of platforms other than github, since i am the founder of one, namely sourcehut. however, i do *not* have a vested interest in codeberg beyond the fact that it serves all of our interests for the github monoculture to be weakened. i selected codeberg to balance the following constraints: 1. using free software infrastructure to host free software projects is important. 2. codeberg is more familiar to the existing community from github than sourcehut. 3. to reduce the appearance that i am serving my own commercial interests by hosting redict on sourcehut. to elaborate on the first point: this is a traumatic moment for the redis community, but also for the free software community. this has happened too many times: mongodb, elasticsearch, now redis, and many other projects, hell, even solaris, and many projects with commercial stewards are transparently keeping the same strategy in their back pocket with clas and copyright assignments from contributors. we could just repeat history, gather the diaspora, and try to rally behind a hastily made fork, throw it up on github, bsd license it, get the commercial interests on board, and press on like everything is normal. but, i think that this is an important moment to question our values, and how we ended up here, and what kind of changes we need to make to curtail this *trend*. it's obvious that we are upset by this change, as succinctly demonstrated by the 1000+ :-1:'s on this thread. why? is it because we are annoyed by the nuisance of having to set up a fork and change all of our software over, or else pony up for redis sal? or simply because it's a transparent and poorly justifiable cash grab and that offends our sensibilities no matter how it presents? these are valid, but shallow reasons to be upset. i want to look deeper. redis ltd né redis labs has done something which, in my view, is morally wrong, and violates the spirit and social contract of free software. that is the root of the problem. redis ltd did not make redis, they were major contributors but it was built with the hard work of many individuals independently of them, at first antirez, but also individuals representing aws, gcp, and other commercial interests, as well as, most importantly, a community of independent contributors. all of these people hold the copyright to redis in aggregate, and redis ltd's changes are only possible because of the permissive bsd license, but not because of any [a link] to the software. that's not to mention the community of users and downstream maintainers who helped popularize it on the back of the social contract of free software, incorporating it into linux distributions, building more free software that made use of it, and so on, none of which is likely to have happened at the same scale if not for this social contract. so, when we are confronted with this crisis, i want to know: what are our values, and how can we better assert our values to prevent these catastrophes from occurring? we believe in the value of free software, so we should walk the walk. this is a moment for solidarity. we can use our strength to re-enforce free software communities like codeberg, rather than lending our legitimacy to non-free communities like github and slack. it's true that github has popularity going for it, but how can we address that as a community? by putting *more* projects on it? the discomfort we'll face in bringing people to a new platform is a rounding error compared to the discomfort caused by this event as a whole, and there's no better time to reconsider these choices than now. this is a political event, and this is a political solution. this is what has guided my thoughts on the formation of this fork, including the platform it's hosted on, and the license, and so on. this is also why i am calling on you to support a grassroots approach over depending on commercial interests once again. i hope that helps.",0,-1,0,0.4494013488292694,0.9560030102729796,0.8258368372917175,0.0,accept,majority_agreement
2022626960,13157,"i fully support your stance and share the sense of betrayal over redi's actions and the broader trend of companies monetizing and controlling open-source projects. corporate greed will not stop on this project only. so deeper actions are needed to protect the spirit of free software. merely migrating to alternative platforms may not fully solve the problem look at openai, even non-profit entities can be co-opted by big money interests and have their open-source ethos compromised. platforms surviving on donations could face similar risks of being bought out or influenced when they gain enough traction. in my opinion and i am no one except another open source maintainer and an immigrant myself, rather than just migrating to different platforms: - broader political actions at the governmental level are required, like within the eu or any entity capable. enacting regulations restrictions and funding to help safeguard major open-source projects from appropriation by commercial interests. - oversight and regulation central platforms like github are also well needed to ensure alignment with free software principles. this is a call for the open-source community to reassert its values through a multi-pronged effort! grassroots initiatives coupled with concrete regulatory measures are needed for sure. keep running away will not solve the root cause issue!",-1,0,-1,0.3940110802650451,0.7857125997543335,0.6289671659469604,-1.0,accept,majority_agreement
2022635958,13157,"i agree with you, this is but one of many lines of activism and organizing that i am pursing. i set up a [a link] to popularize and document the philosophy and practices of free software late last year, and a few weeks ago i set up [a link] as a part of that to build a stronger community with a focus on free software and political action with respect to free software. please join us and help expand the movement :)",1,1,1,0.9702786207199096,0.9943004846572876,0.9954258799552916,1.0,accept,unanimous_agreement
2022972435,13157,"for these that want to stay on github with the original bsd-3 license, [a link] this is what the main community is gonna be working on.",0,0,0,0.9873637557029724,0.9863541722297668,0.9915892481803894,0.0,accept,unanimous_agreement
2022994196,13157,"i know these things can be challenging to navigate, given differing values between core contributors, but i think it would benefit the entire community if the forking (ahem) contributors could collaborate on a single fork, working through whatever compromises that need to take place. when mysql did this, we ended up with percona and mariadb, which created confusion for both contributors and end-users alike.",0,0,0,0.7821299433708191,0.9327736496925354,0.862669825553894,0.0,accept,unanimous_agreement
2023005224,13157,"the redict camp is committed to our approach and in particular the copyleft license for the reasons i stated above, but we'd be happy to work with anyone that can get on board with that plan. we approached placeholderkv but they aren't particularly interested. the discussion mostly took place here: [a link]",0,1,0,0.6592090725898743,0.9403814673423768,0.6890900731086731,0.0,accept,majority_agreement
2023012653,13157,"it is a fundamentally different set of opinions. we (placeholderkv/new bsd-3 fork) want to keep redis more or less as it was but just keep developing it in open-source, and the redict folks don't. i didn't see a way to reconcile them. drew listed out the detailed discussion.",0,-1,0,0.8465660810470581,0.65013188123703,0.7547317147254944,0.0,accept,majority_agreement
2023062657,13157,"to phrase the same differences in my own words, i would say that placeholderkv represents a small quorum among commercial stakeholders in redis (e.g. aws, gcp, etc) who are interested in resuming business as usual as soon as possible, without being willing to have a conversation about the points i raised in this thread, and the fact that business as usual is what led us to this situation to begin with. placeholderkv is about minimizing the financial impact of the redis re-licensing as it pertains to aws et al, and ensuring that control over redis is maintained in the hands of its commercial interests rather than a broader community. i am disappointed, but not surprised, by this outcome. the only hard requirement we posed to placeholderkv for merging our forks and keeping the community united was the use of a copyleft license, and we proposed one that would meet the needs of all of the commercial stakeholders without imposing onerous compliance regimes (like the agpl, for instance, would have). nevertheless, this was rejected by the commercial stakeholders, and so we are moving forward with an independent, grassroots approach which places a greater emphasis on the needs of the free software community as a whole than on the wishes of a bloc of commercial interests. it is unfortunate that our forks must compete, but i hope for the future of free software that redict wins out.",0,0,0,0.9220646023750304,0.961048424243927,0.95358008146286,0.0,accept,unanimous_agreement
2023132694,13157,i'm disappointed you view it in such a black and white way.,-1,-1,-1,0.983887016773224,0.9791021943092346,0.9898953437805176,-1.0,accept,unanimous_agreement
2023152770,13157,"it sounds like all parties _want_ to come to an accord, but it's been challenging to come to agreed upon compromises. would it be possible for the interested parties to hop on a call to discuss this? sometimes nuance can get lost when communication is held solely via text on a screen. if it would be helpful, i'd be happy to set that up, as a mostly unbiased third party.",0,1,1,0.7388696670532227,0.6235219836235046,0.633389413356781,1.0,accept,majority_agreement
2023219421,13157,"all this pull towards copyleft will just polarize the community into 2 sides, of proprietary software and copyleft software, which is bad outcome (in my opinion). although it is a natural reaction after companies like redis switch to source available licenses, but it's not a reason to destabilize things further playing tug of war with big companies, since it is a lost battle from the beginning (in my opinion). to me true freedom is the permissive licenses, since it the best way businesses and community can work together where community doesn't restrict businesses freedom and vice versa. and if businesses start to pull their way, the best thing is to continue advocating for the freedom permissive licenses give, rather than jumping ship towards copyleft further polarizing community. p.s. after all this ""copyleft activism"", licenses like [a link] start to look more appealing, but it also restricts freedom in a way.",-1,-1,-1,0.8911643028259277,0.6778079271316528,0.7136014699935913,-1.0,accept,unanimous_agreement
2023253445,13157,"i don't see any reason why copyleft should polarize anyone. by all means provide an argument against the use of lgpl if you wish; so far i have heard none. i have spoken at length about why it is important. if i understand correctly, what you are arguing for is a passive freedom, which is the absence of obligations; i am arguing for a positive freedom, which is the guarantee of rights.",0,0,0,0.7537252902984619,0.9161678552627563,0.8736846446990967,0.0,accept,unanimous_agreement
2023297306,13157,"the license is not what is polarizing me. it's the unilateral decision to change it without gathering input. i want to build a coalition of individual and corporate contributors to help maintain a new version of redis. if some people find the license polarizing, i would rather include them in the discussion and decide later. that is what it means to me to build a community around a project. that is what [a link] is right now. there have been successful examples of these types of groups, postgresql is one that comes to mind that has been quite successful. the other comment, which you didn't mention here, was about committing to the name redict, which (again, i'm not a lawyer) sounds way too much like redis to me. i neither want them coming after us with a cease and desist or to cause confusion about what we are.",0,0,0,0.6029631495475769,0.8497230410575867,0.5918483138084412,0.0,accept,unanimous_agreement
2023335256,13157,"i hate to raise our dirty laundry here, but alas, this is politics so eventually we must. you have never participated in a discussion on these questions in a manner which is collaborative and open to external input. the consensus you seem to value is transparently the consensus of the four representatives of commercial interests that you have gathered into an insular group in which the ultimate decision making authority is vested. the input of the community, as it were, seems to be relegated to that of an advisory group. the consensus making process which you cite as necessary to change the license is utterly opaque to us. redict has strongly encouraged collaboration in all matters related to decision making and governance within the constraints that the raison d'etre of the redict fork is to create a community which is protected from the kind of abuse of commercial power that led to us being here today, constraints we distilled into the simple choice of a copyleft license for the future of the project. we have invited you to participate at every opportunity, to adopt positions of authority within the project with deference to your experience and resources, and to help establish processes and in every other respect have a foundational role in establishing what redict shall be. as a matter of fact, , you are currently an admin on the redict organization codeberg, and have since the first time we spoke on the matter a few days ago. we were never extended anything close to this level of respect and willingness to collaborate by the people working on placeholderkv. i don't think that redict has any problems with the redis trademark. if we were adopting a more collaborative approach, perhaps something the commercial stakeholders could offer is the services of your legal team(s) to issue an opinion on the matter.",-1,-1,-1,0.9822761416435242,0.9836339354515076,0.972908854484558,-1.0,accept,unanimous_agreement
2023386885,13157,"a quick google search will yield a lot for and against points for copyleft licenses. i am not an expert in lgpl, and don't have a lot of experience, but there are a few people above who showed examples where copyleft is problematic. also, i am not really anti gpl, i believe that in certain areas it is a fine license, like in linux kernel for example. the whole questions, is about whose freedom you are fighting for, since any restriction to maintain a certain group freedom, restrict other groups freedoms. in this specific example you are advocating for positive freedom of the free software community, but what about businesses, what about people who want to make a derivative work and monetize it in the way they see fit. i am not saying i agree with them, but why should we take their freedom away just because we disagree on something. the whole point of freedom, is that everyone has it, not just a specific group. that's why i like bsd-0 clause license, it's simple, and have pretty much no restrictions at all. since any restriction, restricts someone's freedom by the definition of the word. i was never involved with redis (other than as a user), and is not wright for me to participate in the specific decision makings of the fork, and it's community. the only reason i posted my opinions above, is that this trend as a whole worries me. where a permissive license project goes to source available, and then a fork goes completely other way to copyleft. and a broader discussion of this trend was already started above by some people.",-1,1,0,0.6343404650688171,0.7540191411972046,0.7360630035400391,,review,no_majority_disagreement
2023403080,13157,"i looked through this thread again and i couldn't find any. can you not make any arguments for yourself? this is a very basic understanding of how freedom works. your logic taken to its conclusion is just an argument for anarchy. workers rights are guaranteed by restricting the behavior of businesses. freedom of the press is guaranteed by restricting the government's ability to censor media. the lgpl was chosen with this case in mind. the most common approach to commercializing redis, including the approach favored by the four commercial stakeholders working on placeholderkv, is provided for by the lgpl license without imposing any further obligations on these stakeholders, which is in part why it was selected over more aggressive copyleft licenses like agpl or eupl. by the way, i represent a commercial interest and i make my living by selling software that uses a copyleft license. copyleft does not and has never prevented the commercialization of software.",0,0,-1,0.8745515942573547,0.9340335726737976,0.6365478038787842,0.0,accept,majority_agreement
2023468183,13157,"i'm still not convinced you're for the community. you came up with the name on the spot without consulting the actual community, unlike placeholderkv, which is still taking a lot of input to decide. you decided to use the lgpl without gathering more feedback either and lastly you want to move everyone, devs and users alike to an entire different platform when github itself was never the issue and you are still trying to redirect people there. noone besides the few involved in redict were in favor of that either. all these points represent your personal interest rather than the of the actual community that is here on github.",0,-1,-1,0.6689990162849426,0.8757787942886353,0.885840117931366,-1.0,accept,majority_agreement
2023476291,13157,"i can, but i won't. since i believe this is not the best place for such a debate. and i am not an anti gpl activist (yet), so i don't have required knowledge and time to keep up with this debate. i am just a regular person with some opinions (which i have rights to have) in a **relatively** simple system anarchy can be good, specifically when it allows people with different opinions to work together. and i am **not** talking about much more complex systems such as humanity and governments. and that's a great thing. but my argument was about people who wish to monetize derivative work the way they see fit (that includes ways lgpl forbids/or makes it hard). and personally i don't agree with their methods of monetization, but that doesn't mean they should not have rights to do that. also, it's not like copyleft and gpl specifically have not been abused in the past, red hat drama is great example of that. so to me, it much more important to advocate with projects success rather than play with licenses and laws. people who want to abuse a license will find a way to do that. and the longer and more complicated the license is, the more potential holes it contains. in my opinion it's better to advocate for world where open source is considered the norm by users and big companies, rather than creating complicated licenses to hide behind laws.",1,1,-1,0.7998014688491821,0.9631991982460022,0.4199657738208771,1.0,accept,majority_agreement
2023478409,13157,"for the record, redict has the same number of contributors as placeholderkv at the time of writing, which is 8, all of whom are quite pleased to be using codeberg and agree with the choice of license.",0,1,1,0.7120630145072937,0.942087173461914,0.9479491710662842,1.0,accept,majority_agreement
2023482059,13157,"ah, correction, redict has more contributors than placeholderkv as of about 30 minutes ago; now 9.",0,0,0,0.9780136346817015,0.90203595161438,0.9853581190109252,0.0,accept,unanimous_agreement
2023563875,13157,"just to be precise here, anarchy as in the political governance system is exactly what you are trying to achieve with redict: a community-based system where everyone has equal rights, putting collective weight behind decisions to enforce positive rights and emancipation for the community. it's the thinking that there is no real freedom without a society working all together towards it, not only for the collective, but also for the individuals. what you are talking about, allowing as much individual freedom regardless of others, is libertarianism.",0,0,0,0.5315032005310059,0.8592309951782227,0.9619988799095154,0.0,accept,unanimous_agreement
2023576683,13157,"yeah, i'm aware of the nuance there, i just thought that calling out 's arguments as libertarian nonsense directly would be unfair.",-1,-1,0,0.9487752914428712,0.5731410980224609,0.620969831943512,-1.0,accept,majority_agreement
2023703633,13157,"it's pretty surreal to me that most of the reactions to redis' announcement along the first week have totally rejected the idea of making it unilaterally non-free, even to the point of thinking the bsd-3 license protected us from that move (that sadly it doesn't) or saying github as a platform should prohibit that move, and now when someone brings to the table the concept of a *copyleft* license, whose mere reason to exist (or one of them) is to protect that freedom, suddenly people dislike the idea and start calling it *polarizing*. if we fork and keep the license, it's a matter of time another bad actor uses all the contributor's work for their benefit and privatizes it again. this should be a call to action not just for this project but for every similar foss project.",-1,-1,-1,0.985096275806427,0.9832415580749512,0.9781941771507264,-1.0,accept,unanimous_agreement
2023960784,13157,"well, yes. as far as i'm aware though 's interests are that (a) the new redis fork remains free software in perpetuity, and (b) relies on as little proprietary software as is feasible. as someone who grew up with the proprietary stranglehold over tech in the 90s, i'm astounded to see contributors to free software themselves continue to argue *for* permissive licenses and proprietary technology, even in a thread discussing the manifest harms those have brought to the *very software they contributed to*. surely repeating the same steps as last time will lead to the same result again?",-1,0,-1,0.9442977905273438,0.7127141356468201,0.9276453852653505,-1.0,accept,majority_agreement
2024011357,13157,"are you referring to the choice of license? if you'd advocate a compromise - what would it be? as far as i can tell there *is* no compromise position possible between *gpl and bsd licensing. one is permissive (and preferred by redis' commercial users), the other is not (and preferred by the community - just, as measured by contributors so far). have you a suggestion for what compromise would look like in this case? because i genuinely don't. i don't see how you can water down either a bsd or lgpl license, and i don't see how to reconcile the interests of both groups (redis' old commercial users, and contributors wishing it to remain free in perpetuity).",0,0,0,0.6693226099014282,0.8754552006721497,0.8282661437988281,0.0,accept,unanimous_agreement
2024061735,13157,"just fyi... both projects can co-exist, and share contributions.... you don't have to agree on a license. that being said, this is likely not the time, nor place to discuss. both viewpoints do have valid points, and ideas. granted, agreeing, and making a singular project, would be more beneficial in the long run. but, then, would come the above discussions between codeburg, and github, also- both sides have valid reasons.",0,1,0,0.8578511476516724,0.6643778085708618,0.8341551423072815,0.0,accept,majority_agreement
2024126032,13157,"to clarify, redict can take code from placeholderkv (and we will, if it makes sense to), but not the other way around, because of the difference in license between the two projects.",0,0,0,0.9883727431297302,0.9917358756065368,0.9881436228752136,0.0,accept,unanimous_agreement
2024131870,13157,"the problem wasn't the license, the problem was that a single company had complete control over the project including the ability to change said license. what is trying to do will solve that issue.",0,0,0,0.8937613368034363,0.955702304840088,0.9691573977470398,0.0,accept,unanimous_agreement
2024135476,13157,"this is not true. redis ltd did not hold any kind of privileged position with respect to the copyright of redis and control over its licensing. the same is true of placeholderkv: it offers exactly the same protections over the code as redis did, which is to say none. in fact, redis ltd can pull improvements from placeholderkv to incorporate into their proprietary software even now.",0,0,0,0.9551768898963928,0.9921187162399292,0.9666706919670104,0.0,accept,unanimous_agreement
2024139694,13157,"they owned the trademark to the name of the project and all the material around it. they essentially controlled the codebase regardless of what the license was. from what i see, what madolson is trying to do is create a governance so that way the community/contributors control it, not a corporation who can, will, and has already decided to take their toys out of the sandbox and not share with anyone else.",0,0,0,0.902592122554779,0.9590381383895874,0.8636981844902039,0.0,accept,unanimous_agreement
2024142094,13157,"i think the issue that has with 's proposal, though, is that the aforementioned corporation can just copy any improvements the community creates into their own closed codebase. the purpose of the lgpl license, vs. the bsd license, is to prevent that.",0,0,0,0.9795544147491456,0.9892441034317015,0.96429181098938,0.0,accept,unanimous_agreement
2024146613,13157,"-bayne i don't think the idea of this fork is really to say nobody could do this, but to say ""you don't get free direct work from us, if we cannot compete with you"". sure they don't get to say they offer hosted redis anymore, but they can say they offer hosted placeholderkv which is a fork and drop in replacement for redis, maintained by the same people who maintained redis. the benefit here is that since redis themselves took their toys out of the sandbox, the people doing this fork are the people who built the toys, and work for the companies who can push the new project into the standard, thus making people go ""why would i use redis over placeholderkv which is available at all major infrastructure providers, and redis isn't?""",0,0,0,0.7233237028121948,0.8710922598838806,0.6524475812911987,0.0,accept,unanimous_agreement
2024162314,13157,"but if any redis fork continues to use the bsd license, the redis folks can simply take the community's free contributions, and roll them into their own proprietary version, in perpetuity. if you want to prevent your free contributions from being used in this way, one of the gpl-style licenses is the only way to go.",0,0,0,0.9838148951530457,0.9921980500221252,0.9903144240379332,0.0,accept,unanimous_agreement
2024178706,13157,"yes, that's the point. unless the new license specifically prohibits redis, inc from doing it, then it's going to prevent everyone from doing it, including amazon, alibaba, and red hat. or more importantly known as significant contributors to redis.",0,0,0,0.9529542922973632,0.9641430974006652,0.9708839058876038,0.0,accept,unanimous_agreement
2024230864,13157,"well, no. as far as i can tell, all those companies are in the business of just _hosting_ redis. and besides, they still could very well be taking lgpl or gpl-licensed forks, modifying them internally and offering them as saas without the obligation of releasing sources. only licenses like agpl or eupl try to prevent this. redis inc. on the other hand is in the business of hosting redis, preventing others from doing so and maybe also licensing it to others. this model is still able to take advantage of any permissively-licensed forks, but cannot pull in any changes from copyleft ones.",0,0,0,0.9412567019462584,0.9764972925186156,0.9333470463752748,0.0,accept,unanimous_agreement
2024296252,13157,it's time for a solid fork with custom restrictions banning redislab from ever using new code from new forks. let's starve them,0,-1,-1,0.7844617366790771,0.5882288217544556,0.6084003448486328,-1.0,accept,majority_agreement
2024305857,13157,"i honestly don't care if redis would use code from a new fork. its open source for a reason. we can't become the thing we are bashing. and- blocking redis from pulling commits, really defeats the purpose of the original license we are all upset over them removing.",-1,-1,-1,0.9860625863075256,0.9631733298301696,0.9899293780326844,-1.0,accept,unanimous_agreement
2024333835,13157,"that _particular_ reason, is the reason we are here having this conversation: redis has taken all the hard work from its volunteer community, and re-licensed it under a proprietary license. perhaps this is the core difference between those proposing the use of the lgpl, and those proposing the use of the bsd license: whether or not you see that behaviour as a bug, or a feature.",0,0,0,0.9552816152572632,0.9369480013847352,0.9432790279388428,0.0,accept,unanimous_agreement
2024393466,13157,"fundamentally this ultimately only results in a name change for what we used to call redis (at least for users). the community might fracture for a bit between the lgpl or bsd licensed forks but ultimately one will likely win. this means nothing, and will change nothing, i will literally just change the `redis` label in my container to something else. i will never use defacto redis again, nor will i have to change anything about how i used to use redis.",0,0,0,0.7694473266601562,0.919964075088501,0.6656405329704285,0.0,accept,unanimous_agreement
2024677658,13157,"but that's the whole point of open source (at least to me). we have one group who says aws uses their software to earn money and doesn't share profit, and another group who says redis uses their software and adds changes, and doesn't share source code. is copyleft a solution to the problem we are seeing here, sure, i agree with that. but the different question, is it the best solution? every solution to some problem, creates other problems. and in my view copyleft creates more problems than it solves. another way to solve this problem would be not to publish any code at all, and that's also a solution, yet a terrible one. permissive licenses have a solution to that problem, called creating a fork. and if a new permissive fork goes proprietary, another fork is created and so on. sure you can argue that each time it happens community shatters, and user base too, but in reality it rarely happens more than once within one project. and if it's a particular problem, there are big non-profit foundations, which have some goals and regulation to keep projects in open source in perpetuity (though i might need to do more reading on that one). please note again, that i am not a part of redis developer community, i only wished to participate in the broader discussion about how open source projects moving to proprietary licenses causes a shift towards copyleft. and if it wasn't clear, i would still prefer copyleft to proprietary, but not to permissive. so the most important thing here is what the community chooses. and to me judging by discussion above is just doing that, trying to listen to the community. while playing politics and advocating people for his views, and it is seem to be working quite well. but that's not surprising given the place and the situation here. still i think it is more important for the community to decide on their own, without politics and other. since i have also been advocating for my views, and it was never my intention to influence this community into making a specific decision, this will be my last message in this pr regarding permissive/copyleft debate.",0,0,0,0.5934533476829529,0.6613660454750061,0.7707522511482239,0.0,accept,unanimous_agreement
2024866146,13157,"that doesn't seem correct afaik, at least amazon was mentioned in this thread as providing substantial contributions via an aws developer working on redis on the clock? i'm not sure if that's true or not, i just saw it mentioned here and elsewhere.",0,0,0,0.9327377676963806,0.8232840895652771,0.9691545367240906,0.0,accept,unanimous_agreement
2025023377,13157,"a majority of contributions come from companies hosting databases as as service. they all benefit from upstreaming their improvements, so that their internal forks don't diverge too much from the main project. they all benefit from that. that's why they hire people to do that. talking about ""community"" as if those companies weren't part of it. they are a very important part of the community.",0,0,0,0.569550633430481,0.567419707775116,0.7088582515716553,0.0,accept,unanimous_agreement
2025586419,13157,"that's true. i didn't mention it in my comment because it wasn't really relevant and also because that upstreaming work isn't making them money directly. they can continue to do that just the same regardless of the fork being bsd or lgpl licensed. they can still choose to upstream their changes, but if they have any local modifications they _do not want to upstream_ they can keep them private just as well.",0,0,0,0.9596739411354064,0.96452134847641,0.9811406135559082,0.0,accept,unanimous_agreement
2026418088,13157,"the name of placeholderkv was set to [a link] and it seems to be the primary fork of the community with good support and endorsement by the linux foundation as you can read in the blog at the bottom. ""[a link] will continue development on redis 7.2.4 and will keep the project available for use and distribution under the open source berkeley software distribution (bsd) 3-clause license."" ""industry participants, including amazon web services (aws), google cloud, oracle, ericsson, and snap inc. are supporting [a link]. they are focused on making contributions that support the long-term health and viability of the project so that everyone can benefit from it."" [a link]",0,0,1,0.6293144822120667,0.7319247126579285,0.7933918237686157,0.0,accept,majority_agreement
2026850356,13157,this sentence just makes me confused: have you ever read lgpl? this is the specific use case it was made for ... so that business can use the library in any type of product they wish with any licence they wish on their product while maintaining openness of the library they use. have you ever read lgpl?,-1,0,-1,0.8753517866134644,0.7353264093399048,0.976231038570404,-1.0,accept,majority_agreement
2026851902,13157,you don’t have to add any ideological adjectives on it ... just a noun is sufficient.,0,0,0,0.9588907957077026,0.9536567330360411,0.9861308932304382,0.0,accept,unanimous_agreement
2026951363,13157,"base on votes under your post, you can be wrong about copyleft and community. ;)",1,1,1,0.4908590912818908,0.992721140384674,0.9648833274841307,1.0,accept,unanimous_agreement
2027040271,13157,"votes in this pr are quite subjective, especially in this discussion since copyleft is a natural pushback. also, this pr has a lot of copyleft and fsf activists. that not to say that i am wright, i could be wrong as any of us here. and popularity of opinions doesn't always correlate with what's wright or wrong. furthermore, i find it incorrect to call someone opinion completely wright or wrong, since it all depends on context, and even within a context it is not black and white. according to own blog, he also used to hold similar opinions in the past, which changed over time. and that's fine, i might change my views on this matter over time too, or i might not. i actually used to like gpl license at some point before. but today those are the opinions i hold on permissive/copyleft debate, even if they are unpopular in this discussion.",-1,-1,-1,0.895389974117279,0.9245275259017944,0.530983567237854,-1.0,accept,unanimous_agreement
2027145348,13157,"well, in the end, it seems that the majority will get behind valkey. but if redict exists and is maintained, i think that is very positive. having both a copyleft and a more permissive(?) versions is never a good outcome :) hopefully both of them are completely legal ahah also, it seems to me that the linux foundation blog post officially marks the end of redis labs. that's harsh. signs of the times, maybe?",1,1,1,0.8338078260421753,0.6502113938331604,0.9821603298187256,1.0,accept,unanimous_agreement
2027153571,13157,"the ultimate betrayal of the decade, hope they get sued for breach of contract",-1,-1,-1,0.8881458640098572,0.8198685646057129,0.9926583170890808,-1.0,accept,unanimous_agreement
2027155346,13157,but why?,0,0,0,0.8664745688438416,0.8641574382781982,0.9554293155670166,0.0,accept,unanimous_agreement
2027269660,13157,why not restrict the use/license of name redis:trade_mark: rather than licensing the whole project? like [a link],0,0,0,0.9856034517288208,0.9920864701271056,0.9932222366333008,0.0,accept,unanimous_agreement
2028083057,13157,"go woke, go broke -- bye redis, hello [a link]",0,-1,-1,0.9108238816261292,0.9525775909423828,0.8618884086608887,-1.0,accept,majority_agreement
2029044182,13157,redict - [a link],0,0,0,0.9866794943809508,0.9901903867721558,0.9899312257766724,0.0,accept,unanimous_agreement
2029065766,13157,"[a link] if you prefer the original bsd license. [a link] if you prefer a stronger copyleft license. either option is an improvement over what redis has done here. lets not nick-pick the individual options, and instead, work on moving away from redis. also- the linux foundation is backing valkey. i am going to guess, it will win the redis forking wars.",0,0,0,0.9502182006835938,0.9293500185012816,0.6865948438644409,0.0,accept,unanimous_agreement
2029206579,13157,"not sure, i am very skeptical for a such organization, also valkey it's mostly big companies fork, and redict it mostly community and small contributes fork. as small user of redis i prefer redict, but i understand why big companies try to push valkey.",-1,-1,-1,0.8639875650405884,0.8645383715629578,0.8931760191917419,-1.0,accept,unanimous_agreement
2029613429,13157,"nothing justifies this. very high profile companies invest resources and effort to make this software better. well, keydb is better anyways. goodbye redis.",1,1,-1,0.783619225025177,0.9310935139656068,0.9905942678451538,1.0,accept,majority_agreement
2034665144,13157,how does this have anything to do with “wokeness” lmao. it is corporate greed not wokeness you dork.,-1,-1,-1,0.9449623227119446,0.9474409222602844,0.9656471610069276,-1.0,accept,unanimous_agreement
2034711971,13157,"one thing for you to consider- the developers such as myself, who work for big companies... we get paid to produce quality code. given, there is generally a large financial impact to said companies, based on the performance and reliability of said open source projects- i can assure you my development team will push a lot more high quality code, as opposed to random joe chilling at his house. that being said, i'd elect to stick with the fork, that has quality developers... the ones who understands unit tests, etc.",0,0,0,0.5859388113021851,0.7320157885551453,0.8116798996925354,0.0,accept,unanimous_agreement
2034731398,13157,"redict 7.3.0 is now available: [a link] --- as for who writes better code, grassroots communities or commercial developers... i'll encourage anyone with doubts to browse our code, or our documentation in particular, and let you be the judge. aside: here's the diffstat comparing valkey (top) and redict (bottom) in terms of divergence from redis: ![a link] that's just the core codebase -- redict also has comprehensive documentation and official containers ready to use. if you want to focus on where the development action is, valkey has a long ways to go. don't forget where redis started, either.",-1,0,0,0.5330690741539001,0.8825258612632751,0.5735938549041748,0.0,accept,majority_agreement
2034760872,13157,"very fair. that being said though, instead of comparing the two- we should just celebrate that we are away from redis's proprietary license.",0,1,0,0.5495508313179016,0.9895003437995912,0.5120335817337036,0.0,accept,majority_agreement
2034766922,13157,fair enough. hooray for forks! :tada:,1,1,1,0.984915018081665,0.995053768157959,0.976035475730896,1.0,accept,unanimous_agreement
2035593159,13157,"showing changed files is not a good comparison as valkey was initially focused on discussions about how to move forward and it also depends how significant the changes were. most of the changes that happened with your fork were mostly refactoring rather than features coming from 3 devs by the looks of it, so the comparison is rather misleading. the main support will be on valkey and it will show with time, this is indisputable. even the first line from the release notes made me stop reading any further. you make it sound like as if that wasn't the goal of valkey, which is again misleading. i'm sure there is more if i continued reading but it is getting rather weird how you're trying people to focus on your fork and the words you are using.",-1,-1,-1,0.8006699085235596,0.9085418581962584,0.7315656542778015,-1.0,accept,unanimous_agreement
2036606287,13157,"this is not a slight against valkey, it's a factual difference between our projects. everyone wants to converge towards stability but it is a matter of fact, not opinion, that adding a bunch of new features and undertaking a lot of major refactoring creates churn and reduces stability, or at least makes it more difficult to achieve. this is a trade-off we make when fostering innovation -- and trade-offs are factual but value neutral.",0,0,0,0.7839263081550598,0.9008938074111938,0.845590353012085,0.0,accept,unanimous_agreement
2038418066,13157,no,0,0,0,0.922592043876648,0.8846297860145569,0.9063705205917358,0.0,accept,unanimous_agreement
2038921980,13157,"rip redis, is valkey the future?",0,0,0,0.9876338243484496,0.992051601409912,0.9892532229423524,0.0,accept,unanimous_agreement
2042485893,13157,"sure its, lock redis on the last bsd version in your package manager and wait for a stable valkey release",0,0,0,0.983130931854248,0.984643816947937,0.9888534545898438,0.0,accept,unanimous_agreement
2042804777,13157,redis made the worst decision. there's not much more to say except that [a link] is the future.,-1,-1,-1,0.8014422059059143,0.5374109148979187,0.9855085015296936,-1.0,accept,unanimous_agreement
2042817147,13157,reddit seems innocent:face_with_tears_of_joy:,-1,-1,1,0.7394360303878784,0.9798933267593384,0.9496379494667052,-1.0,accept,majority_agreement
2042839194,13157,better _read it_ twice...,0,0,0,0.9684199690818788,0.9749040603637696,0.9760603308677672,0.0,accept,unanimous_agreement
2043367792,13157,i meannnnn [a link],0,0,0,0.944767415523529,0.959963321685791,0.9875360131263732,0.0,accept,unanimous_agreement
2044807080,13157,i was not redis for this.,-1,0,0,0.7658259868621826,0.5247591137886047,0.9319092631340028,0.0,accept,majority_agreement
2045327681,13157,yeah i did a misstake i obviously mean redis.,0,-1,0,0.8882890343666077,0.6078647375106812,0.8122832775115967,0.0,accept,majority_agreement
2055941051,13157,lol,1,1,1,0.9700573682785034,0.96007639169693,0.8464275002479553,1.0,accept,unanimous_agreement
2065470446,13157,"this bothered me. churn is a bad statistic for gauging productivity, and since i am certain knows that, i thought i'd have a look at what was actually different. if one actually *reads* the diff between head and e64d91c37105bc2e23816b6f81b9ffc5e5d99801, the overwhelming majority of changes are adding `spdx-filecopyrighttext:` comment blocks to the top of every file (to the tune of six lines added per file), juggling metadata files (e.g., removing everything under `.github/`, `.codespell`, etc.), and doing `s/redis/redict/g;s/redis/redict/g;s/redis/redict/g`. that is, the churn is almost entirely fluff. since the changes were made by , meaning that he knows that the amount of churn is mostly fluff, and since he knows that churn is a bad metric to begin with, i can't come up with a characterization of this other than to say it is intentionally misleading and relies on a lack of scrutiny (which, as a hacker, almost feels like a personal insult to me). that made his claim about the number of contributors suspect, so i thought i'd have a look. 85.4% of the commits in redict are from , who has made 88 of the 103 commits as of d2e5e9655, while in valkey, the curve is much flatter, with the top contributor () producing 16.2% of the commits as of cc94c98a9. (number of commits is about as misleading as churn; the numbers are cited only to demonstrate that the claim of open-source contributors stampeding towards redict is also false. you can check this with `git log e64d91c37105bc2e23816b6f81b9ffc5e5d99801..head | awk '$1==""author:""{a[$0]++;t++}end{for(i in a)printf ""%6.02f%% (%d) %s\n"",(100*a[i])/t, a[i], i; print t, ""total""}'` in either repository.) the majority of committers to both projects have a single commit, but only has more than a few commits in redict. to head off any suggestion of a hidden agenda, i don't have a dog in this fight: i'm not pushing either fork, just waiting for the dust to settle, and i have no corporate overlords. i just do not like fud and misinformation, and i think the mudslinging in this thread is shameful. (i do think, long-term, a project probably has a better shot promising community governance rather than being run by someone that launches ad hominems against the other project, to say nothing of the misleading statistics, but looking at the commits, i see a trustworthy friend of mine involved in redict.)",-1,-1,-1,0.9822560548782348,0.9662048816680908,0.9722036719322203,-1.0,accept,unanimous_agreement
2066114551,13157,"i don't think that changed lines is the same thing is churn, and i also don't think changed lines is a measure of quality. what i'm measuring here has more to do with the tangible work that both redict and valkey have had to do in order to create a fork, which is the substantial effort of renaming everything and which necessitates a lot of lines changed. valkey will *have* to complete a similar level of work in order to complete the forking process, and redict is far ahead of them in this respect. that's all i'm pointing out with this comparison. this ""s/redis/redict/g"" work is the bulk of the ""churn"", and it is not actually as simple as running ""sed"" over the codebase. it is a largely manual process which requires attention given to each case; in some respects it can be automated but in order to be done properly it requires a lot of manual attention and labor. it is true that i have written most of the commits per-se as well as most of the lines changed, but as you said these are not a good measure of much. but, importantly, i have done most of the work for *redict*, i.e. the c language implementation of the redict server and client, while other contributors have done most of the work in other respects; equally valuable and necessary work which, as it happens, has also mostly been completed on redict whereas valkey hasn't really started. this includes forking and cleaning up hiredict, which was led by anna, most of the work on the redict containers, which was done by hugo and micke, as well as writing and rewriting redict's comprehensive documentation, which includes both content derived from the cc-by-sa portions of the redis documentation as well as original content written for redict, most of which was led by lucas. redict does have an agenda, though it's not hidden, which is to create a grassroots, community-governed fork of redis with a copyleft license to protect our community from future exploitation. valkey is only ""community"" led insofar as the main commercial interests in redis constitute a community, as they are the sole decision makers and leaders of the valkey community. this is not some kind of slander or ad-hominem, but a value-neutral statement of fact; in fact i acknowledge that some users of redis would prefer the project to be led by commercial interests.",0,0,0,0.7108867168426514,0.8011729121208191,0.8883852362632751,0.0,accept,unanimous_agreement
2066211637,13157,[a link] valkey :heart: redict,0,1,1,0.7741314768791199,0.6185925006866455,0.9634671807289124,1.0,accept,majority_agreement
2066394491,13157,"quoting : as valkey is not changing licenses (yet) and not moving away from github (yet?), no, none of those things need to be done. licensing headers don't need to be added to example scripts at all, and are not a hard requirement to have a viable fork. you know this. i'm aware. it is still not a meaningful change and is not an indicator that valkey is somehow ""behind"", and it doesn't do anyone any good to suggest that. mariadb kept the exported symbols and executable/library names for the sake of compatibility; i imagine redis labs is far less litigious than oracle. there's plenty of value in doing things that way: merges from upstream are easier, people have less to rewrite, and eventually merging forks is easier. what i meant to head off was the repeated insinuations redict's self-appointed bd has been making about valkey; they wouldn't apply in my case. this is disingenuous. they have professed an aim to produce a community-led effort; that's as much as you have said. there's not been time for a community to coalesce. digging trenches and encouraging a rush to judgment is a very bad move, the opposite of trying to get a community to coalesce. the sensible thing to do, if a project actually were community-led, would be to keep alterations minimal until there's some kind of consensus, and there is no consensus until there are enough people for a consensus to be meaningful. it seems like even if they moved to the lgpl, you'd not be interested in merging the forks: is this accurate? (i would like for it to not be accurate, but i suspect that if it were not true, then you'd have said as much already.) it is completely impossible to construe this as fact with a straight face. making various and sundry claims about their motivations is fud, and claiming that this is an objective, verifiable fact is impossible for me to perceive as good faith. it is a fact that some companies that do not like redis's new license have started backing valkey: that is a fact, that's verifiable. you could say it's probable that they prefer a license that lets them continue business as usual, or that they are making a conservative move to the fork that has not changed licenses yet. if you said that, i'd agree that it was probable, though not a fact. several steps past this, declaring that it's because valkey (who seem to have expressed some openness about changing licenses to something more free in this thread) wants all of us crushed under the corporate boot is neither a fact nor probable, but fud. speculating about their motivations is absolutely an ad hominem, and antagonizing them instead of asking is indicative of of nothing good. i could drop the qualifiers on this kind of speculation and start insisting things i say about your motivations are value-neutral statements of fact, but it seems extremely unhelpful to do so. it seems like a better idea to figure out common ground: there might not need to be so many forks. that'd be a much better outcome for everyone, but it's impossible to get to that outcome while you're antagonizing them and producing these ""facts"". (if i'm composing a wish list, i'd prefer whichever fork drops the line-editor and builds cleanly on plan 9, but i'm fairly certain i'm in a small enough minority that i'd have to do the latter myself. on the other hand, i suspect that more people would agree that one fork would be more manageable than n forks that all hate each other--some of which have introduced intentional breaks in compatibility--and probably that lgpl would be a preferable license.)",0,0,0,0.8591513633728027,0.5505135655403137,0.927133560180664,0.0,accept,unanimous_agreement
2067103668,13157,i could summarize this in a single word `sabotage`,0,0,0,0.9498496651649476,0.9932076334953308,0.9586585164070128,0.0,accept,unanimous_agreement
2314787666,13157,rip,0,0,0,0.9615032076835632,0.9172170758247375,0.938078999519348,0.0,accept,unanimous_agreement
2328537955,13157,"oh, so elastic admits it was a mistake after all... who would've guessed? :smirking_face: [a link]",-1,1,1,0.80910724401474,0.6656832098960876,0.9653661251068116,1.0,accept,majority_agreement
2328551640,13157,damage is done. if there's one thing that is hard to regain is trust. i have already moved on to [a link],0,0,-1,0.6333580613136292,0.8523149490356445,0.9306773543357848,0.0,accept,majority_agreement
2328693902,13157,"that is neither what we said nor as simple as something being right or wrong — it's much more actions and reactions. [a link] has a more nuanced take from someone who was at aws when the opensearch fork happened — if you're actually interested. it also touches on redis and valkey. ps: having been involved in one relicense and following others closely, it's interesting how the context is still so different in each.",0,0,0,0.5424674153327942,0.94352126121521,0.9215763211250304,0.0,accept,unanimous_agreement
2328821297,13157,"thanks for the link. like with redis, the trust issue goes beyond resolving trademark disputes with aws. moving 'kind of' away from open source initially made it seem like control was prioritized over collaboration. returning now is a step, but with opensearch and valkey growing, it’ll take more than a license change to fully rebuild trust in the community. i hope we can all focus on open collaboration moving forward, rather than fracturing the community again.",1,1,1,0.9452416896820068,0.9499715566635132,0.9690908193588256,1.0,accept,unanimous_agreement
2332814366,13157,"i think that comes back to the different context to some degree: for elastic >95% of the code came from elastic employees and while open for collaboration, it was never an open governance (and that's also a separate topic than open source licensing). anyway, i don't want to derail the discussion around redis. i'll just clarify when i see us mentioned.",0,0,0,0.96068674325943,0.946077823638916,0.9825547337532043,0.0,accept,unanimous_agreement
2352742702,13157,"i would be really pissed off. crossed fingers, you will solve this and get your rights. and i wish redis happy rebasing.",-1,-1,-1,0.9890562295913696,0.9901156425476074,0.9926902651786804,-1.0,accept,unanimous_agreement
2352984443,13157,"the bsd license _is_ what lets them do this. since anyone can use such code in projects licensed differently, they can use these contributions in a proprietary library. yes, they still have to obey the license terms (i.e. keep the original license text around[0]) and everyone can still use those files using the old license[2]. but the new changes copyrighted by redis ltd. are not bsd-licensed. if you don't like it, you should look into share-alike (aka copyleft) licenses like the gpl or the mpl. 0: they do. see the file `rediscontributions.txt`. as for retaining ""above copyright notice"", the only ones removed[1] were either from salvatore or from redis ltd. employees and they presumably got their permission to do so. 1: as far as i can see, all other copyright notices are left intact. to be extra safe they even left the whole license text in the files that contain those. 2: the easiest and legally safest way to do this is to only look at the commits prior to 0b343969.",0,0,0,0.9091309905052184,0.9721876978874208,0.9633779525756836,0.0,accept,unanimous_agreement
2353415020,13157,"yup, but those bsd-3 parts of code should be marked as bsd-3. only the new code is under the new license. except for the code where the authors *explicitly* allowed relicensing (not sure about this part). the fact that it's quite inconvenient is at the side of the one who did the rug pull. 725 contributors, i bet they didn't ask a single one about the relicensing. and just deleted their licence under which they published their work.",0,-1,0,0.834100067615509,0.8577592968940735,0.6499494910240173,0.0,accept,majority_agreement
2354132975,13157,"do they? i don't see any such clauses in the license. it does mandate that both the license and any copyright notices have to be retained when distributed in source form, so i guess one could argue that it has to be within the same file. but that's not really stated so we can only know if a court rules one way or the other. but even if they had to keep it in all those files, they could simply prepend something like this to each one [code block] and call it a day. they definitely do not have to keep their new proprietary code in separate files. that's what the mozilla public license had to be created for. they are sub-licensing that code. it's a general consensus that you can sub-license permissively-licensed code under any license you want.",0,0,0,0.948040246963501,0.9403157830238342,0.947868287563324,0.0,accept,unanimous_agreement
894797972,9320,"hello , this looks like a big chunk of work - thank you! i only quickly looked at the code and plan to do so more in-depth in a few days, but i wanted to ping /core-team about this. i think we need to agree on the supported use cases and scope here, and the kind of problems that can and cannot be solved by pluggable connection modules. there are many constraints that will make it difficult to support any arbitrary configuration: * configuration assumes a single tcp, tls, unix socket listener * cluster bus protocol assumes a single tcp and tls listener * integration with different channels (clients, replication, cluster bus, sentinel) based on that, use cases that could reasonably be supported are: * single (optional) instance of a tls module, for all channels (this pr) * arbitrary (optional) instances for user-defined connection types, only used to accept client connections. i assume the future rdma transport could fall here, as well as others. i think we need to agree that supporting the ""all channels"" for arbitrary connection types is out of scope for now. another issue i noticed with this pr is that it implements a dedicated mechanism for loading the module. i lean towards using the existing modules mechanism, even if this kind of third party modules have a surface area that's much bigger than the redis module api. would be happy to hear other thoughts.",1,1,1,0.9587882161140442,0.983890950679779,0.9931715726852416,1.0,accept,unanimous_agreement
894805688,9320,"just to be clear, you mean that for client connections we can support multiple plugins, even working side by side, but these won't (at least in this stage) integrate with cluster bus and replication channels. and on the other hand, the only plugin that integrate with all channels, is specifically a tls plugin, and also we only allow one at the time, so someone can replace the openssl one with an s2n one, but can't have them both loaded, and can't use that interface (all channels) to implement any other extension (other than for tls). regarding the module loading interface, i actually believe this mechanism should be completely separate from the current redis modules one, i don't wanna see module capabilities or limitations get mixed with the capabilities and limitations of this mechanism.",0,0,0,0.980083703994751,0.9843407273292542,0.9913386106491088,0.0,accept,unanimous_agreement
894916621,9320,"sure, i'm looking forward to seeing it.",1,0,1,0.949445903301239,0.9700681567192078,0.9535459876060486,1.0,accept,majority_agreement
908588894,9320,"it makes sense to split it into several stages, and review each separately, but then i may be worried that we're not seeing the end goal properly when working on stage one, so i'm also willing to consider splitting to several commits, each with a clear separate purpose. not sure which approach is better in this case, i leave it to you to decide.",0,0,0,0.6135610342025757,0.6127378344535828,0.9023582935333252,0.0,accept,unanimous_agreement
912462600,9320,"hi, before the next stage, i guess reviewing the abstract part seems also necessary, it changes a lot. how about reviewing this feature patch by patch in this pr, and merge it until the full job done?",0,0,0,0.9617639183998108,0.9713894128799438,0.9841209053993224,0.0,accept,unanimous_agreement
919091375,9320,"hi, , i have fixed several problems as your comments. `fully encapsulate connectiontype` & `tls-load-extension` are ready to review, and the performance test seems ok. i'm a little confused about `consider if/how additional arbitrary connection types can be supported`: currently before using a connection type, we must declare the type(conn_type_socket and conn_type_tls), so an arbitrary connection type can't be loaded without the type define. but it's can be implemented by this mechanism: * remove type define from connection layer * introduce type string ""socket"" & ""tls"" in each connection type * connection layer keeps a linked list to store all the types should i take the next step to separate connection.c into connection.c(abstract layer) and socket.c?",0,0,1,0.7686992287635803,0.5835169553756714,0.7978519201278687,0.0,accept,majority_agreement
944306255,9320,"the main part of this job is almost complete. and base these patches, the rdma feature([a link] also supports built-in/extension mode, the features of rdma have been implemented: - rdma server built-in & extension mode work fine - replication works fine - cluster also works fine could you take a look at this pr?",0,1,1,0.9376224279403688,0.5691779851913452,0.708862841129303,1.0,accept,majority_agreement
1137927591,9320,"hi, -steinberg i reworked this series, and separated a large patch into 9 small changes: implement a complete connection framework, redis could hide all the connection related functions, uplayer accesses the connection via framework only. currently, for both socket and tls, all the methods of connection type are declared as static functions. i would appreciate it if you could review this series!",1,1,1,0.980307400226593,0.9863451719284058,0.9900407195091248,1.0,accept,unanimous_agreement
1141600005,9320,ping -steinberg,0,0,0,0.9793009757995604,0.8892050981521606,0.9583277702331544,0.0,accept,unanimous_agreement
1142512213,9320,"i'm really sorry for not responding sooner (wanted to wait till i have a chance to look at the code) we're still in the vortex of redis 7.0 (trying to quickly fix issues in 7.0.0), i hope one of us will be able to dive into it in the coming weeks and give provide the necessary feedback to push this forward. we haven't forgot, just still too busy.",-1,-1,-1,0.9887095093727112,0.9887691736221312,0.9930875897407532,-1.0,accept,unanimous_agreement
1142779360,9320,"some guys contacted me to know more about the test result and the plan of redis over rdma, this makes me anxious to do this work. sorry to let you feel pushy, that was not my intention. thanks a lot!",-1,-1,-1,0.6639527082443237,0.982579231262207,0.6829270720481873,-1.0,accept,unanimous_agreement
1155133437,9320,"according to your suggestion, i did the changes except: - instead of adding a forward declaration of tls functions, move 'connectiontype ct_tls' to the end of source code to avoid chunk of changes. other changes: - rebase code, based on commit ffa007704122a3e4947252f3dbbef3e2be4b6033. - define conn_addr_str_len as 128, remove 'todo' in code. - remove 'fd_to_peer_name' & 'fd_to_sock_name', use 'remote' instead. - add comments for new functions: conntypeinitialize, conntyperegister, conntypeconfigure, conntypecleanup, conntypecleanupall. - add conntypecleanupall() to cleanup all connection types for [a link] - move 'conntypeinitialize' after moduleinitmodulessystem(). - use 'reconfigure' instead of 'force' in 'tlsconfigure()'. - remove extra space in 'tlsconfigure'.",0,0,0,0.9808385372161864,0.9956939220428468,0.9880515336990356,0.0,accept,unanimous_agreement
1155146114,9320,"thank you for the adjustments and the detailed list of changes. in the future, if you have to rebase, please force-push it in a separate push, so that we can click on github's `force` link and see the actual changes you did, ignoring the rebase ones. in this one your list of changes was sufficient, so no harm done 8-) please also mark the resolved comments as resolved, and let's figure out what we wanna do next.",1,1,1,0.9686619639396667,0.9692749977111816,0.991302251815796,1.0,accept,unanimous_agreement
1155886386,9320,"hi, i'm not sure if i hit a bug of github ... when i write comment after this [code block] github always shows i started a review on myself ... so i have to resolve this, and write comment here: [code block]",0,-1,0,0.6248365640640259,0.8417190909385681,0.7069117426872253,0.0,accept,majority_agreement
1156075739,9320,"not sure about that gh issue, but let's put it aside and try to move on. i wrote that comment before i saw your other branch and wrote the review summary [a link], so it was somewhat outdated. reading that again and re-thinking, i think i would like to add more content into this pr. * move the listen and accept logic into the extensions * have an array of extensions were we loop on them to listen instead of hard coded calls to each. * don't have a hard coded indexes to the extensions, instead we can have a global variable holding the index (e.g. for the tls extension) that's set at init time. * allow tls to be compiled as dynamic lib and loaded at runtime. what i would not add at this time is: * any rdma specific code. * a way for an extension to add configs (meanwhile, the tls configs will remain hard coded) * any change that would break the existing tls configs. i hope i'm not rushing too much content into this pr, would love to hear feedback. i feel that as long as we just refactor things to be more generic without adding or changing anything user facing, it can fit here. we should still attempt to avoid unnecessary code re-location, or if we do that, keep that to a separate commit that only moves code without modifying it, so that reviewing this one commit at the time is easy. the only exception about the above (not changing anything user facing), is the runtime loadable extensions (which does add a config and a makefile option), but i think we can let it slide so that we can better judge this pr, but we can also leave it out for the next one. please let me know what you think.",0,0,0,0.8928638696670532,0.595755934715271,0.9006611108779907,0.0,accept,unanimous_agreement
1156123194,9320,"hi, oran this comment is quite clear, i know the goal of this pr, thanks! the only question is that: i have a plan to drop the index of connection types, use a list to store all the types, and search the connection type by string(ex, ""socket"", ""tls""), what do you think about this?",1,1,1,0.9740232825279236,0.9884938597679138,0.9931535720825196,1.0,accept,unanimous_agreement
1156135630,9320,"i think that considering that we don't expect to have more than 4 extensions, maybe we can start with an array rather than a list (faster index), and that in the few places that we need to search for a specific item in that list, i'd rather avoid the string matching, and instead find the right index (using string matching) at init time, and then save a global with that index. but i guess it'll be just the same if we save a pointer directly to that connection class instead of save an index, so the fast indexing doesn't matter. so to reply to your question, do what feels natural, but avoid iterating on the list at runtime doing string matching, and instead cache the element or it's index in a global for use in the few places that specifically require a certain class (we shouldn't have many of them).",0,0,0,0.947986900806427,0.9492784142494202,0.9714593887329102,0.0,accept,unanimous_agreement
1163008128,9320,"hi, i pushed another 7 commits to achieve the goal as you mentioned. the detailed changes is described in the commit message. i also take a further step to abstract the unix socket connection type, and it's possible to build it as a shared library too. if this is not reasonable to you, i'd remove this change. every commit in this pr is tested by: [code block] and since connection extension get supported: [code block]",0,0,0,0.9689950942993164,0.955623984336853,0.8616753816604614,0.0,accept,unanimous_agreement
1165290265,9320,"hi, there is a conflict in src/cluster.c. if it's an obstacle to review, please let me know, i'll fix this and force-push a new version.",0,0,0,0.970547378063202,0.9366025924682616,0.9679564833641052,0.0,accept,unanimous_agreement
1169598282,9320,"hi, i fixed several problems as your suggestions, and force pushed. there is still a conflict in src/cluster.c. i did not rebase code to avoid unnecessary changes(i did tests on my local server).",0,0,0,0.9783092737197876,0.9604541063308716,0.9281534552574158,0.0,accept,unanimous_agreement
1170817510,9320,"hi, i force pushed twice. the [a link] fixes the problems as you suggested. the latest version re-bases the code from the latest commit from unstable branch to fix the code conflict. thanks for your review suggestions!",1,1,1,0.9709320068359376,0.9911120533943176,0.991357445716858,1.0,accept,unanimous_agreement
1173626347,9320,"hi, i force pushed a new version as you suggested, and leave tls part to do in the next version(from your and yossi's feedback).",0,0,0,0.9855334162712096,0.9666776061058044,0.992588996887207,0.0,accept,unanimous_agreement
1174671595,9320,"hi, i append this [a link] to hide tls specified methods, use conncontrol to call connection type methods. could you please take a look at this idea?",0,0,0,0.98057621717453,0.979399561882019,0.992334246635437,0.0,accept,unanimous_agreement
1174731038,9320,"hi, during developing this feature, i have the same feeling with you. even to the unix socket, it also has a `bind` address, ignore the `port` ... base on the latest code, rdma extension works fine: it take the argument like this `./src/redis-server --connection-extension src/redis-rdma.so bind=xx.xx.xx.xx port=6379`(this is almost ready to push, if you are interested in this, please let me know, i'll push it to another branch). of cause, rdma may need more arguments in the future to optimize performance.",1,1,1,0.60079425573349,0.9289718866348268,0.7939270734786987,1.0,accept,unanimous_agreement
1174742765,9320,"yeah, i realize you used the startup argument for now. i think it'll be nice to expose better integration with configs so they can be changed / queried at runtime, and i assume some extensions will need additional configurations (like tls has). but i'm now under the understanding that the only reason we need the ctrl_tls_set_config interface is because the tls configs are built into redis, and we extract the extension code. so this means other extensions won't need a similar interface.",0,0,0,0.9147839546203612,0.8383787870407104,0.9336951971054076,0.0,accept,unanimous_agreement
1174806924,9320,"`ctrl_tls_set_config` can be done in the callback function during tls config change. but `ctrl_tls_get_ctx, ctrl_tls_get_client_ctx & ctrl_tls_get_peer_cert` do not come from the config, so i guess we still need an interface to get these. frankly, i don't know other extensions need the similar interface or not. from my understanding, if the user context of a connection type is simple, it may not need this(to store the context in the `connection` is enough); if the user context is complex, it probably need this.",0,0,0,0.9247968196868896,0.9427734017372132,0.9833314418792723,0.0,accept,unanimous_agreement
1175676468,9320,"hi, could you please give me any hint about the next step? should i revert the last [a link]?",0,0,0,0.9816597104072572,0.987622857093811,0.9864938259124756,0.0,accept,unanimous_agreement
1181539080,9320,"hi, sorry for not replying sooner. in fact, i implemented a version in another [a link] , it supports command like `config set rdma-port 6380` to rebind on a new port. it may be a choice. showing connection types is reasonable to me too. i will support it later(i'm on vacation this week, i will support this feature asap in next week).",-1,-1,-1,0.9907050728797911,0.9810211658477784,0.98329895734787,-1.0,accept,unanimous_agreement
1181616832,9320,"thanks take your time.... p.s. i took at look at the branch you linked, what you did will only allow interacting with the config command, not with the config file (and i'm not sure what'll happen after rewrite). i.e. when loaded from an extension, the configs are registered after the config file was already parsed. considering that we already have the mechanism we added for modules, i feel we should use it.",1,1,1,0.9547686576843262,0.948104977607727,0.9805431962013244,1.0,accept,unanimous_agreement
1181645923,9320,"ok, i will try/test as soon as this weekend. (i left my pc home, currently reading code by a mobile phone is quite hard...)",-1,-1,-1,0.9636659622192384,0.6950429677963257,0.904844045639038,-1.0,accept,unanimous_agreement
1189718650,9320,"hi, i force-pushed a new [a link] as you suggested, `info server` shows like [code block] i'll fix the conflict when you make the decision about connection extension.",0,0,0,0.9841235280036926,0.9718653559684752,0.9899659156799316,0.0,accept,unanimous_agreement
1197536392,9320,"hi, i force pushed 16 commits: - fully abstract connection except dynamically loadable tls library. - fix conflict against the latest unstable branch with only one change: connpeertostring -> connsocketaddr. see [a link]. i would be appreciated it if you could review this change. and during i try to build the tls as a module, i hit several problems: - the connection type libraries must be loaded before the listeners setup, it means that we need to call moduleloadfromqueue() early than initserver(), and monotonicinit() is needed by module system. - the tls connection type is expected to build into 2 types(ignore build_tls=no): `build_tls=yes, build_tls=mod`. if we choose `build_tls=yes`, tls.c needs `#include ""server.h"" #include ""connhelpers.h"" #include ""adlist.h""`; if we choose `build_tls=mod`, tls.c needs `#include ""redismodule.h""` only. something conflict from the two case: [code block] this means that we must remove server.h from tls.c to build it into a redis module. to reproduce this problem, please apply the following diff code on this branch: [code block] could you please give me any hint about this?",0,0,1,0.5743128657341003,0.5597673654556274,0.9291452169418336,0.0,accept,majority_agreement
1198585779,9320,"thank you. i've sorted this mess out (actually it's even messier, but works), and pushed a commit with your code and my adjustments. i see we already have one minor conflict, but it's easy to solve, i'll leave it for later. please have a look at my commit and commit comment for a summary of what i did. note that the changes you made in server.c were problematic. first you moved many things that used to be after daemonization to be before it, these things need to be done with care. secondly, we can't afford to load the modules before initserver (many things will crash), so instead i just extracted the listener creation to a separate function to be done after the modules are loaded. i've added some poc code to tls.c to test that the various module apis are working and we don't get funny compilation or link errors due to the tricks i did in redismodule.h, but these needs to be either reverted or put to a good use. besides that, i think this commit needs to be amended with some of the other loadable extension things you reverted (like readme file and probably others)",0,1,1,0.4225594103336334,0.9291499257087708,0.964222013950348,1.0,accept,majority_agreement
1199088330,9320,"hi, i force pushed a new version to fix conflict. and a new [a link] to introduce ""bootup-only"" & ""deny-unload"" for redis module option. until this commit, all the auto-test works fine for both tls mode and regular mode. then i modified the latest code you pushed, changes in this [a link]: * config tls after initialization of listeners. * init cluster after initialization of listeners. * set tls module options as ""bootup-only|deny-unload"" * add information in tls.md and auto-test code. an issue can be reproduced by `./runtest-sentinel --tls --tls-mod` because sentinel timer works before initialization of listeners. during i developed this part, i realized that moving code in server.c may probably be problematic by myself. so could you please continue this part or give me any hint?",0,0,0,0.904388427734375,0.9850152134895324,0.9423435926437378,0.0,accept,unanimous_agreement
1203085606,9320,i think it makes more sense to support loadable tls in sentinel.,0,0,0,0.981248140335083,0.9701724648475648,0.9818814396858216,0.0,accept,unanimous_agreement
1203128947,9320,"ok.. it means sentinel supports modules.. including registering commands, etc.",0,0,0,0.981702983379364,0.9703485369682312,0.992278814315796,0.0,accept,unanimous_agreement
1203678442,9320,"hi, the [a link] fixes the previous problem you mentioned. the [a link] fixes the compiling error for hiredis. there are something remained: `./runtest-cluster --tls --tls-mod` fails because of `""skipping diskless-load because there are modules that are not aware of async replication.""`, i'm not familiar with this background, so i did not add code blendly like `redismodule_setmoduleoptions(ctx, redismodule_options_handle_repl_async_load);`. (and yes, adding this will lead auto-test passed). ci build on centos7 fails, `redismodule.h:759:30: note: previous declaration of 'redismoduleio' was here` seems to be related to macro definition.",0,0,0,0.9510768055915833,0.9612662196159364,0.9694715738296508,0.0,accept,unanimous_agreement
1203738481,9320,"i'm looking into that compilation issue on centos, i'll let you know when i realize how to resolve it. regarding redismodule_options_handle_repl_async_load. maybe the code in moduleallmoduleshandlereplasyncload should be modified to only check modules with registered data types? or with registered commands? wdty? i.e. a module that doesn't care about data should not fail async diskless loading.. shall we still require it to declare that flag?",0,0,0,0.97426038980484,0.979141891002655,0.9118279218673706,0.0,accept,unanimous_agreement
1204099432,9320,i force pushed an update to the last commit to resolve the problems with gcc 4.8.,0,0,0,0.9878863096237184,0.9915707111358644,0.9929391145706176,0.0,accept,unanimous_agreement
1204963788,9320,"hi, it seems that we have only one thing(about redismodule_options_handle_repl_async_load) remained, could you please give any hint about the next step?",0,0,0,0.9853980541229248,0.987824559211731,0.978959619998932,0.0,accept,unanimous_agreement
1204991051,9320,"let's mark it with that flag. we may conclude that the current logic in moduleallmoduleshandlereplasyncload is wrong and change it as part of another pr. it'll just mean that this flag this pr adds will be excessive, and not harmful... so with that final change this pr is ready for merge? if so, could you please skim though the changes and make sure everything is specified in the top comment or commit comments. when i'll ask the team to approve it, they're likely to skip reading the code, but they should be aware of the implications of what we changed, any unrelated or dangerous changes, and all interface changes. in that respect, it may be enough if the top comment just describes the interface changes, and directs people to read the commit comments for the technical ones.",0,0,0,0.7682809829711914,0.9527830481529236,0.9388996958732604,0.0,accept,unanimous_agreement
1205059500,9320,"hi, i force pushed the latest version, and modify the main changes in the top comment.",0,0,0,0.9883070588111876,0.9818794131278992,0.9940603375434875,0.0,accept,unanimous_agreement
1205143557,9320,"thank you. i edited the top comment and added a few more details, ptal. besides, i now remember that we wanted to make some effort to prevent people from loading the tls module into a different redis version / build. i think about including release.h and version.h and matching them against the values we'll get from the functions in release.c. can you look into it?",1,1,1,0.9511793851852416,0.9826615452766418,0.9749710559844972,1.0,accept,unanimous_agreement
1205144104,9320,"/core-team this pr is ready for approval and merged, please comment.",0,0,0,0.9659385085105896,0.6449519991874695,0.9881181120872498,0.0,accept,unanimous_agreement
1205747796,9320,"it seems to make sense. if not urgent, i can make a pr next week to loosen the restriction. right now it should fallback to disk replication.",0,0,0,0.9681620597839355,0.9854920506477356,0.9650872945785522,0.0,accept,unanimous_agreement
1205976199,9320,"hi, since connection type gets driven by redis module subsystem, i think we have several ways to implement this: * `redismodule_init(ctx,""tls"",redis_version_num,redismodule_apiver_1);` tells the module version to redis server, then redis server can test this, and allow/refuse to load it. this would be a common method, and other modules can also use this. if so, i prefer a followup pr. * compare `redis_version_num` with `redismodule_getserverversion()` in tls module. if so, i prefer a followup commit. which one do you prefer?",0,0,0,0.9404505491256714,0.9763889908790588,0.9846829771995544,0.0,accept,unanimous_agreement
1206150331,9320,", i had a discussion with and we concluded we rather keep things as they are. the example case we had in mind is a module that doesn't register any data types, but does have a command that accesses basic type keys, and keeps some cache of it's results in a global. such a module (although being an abuse), will need to properly handle the new hooks we created for the async loading mechanism (despite not registering data types). i don't think we need to modify the module api for this. this ""capability"" is something only needed for modules that include server.h and are built as part of redis, so i'd rather that extra check be done directly against it's internals. i force pushed an update to the last commit to perform that check.",0,0,0,0.9481772780418396,0.9764465093612672,0.935007870197296,0.0,accept,unanimous_agreement
1209290180,9320,"hi, i pushed a [a link] which is based on the [a link] you pushed, it works fine!(redis-benchmark works fine, and config get/set rdma.port&rdma.bind works fine) i think it's a good hint, module based connection types seems good!",1,1,1,0.97995126247406,0.9884868860244752,0.9952812790870668,1.0,accept,unanimous_agreement
1210183655,9320,"thank you. i think we're nearly ready to merge this. let me know if you think there's anything still missing. maybe the last thing that's missing is to add it to the ci. i made some changes which i'm testing in my repo, i'll squash them into the last commit if they work. [a link] ci runs: [a link], [a link]",1,1,1,0.9351478219032288,0.9266099333763124,0.9741989374160768,1.0,accept,unanimous_agreement
1210219943,9320,"hi i have nothing missing in this pr. thanks! by the way, do you have plan to support tls module only(remove build_tls=yes in the future)?",1,1,1,0.9737045764923096,0.994808793067932,0.9963194131851196,1.0,accept,unanimous_agreement
1210231537,9320,"i don't think we'll want to remove `build_tls=yes`, since we have it working it seems it would be an unnecessary breakage for people who already use it in this way. is there any reason why we would want to do that? p.s. i see quite a lot of failures in the links i provided above. some are really unclear, trying to figure them out. but there are also some issues reported by valgrind, maybe you can handle them?",0,0,0,0.7361944913864136,0.8456182479858398,0.9541059136390686,0.0,accept,unanimous_agreement
1210256285,9320,"rdma connection type supports module only, the changes in makefile is small and easy. originally, i think removing `build_tls=yes` makes tls.c&makefile clear, keeping the two style is also fine to me. i'm using debian, i need to setup the same environment to reproduce issues, so it may takes a long time. i'll try my best to fix these.",0,0,0,0.8180615305900574,0.8434887528419495,0.7440798878669739,0.0,accept,unanimous_agreement
1210287208,9320,"should be as simple as [code block] (i'm not sure if all / other the reports are due to the same issue, or additional ones)",0,0,0,0.9858328104019164,0.9808257222175598,0.988899290561676,0.0,accept,unanimous_agreement
1210812686,9320,added a few more fixes to the last commit: [code block],0,0,0,0.9876119494438172,0.987210750579834,0.9899583458900452,0.0,accept,unanimous_agreement
1211638942,9320,"hi thanks for your suggestions! i force pushed a new [a link], fixed the coding style.",1,1,1,0.972746968269348,0.9843167066574096,0.9902504682540894,1.0,accept,unanimous_agreement
1217887558,9320,"hi i pushed a single [a link] as your suggestion. if squashing this into the previous commit looks better, please let me know.",0,0,0,0.9591622948646544,0.9487297534942628,0.973774552345276,0.0,accept,unanimous_agreement
1222031240,9320,"hi the first [a link] fixes: * revert `.control` method * run tcp sentinel when make build_tls=module * test sentinel flag in tls module the second [a link] changes a little, just fixes the conflict in server.c.",0,0,0,0.9871838092803956,0.9917134046554564,0.9674473404884338,0.0,accept,unanimous_agreement
1222422966,9320,full ci: [a link],0,0,0,0.9845400452613832,0.8597871661186218,0.9959890246391296,0.0,accept,unanimous_agreement
1222847131,9320,summary of test failures (as far as i can tell): 1. `lmap` isn't supported on tcl 8.5 (fails on macos and centos) 2. some connection issue in `tests/integration/aof.tcl` which fails in all libc malloc based tests (valgrind and sanitizer included),0,0,0,0.9834438562393188,0.9932534098625184,0.9809380173683168,0.0,accept,unanimous_agreement
1223492757,9320,"hi, i force-pushed a new [a link] to fix crash when `make build_tls=module && ./runtest-sentinel --tls-module`. about the ci fail issue, i noticed `./runtest --valgrind --no-latency --verbose --clients 1 --timeout 2400 --dump-logs --single integration/aof` fails because: * the vm is a little slower than a bare metal server. (this test usually succeed on a bare metal server) * running in valgrind uses more time. (this test usually succeed without valgrind) * during testing aof, `server_is_up` is ignored. because running valgrind in a vm, client has a chance to try to connect an not-ready port. the following change makes the test always succeed.(i'm not familiar with tcl, this is absolutely wrong, just the easiest way to test...) [code block]",0,0,0,0.6897785663604736,0.9427005648612976,0.98166161775589,0.0,accept,unanimous_agreement
1223832608,9320,"please have a look at my last batch of [a link] other than fixing the test failures with old tcl and the test framework race, i also made sure that when redis is built with tls as a module, it will not link against the tls libraries (only redis-cli, redis-benchmark, and the redis-tls.so will, but not redis-server and thereby redis-sentinel, which is the same binary). new ci run: [a link] [edit] some test fail on `master and slave consistency with expire`, that's a known issue in `unstable` [edit] triggered another ci batch for the ones that failed: [a link]",0,0,0,0.9825291633605956,0.9895211458206176,0.9920777678489684,0.0,accept,unanimous_agreement
1223862800,9320,this looks good to me! thanks!,1,1,1,0.9895099401474,0.9956369996070862,0.9966097474098206,1.0,accept,unanimous_agreement
1225220438,9320,"wow... just slightly over a year after the pr was opened, and nearly 1.5 years after the original rdma pr was submitted, this journey is finally complete (or maybe it's just it's first step?). thank you for pushing this through!",1,1,1,0.9868903160095216,0.9846405982971193,0.9951144456863404,1.0,accept,unanimous_agreement
1225248746,9320,hi thanks for your suggestions and patience in this journey!,1,1,1,0.9715726971626282,0.9383721947669984,0.9640162587165833,1.0,accept,unanimous_agreement
991876178,9934,"this code change passed all tests on my macos but now fail on linux: ![a link] i fail to see the connection between this code change and `tests/unit/scripting.tcl`, any clue for this?",-1,0,-1,0.9858419299125672,0.6889694333076477,0.9899476170539856,-1.0,accept,majority_agreement
991886804,9934,"thank you. note that this isn't only saving system calls, there's also a good chance it saves tcp packets, specifically when the reply list is composed of many small objects, which is the case of many calls for addreplydeferredlen, see #7123 regarding the tests, the failures seem very consistent, i suppose it's some side effect of your change.",1,1,1,0.8917114734649658,0.8777021169662476,0.9596961140632628,1.0,accept,unanimous_agreement
992398718,9934,"hi , could you share some details about these two failed tests with me? i have no idea what might cause these failures and how to fix it, thanks!",1,1,1,0.96154522895813,0.978095769882202,0.8557834625244141,1.0,accept,unanimous_agreement
992550252,9934,"when use `loglevel verbose`config to start server and run` eval ""local a = {}; local b = {a}; a[1] = b; return a” 0`, you can see `error writing to client: invalid argument` log. `connwritev` always returns -1 in `script return recursive object` test.",0,0,0,0.9888206720352172,0.993371605873108,0.992651641368866,0.0,accept,unanimous_agreement
992576707,9934,"i have a question, whether using `writev` may cause blocking of other connections. for example, if connection a has 100m replied data (10 clientreplyblocks), in the old code, it would be issued in 10 event cycles, but now it is issued in 10 clientreplyblocks in one cycle.",0,0,0,0.9728463292121888,0.982783317565918,0.974970042705536,0.0,accept,unanimous_agreement
992615186,9934,"i don't think that would be a problem cuz it will stop sending data when it reaches the limitation of net_max_writes_per_event (1024*64). furthermore, the `writev()` call ought to return immediately since the socket is non-blocking, therefore, calling `writev()` will not result in blocking even if there is a large number of bytes in the reply list.",0,0,0,0.9749602675437928,0.991320788860321,0.9760987162590028,0.0,accept,unanimous_agreement
993122959,9934,"the replbufblock size is 16k, which means we can easily break the net_max_writes_per_event 16k limit. yes, but writev needs to copy memory from user state to kernel anyway, which is probably the reason for the net_max_writes_per_event limit.",0,0,0,0.987019658088684,0.9846307635307312,0.993338108062744,0.0,accept,unanimous_agreement
993128358,9934,"good point, maybe we should put `net_max_writes_per_event` into _writetoclient() for `writev()`.",1,0,1,0.5242871642112732,0.8697434067726135,0.6736660599708557,1.0,accept,majority_agreement
993212994,9934,ptal,0,0,0,0.97641783952713,0.9552443027496338,0.9751940965652466,0.0,accept,unanimous_agreement
993320589,9934,"which is probably the reason for the net_max_writes_per_event limit. actually, there's no explicit copying of data within pointers of iov into the kernel's memory. see [a link] and [a link] therefore, even if we don't limit the number of bytes to net_max_writes_per_event, kernel will write bytes up to its maximum of socket send buffer (defined by /proc/sys/net/ipv4/tcp_wmem) instead of all bytes from user space(like 100mb)",0,0,0,0.98509418964386,0.9913250207901,0.9933945536613464,0.0,accept,unanimous_agreement
993321265,9934,"i'm sorry guys, i'm really busy elsewhere, and didn't review the code or correspondence here. keep up the good work, i'll get to it some day.",-1,-1,-1,0.9888973832130432,0.9936444759368896,0.987161099910736,-1.0,accept,unanimous_agreement
993651193,9934,"another aspect around this matter - we might deteriorate the performance in case of big replies, especially with tls. i think we better always verify that the total aggregated bytes is not more than net_max_writes_per_event, as like before by the caller. this includes refining the first entry condition `(listlength(c->reply) > 1)` to a condition that verifies the sum of first two replies is no more than net_max_writes_per_event. we also let the higher logic at function writetoclient() to check its own conditions of whether to continue loop, or break.",0,0,0,0.9602981805801392,0.9889841675758362,0.9843706488609314,0.0,accept,unanimous_agreement
993756335,9934,"i think `max_iov_size_per_event` (latest commit in this pr) can achieve the same goal as using `net_max_writes_per_event`, or you have some other comments on the latest commit?",0,0,0,0.9885249733924866,0.9937864542007446,0.9923800230026244,0.0,accept,unanimous_agreement
993865373,9934,"`max_iov_size_per_event` limits the number of buffers to write. whereas `net_max_writes_per_event` limits the total number of bytes to write. current implementation regulates the number of transmitted bytes, at some level, by returning from `_writetoclient()` after each single write reply and testing the conditions to continue writing or breaking - such as checking `totwritten > net_max_writes_per_event`. on new implementation, we are writing multiple replies, up-to `max_iov_size_per_event` replies, of any size, for a single call to `_writetoclient()`. imo we need to be more careful as long as we don't test it thoroughly. in addition, note that the big impact will be with small replies that can be aggregated easily. for big ones, say of size 1mb, we might find worse performance, especially around ssl, which in the new implementation forces us to make another memcopy of all the replies into a new multi-buffer (note that your writev ssl implementation might allocates huge buffer on stack). in other words, restricting writev() to multiple replies up-to size of `max_iov_size_per_event` , might be a safe bet, that won't change behavior, effective for small packets, and won't worsen performance in case of huge replies.",0,0,0,0.9834559559822084,0.9923229217529296,0.9900699257850648,0.0,accept,unanimous_agreement
996655479,9934,"i read the implementation of `writev` in `glibc`, which memcpy unconnected memory to a new buffer, and its implementation looks similar to your implementation in `tls`. [a link]",0,0,0,0.9871106147766112,0.9869994521141052,0.9928919076919556,0.0,accept,unanimous_agreement
996932271,9934,"this is just a generic implementation of glibc, whereas different devices have their own implementations. actually, linux has carried out a standardized standardization of the device model. for example, devices are divided into character devices, block devices, network devices, etc. for developers, to implement a driver for a device, it must be implemented in accordance with the specifications provided by linux. for the interaction with the user layer, the kernel requires developers to implement a structure called `file_operations`. this structure defines callback pointers for a series of operations, such as `read`, `write`, and other operations. when the user calls methods such as `read()` and `write()`, the kernel will call back to the `file_operations.read` and `file_operations.write` methods of this device essentially, and these rules also apply to `writev`. the path of `writev` is `writev -> vfs_writev -> ... -> write_iter`, and the `write_iter` implemented by socket is `sock_write_iter`, eventually the kernel will call `tcp_sendmsg_locked` to send out data, just like i mentioned before: the kernel only copies the iovec array consisting of data addresses without copying any actual data, in `tcp_sendmsg_locked()`, the data in user space goes directly into the socket send buffer in the kernel without any intermediary buffer like it has in the generic `writev()` of glibc. think it this way: what's the point if `writev()` just simply copies all scattered buffers into one big successive buffer and then calls `write()`?",0,0,0,0.9832929968833924,0.993062436580658,0.9780033826828004,0.0,accept,unanimous_agreement
998461974,9934,"hi , got some time to take a look at this now? we've had a lot of discussions last week and i think we need more opinions and advice here.",0,0,0,0.5729724764823914,0.8141446709632874,0.8582063317298889,0.0,accept,unanimous_agreement
998520957,9934,"i'm sorry. i'm extremely busy preparing for 7.0 rc1 (need to get all the api and major changes in time). i'll have to look at this and merge it after rc1 (since it's not an interface / breaking change, or major roadmap feature, we can merge it in any version we like). if you need further input in order to proceed, maybe try to write a short summary of your conclusions and questions (something i can read, make my opinion, and answer in some 10 minutes), please post and i'll try, if it's more complicated than that and require a deep look at the code, i'll just get to it after rc1 (few more weeks). sorry.",-1,-1,-1,0.9897550940513612,0.9909551739692688,0.986420214176178,-1.0,accept,unanimous_agreement
999272557,9934,"ok, we'll wait until then, when is 7.0 rc1 planned to release?",0,0,0,0.986761212348938,0.9922571182250975,0.992657482624054,0.0,accept,unanimous_agreement
999328647,9934,"few more weeks, unless there are delays.",0,0,0,0.932974100112915,0.8602226972579956,0.9674164056777954,0.0,accept,unanimous_agreement
1006566353,9934,/core-team if you agree i would like to extend the [a link] and use it to verify this pr performance impact across the commands.,0,0,0,0.9734294414520264,0.988321840763092,0.9916486740112304,0.0,accept,unanimous_agreement
1006575763,9934,i obviously won't object.. what is there to extend?,0,-1,0,0.6952810287475586,0.5257432460784912,0.8835027813911438,0.0,accept,majority_agreement
1006580388,9934,i believe extending it to include: might be good.,1,0,0,0.879640519618988,0.560944676399231,0.9504342675209044,0.0,accept,majority_agreement
1011907115,9934,everything goes well with 7.0 rc1?,0,0,0,0.9845637083053588,0.9632321000099182,0.9500084519386292,0.0,accept,unanimous_agreement
1012012352,9934,sorry.. still busy. rc1 delayed to end of january.,-1,-1,-1,0.988924205303192,0.9930601119995116,0.9924557209014891,-1.0,accept,unanimous_agreement
1026423298,9934,"just to confirm, i guess 7.0-rc is delayed again?",0,0,0,0.9871610403060912,0.9807394742965698,0.986901581287384,0.0,accept,unanimous_agreement
1026509627,9934,no.. it was just released... and you're even listed in the release notes: 8-) [a link],1,1,1,0.7619032263755798,0.9267273545265198,0.985269010066986,1.0,accept,unanimous_agreement
1026512014,9934,"oh sorry, 7.0-rc didn't show up in [a link] and now i know it's because it's a pre-release.",-1,-1,-1,0.9893450140953064,0.9713202118873596,0.9862215518951416,-1.0,accept,unanimous_agreement
1028631548,9934,"not so sure what it is when you said ""top comment"", did you mean the comments in this issue thread or comments of code review? i think those comments are still there?",0,0,0,0.8695630431175232,0.9710120558738708,0.907441020011902,0.0,accept,unanimous_agreement
1028672798,9934,"by top comment i meant the description of the pr, the one currently says ""as title."" i'd like it to contain an up to date description of what it does, why, and if there are any other special considerations or unrelated changes it contains.",0,0,0,0.9693015217781068,0.9804494976997375,0.9872724413871764,0.0,accept,unanimous_agreement
1029020994,9934,since the last month i've added more benchmarks to the oss automation . i'm just missing the tls performance check. until sunday should a table with an extensive comparison on this branch vs unstable.,0,0,0,0.9577760696411132,0.8642457127571106,0.9670685529708862,0.0,accept,unanimous_agreement
1029025455,9934,"please make sure that the benchmark is run on the latest commit of this branch, thanks!",1,1,1,0.9496848583221436,0.9637237191200256,0.7841581106185913,1.0,accept,unanimous_agreement
1032266676,9934,any update?,0,0,0,0.9836139678955078,0.984815239906311,0.9862627983093262,0.0,accept,unanimous_agreement
1032459063,9934,need some more days. i'm taking this chance to extend our standard benchmarks (as seen in [a link] ) so i just need one extra change we will have a deeper comparison.,0,0,0,0.7405111193656921,0.9684683084487916,0.9800615906715392,0.0,accept,unanimous_agreement
1032497946,9934,"got it, thanks for your efforts.",1,1,1,0.8995634317398071,0.753199577331543,0.9843136072158812,1.0,accept,unanimous_agreement
1033661623,9934,"and a bit of detail on the test: - using 13 different tests ( each test is specified in [a link] ), that cover pipelined/single command, hashes, lists, and strings. - each test has the plaintext/tls variant. - the tests were run with m6i.8xlarge vms ( one for client, one for db ).both on same placement group to allow for the less/most stable network overhead. - cpu: intel(r) xeon(r) platinum 8375c cpu @ 2.90ghz, mem: 128gb. notice that this is the latest gen intel on aws (code-named ice lake). - os was ubuntu 18.04 ( kernel version 5.3.0 ). - compiler was gcc-10.3.0 - used commit hashes: - unstable : 34c288fe11dd690686a294f55f0be60e9c5b629d - panj200:use-writev: be5e56cd115ecda2509b0c9a373df3099f3b3ec3 **based on the above we've observed**: - plain text: - it seems that all benchmarks that are ""heavy"" readers/ ""heavy"" on the reply ( vs for example hset in which we do the opposite ) are affected negatively by this change, up to -13.8%. - tls - across all variations, only 1 produced a slight improvement, the remaining 12 test-suites had a negative impact up to -12% achievable ops/sec. - the most affected benchmarks were the ones using small values ( 10bytes - memtier_benchmark-1mkeys-10b-expire-use-case-tls:totals ). the exact same benchmark with 100b values had a negative impact of -6.5%. it seems like we need to further investigate why this change is not having the desired effect. the compiler/os/kernel are rather recent and common, and the benchmarks seem simpler to me. if you agree, i suggest: - we profile this further to understand exactly if the cpu cycles of write are indeed being reduced/not reduced and what's taking that time. please share your thoughts. the detailed results can be checked on the tables below: ----------------------- detail: ### overall achievable ops/sec per testcase/command on a standalone deployment (plain text) | test-case |unstable |panjf2000/use-writev|%% diff| |--------------------------------------------------------------------------------|--------:|-------------------:|-------| |memtier_benchmark-10mkeys-load-hash-5-fields-with-100b-values-pipeline-10:hsets |325447.83| 317522.67|-2.4% | |memtier_benchmark-10mkeys-load-hash-5-fields-with-100b-values-pipeline-10:totals|325447.83| 317522.67|-2.4% | |memtier_benchmark-10mkeys-load-hash-5-fields-with-100b-values:hsets |148311.58| 152999.86|3.2% | |memtier_benchmark-10mkeys-load-hash-5-fields-with-100b-values:totals |148311.58| 152999.86|3.2% | |memtier_benchmark-10mkeys-load-hash-5-fields-with-10b-values-pipeline-10:hsets |374918.59| 374745.06|-0.0% | |memtier_benchmark-10mkeys-load-hash-5-fields-with-10b-values-pipeline-10:totals |374918.59| 374745.06|-0.0% | |memtier_benchmark-10mkeys-load-hash-5-fields-with-10b-values:hsets |179309.14| 195132.74|8.8% | |memtier_benchmark-10mkeys-load-hash-5-fields-with-10b-values:totals |179309.14| 195132.74|8.8% | |memtier_benchmark-1mkeys-100b-expire-use-case:dels | 73620.71| 65928.45|-10.4% | |memtier_benchmark-1mkeys-100b-expire-use-case:gets | 73620.78| 65928.54|-10.4% | |memtier_benchmark-1mkeys-100b-expire-use-case:setexs | 73620.98| 65928.70|-10.4% | |memtier_benchmark-1mkeys-100b-expire-use-case:sets | 73620.91| 65928.64|-10.4% | |memtier_benchmark-1mkeys-100b-expire-use-case:totals |294483.38| 263714.34|-10.4% | |memtier_benchmark-1mkeys-10b-expire-use-case:dels | 75220.92| 65712.19|-12.6% | |memtier_benchmark-1mkeys-10b-expire-use-case:gets | 75220.98| 65712.26|-12.6% | |memtier_benchmark-1mkeys-10b-expire-use-case:setexs | 75221.20| 65712.41|-12.6% | |memtier_benchmark-1mkeys-10b-expire-use-case:sets | 75221.10| 65712.35|-12.6% | |memtier_benchmark-1mkeys-10b-expire-use-case:totals |300884.20| 262849.21|-12.6% | |memtier_benchmark-1mkeys-1kib-expire-use-case:dels | 75318.32| 67571.10|-10.3% | |memtier_benchmark-1mkeys-1kib-expire-use-case:gets | 75318.39| 67571.19|-10.3% | |memtier_benchmark-1mkeys-1kib-expire-use-case:setexs | 75318.56| 67571.36|-10.3% | |memtier_benchmark-1mkeys-1kib-expire-use-case:sets | 75318.47| 67571.29|-10.3% | |memtier_benchmark-1mkeys-1kib-expire-use-case:totals |301273.75| 270284.94|-10.3% | |memtier_benchmark-1mkeys-4kib-expire-use-case:dels | 74074.63| 64732.67|-12.6% | |memtier_benchmark-1mkeys-4kib-expire-use-case:gets | 74074.73| 64732.74|-12.6% | |memtier_benchmark-1mkeys-4kib-expire-use-case:setexs | 74074.88| 64732.90|-12.6% | |memtier_benchmark-1mkeys-4kib-expire-use-case:sets | 74074.78| 64732.84|-12.6% | |memtier_benchmark-1mkeys-4kib-expire-use-case:totals |296299.02| 258931.14|-12.6% | |memtier_benchmark-1mkeys-hash-hget-hgetall-hkeys-hvals-with-100b-values:hgetalls| 79863.67| 70134.25|-12.2% | |memtier_benchmark-1mkeys-hash-hget-hgetall-hkeys-hvals-with-100b-values:hgets | 79862.84| 70133.44|-12.2% | |memtier_benchmark-1mkeys-hash-hget-hgetall-hkeys-hvals-with-100b-values:hkeyss | 79863.39| 70134.01|-12.2% | |memtier_benchmark-1mkeys-hash-hget-hgetall-hkeys-hvals-with-100b-values:hvalss | 79863.13| 70133.72|-12.2% | |memtier_benchmark-1mkeys-hash-hget-hgetall-hkeys-hvals-with-100b-values:totals |319453.03| 280535.42|-12.2% | |memtier_benchmark-1mkeys-list-lpop-rpop-with-100b-values:lpops |159140.28| 140805.72|-11.5% | |memtier_benchmark-1mkeys-list-lpop-rpop-with-100b-values:rpops |159139.73| 140805.18|-11.5% | |memtier_benchmark-1mkeys-list-lpop-rpop-with-100b-values:totals |318280.01| 281610.90|-11.5% | |memtier_benchmark-1mkeys-load-hash-5-fields-with-1000b-values-pipeline-10:hsets |148671.87| 128134.92|-13.8% | |memtier_benchmark-1mkeys-load-hash-5-fields-with-1000b-values-pipeline-10:totals|148671.87| 128134.92|-13.8% | |memtier_benchmark-1mkeys-load-hash-5-fields-with-1000b-values:hsets | 94748.89| 91954.06|-2.9% | |memtier_benchmark-1mkeys-load-hash-5-fields-with-1000b-values:totals | 94748.89| 91954.06|-2.9% | |memtier_benchmark-1mkeys-load-list-with-100b-values:lpushs |223176.90| 206434.21|-7.5% | |memtier_benchmark-1mkeys-load-list-with-100b-values:totals |223176.90| 206434.21|-7.5% | ### overall achievable ops/sec per testcase/command on a standalone deployment (tls enabled) | test-case |unstable |panjf2000/use-writev|%% diff| |------------------------------------------------------------------------------------|--------:|-------------------:|-------| |memtier_benchmark-10mkeys-load-hash-5-fields-with-100b-values-pipeline-10-tls:hsets |117441.71| 123563.30|5.2% | |memtier_benchmark-10mkeys-load-hash-5-fields-with-100b-values-pipeline-10-tls:totals|117441.71| 123563.30|5.2% | |memtier_benchmark-10mkeys-load-hash-5-fields-with-100b-values-tls:hsets |120857.40| 115852.08|-4.1% | |memtier_benchmark-10mkeys-load-hash-5-fields-with-100b-values-tls:totals |120857.40| 115852.08|-4.1% | |memtier_benchmark-10mkeys-load-hash-5-fields-with-10b-values-pipeline-10-tls:hsets |291553.99| 283106.18|-2.9% | |memtier_benchmark-10mkeys-load-hash-5-fields-with-10b-values-pipeline-10-tls:totals |291553.99| 283106.18|-2.9% | |memtier_benchmark-10mkeys-load-hash-5-fields-with-10b-values-tls:hsets |133132.45| 127968.33|-3.9% | |memtier_benchmark-10mkeys-load-hash-5-fields-with-10b-values-tls:totals |133132.45| 127968.33|-3.9% | |memtier_benchmark-1mkeys-100b-expire-use-case-tls:dels | 48705.14| 45558.85|-6.5% | |memtier_benchmark-1mkeys-100b-expire-use-case-tls:gets | 48705.20| 45558.91|-6.5% | |memtier_benchmark-1mkeys-100b-expire-use-case-tls:setexs | 48705.37| 45559.10|-6.5% | |memtier_benchmark-1mkeys-100b-expire-use-case-tls:sets | 48705.30| 45558.99|-6.5% | |memtier_benchmark-1mkeys-100b-expire-use-case-tls:totals |194821.02| 182235.85|-6.5% | |memtier_benchmark-1mkeys-10b-expire-use-case-tls:dels | 48926.57| 42953.74|-12.2% | |memtier_benchmark-1mkeys-10b-expire-use-case-tls:gets | 48926.63| 42953.82|-12.2% | |memtier_benchmark-1mkeys-10b-expire-use-case-tls:setexs | 48926.81| 42954.01|-12.2% | |memtier_benchmark-1mkeys-10b-expire-use-case-tls:sets | 48926.74| 42953.91|-12.2% | |memtier_benchmark-1mkeys-10b-expire-use-case-tls:totals |195706.76| 171815.48|-12.2% | |memtier_benchmark-1mkeys-1kib-expire-use-case-tls:dels | 49220.23| 44032.11|-10.5% | |memtier_benchmark-1mkeys-1kib-expire-use-case-tls:gets | 49220.35| 44032.19|-10.5% | |memtier_benchmark-1mkeys-1kib-expire-use-case-tls:setexs | 49220.50| 44032.35|-10.5% | |memtier_benchmark-1mkeys-1kib-expire-use-case-tls:sets | 49220.42| 44032.26|-10.5% | |memtier_benchmark-1mkeys-1kib-expire-use-case-tls:totals |196881.49| 176128.92|-10.5% | |memtier_benchmark-1mkeys-4kib-expire-use-case-tls:dels | 48864.90| 47031.52|-3.8% | |memtier_benchmark-1mkeys-4kib-expire-use-case-tls:gets | 48864.97| 47031.57|-3.8% | |memtier_benchmark-1mkeys-4kib-expire-use-case-tls:setexs | 48865.13| 47031.76|-3.8% | |memtier_benchmark-1mkeys-4kib-expire-use-case-tls:sets | 48865.05| 47031.70|-3.8% | |memtier_benchmark-1mkeys-4kib-expire-use-case-tls:totals |195460.04| 188126.55|-3.8% | |memtier_benchmark-1mkeys-hash-hget-hgetall-hkeys-hvals-with-100b-values-tls:hgetalls| 48640.77| 45790.97|-5.9% | |memtier_benchmark-1mkeys-hash-hget-hgetall-hkeys-hvals-with-100b-values-tls:hgets | 48639.91| 45790.08|-5.9% | |memtier_benchmark-1mkeys-hash-hget-hgetall-hkeys-hvals-with-100b-values-tls:hkeyss | 48640.49| 45790.68|-5.9% | |memtier_benchmark-1mkeys-hash-hget-hgetall-hkeys-hvals-with-100b-values-tls:hvalss | 48640.19| 45790.37|-5.9% | |memtier_benchmark-1mkeys-hash-hget-hgetall-hkeys-hvals-with-100b-values-tls:totals |194561.36| 183162.10|-5.9% | |memtier_benchmark-1mkeys-list-lpop-rpop-with-100b-values-tls:lpops | 98082.82| 93128.32|-5.1% | |memtier_benchmark-1mkeys-list-lpop-rpop-with-100b-values-tls:rpops | 98082.33| 93127.78|-5.1% | |memtier_benchmark-1mkeys-list-lpop-rpop-with-100b-values-tls:totals |196165.15| 186256.10|-5.1% | |memtier_benchmark-1mkeys-load-hash-5-fields-with-1000b-values-pipeline-10-tls:hsets | 56816.08| 56344.07|-0.8% | |memtier_benchmark-1mkeys-load-hash-5-fields-with-1000b-values-pipeline-10-tls:totals| 56816.08| 56344.07|-0.8% | |memtier_benchmark-1mkeys-load-hash-5-fields-with-1000b-values-tls:hsets | 52479.04| 49139.47|-6.4% | |memtier_benchmark-1mkeys-load-hash-5-fields-with-1000b-values-tls:totals | 52479.04| 49139.47|-6.4% | |memtier_benchmark-1mkeys-load-list-with-100b-values-tls:lpushs |149302.79| 147217.18|-1.4% | |memtier_benchmark-1mkeys-load-list-with-100b-values-tls:totals |149302.79| 147217.18|-1.4% |",0,0,0,0.9406834840774536,0.9866288900375366,0.9491289854049684,0.0,accept,unanimous_agreement
1033761181,9934,"the branch in this pr is nearly 200 commits behind unstable, so it could be that this big impact is due to something else. we can match them by either testing the merge-base (b93ccee45136992fe08398cc9058f9546708062b) instead of unstable, or by merging unstable into this pr. i wouldn't want to re-do the entire benchmark and result review, since it might be a dead end, but let's look into it by running one benchmark from the above mentioned commit and check if it changes the picture dramatically.",0,0,0,0.9693560004234314,0.9315841794013976,0.9861903190612792,0.0,accept,unanimous_agreement
1033872788,9934,it's relatively easy to trigger a new run for all variations for b93ccee45136992fe08398cc9058f9546708062b. in a couple of hours we should have more clarity :),1,1,1,0.9137167930603028,0.9767895340919496,0.8731629848480225,1.0,accept,unanimous_agreement
1034044932,9934,"got new numbers using the merge-base ([a link] instead of unstable. - plaintext - we still see negative impact on multiple results. our test with the largest value size (4kib -- memtier_benchmark-1mkeys-4kib-expire-use-case ) seems to have been the most affected. i suggest we test larger value sizes and more reply variations. - tls - we see a very interesting pattern here. the only regression on this test is for smaller values (10b). same like the above i suggest we extend further the variations with more tests with 10b values to ensure this is an expected pattern. imo, we need at least more 5 tests: todo: - add memtier_benchmark-1mkeys-8kib-expire-use-case - add memtier_benchmark-1mkeys-16kib-expire-use-case - add memtier_benchmark-1mkeys-hash-hget-hgetall-hkeys-hvals-with-1kib-values - add memtier_benchmark-1mkeys-hash-hget-hgetall-hkeys-hvals-with-4kib-values - add memtier_benchmark-1mkeys-hash-hget-hgetall-hkeys-hvals-with-10b-values agree ? wrt to your tls concerns, the bellow numbers seem to ease them correct? ### overall achievable ops/sec per testcase/command on a standalone deployment (plain text) | test-case |b93ccee |panjf2000/use-writev|%% diff| |--------------------------------------------------------------------------------|--------:|-------------------:|-------| |memtier_benchmark-10mkeys-load-hash-5-fields-with-100b-values-pipeline-10:hsets |323704.54| 317522.67|-1.9% | |memtier_benchmark-10mkeys-load-hash-5-fields-with-100b-values-pipeline-10:totals|323704.54| 317522.67|-1.9% | |memtier_benchmark-10mkeys-load-hash-5-fields-with-100b-values:hsets |158263.45| 152999.86|-3.3% | |memtier_benchmark-10mkeys-load-hash-5-fields-with-100b-values:totals |158263.45| 152999.86|-3.3% | |memtier_benchmark-10mkeys-load-hash-5-fields-with-10b-values-pipeline-10:hsets |372609.69| 374745.06|0.6% | |memtier_benchmark-10mkeys-load-hash-5-fields-with-10b-values-pipeline-10:totals |372609.69| 374745.06|0.6% | |memtier_benchmark-10mkeys-load-hash-5-fields-with-10b-values:hsets |187243.13| 195132.74|4.2% | |memtier_benchmark-10mkeys-load-hash-5-fields-with-10b-values:totals |187243.13| 195132.74|4.2% | |memtier_benchmark-1mkeys-100b-expire-use-case:dels | 69777.39| 65928.45|-5.5% | |memtier_benchmark-1mkeys-100b-expire-use-case:gets | 69777.47| 65928.54|-5.5% | |memtier_benchmark-1mkeys-100b-expire-use-case:setexs | 69777.66| 65928.70|-5.5% | |memtier_benchmark-1mkeys-100b-expire-use-case:sets | 69777.55| 65928.64|-5.5% | |memtier_benchmark-1mkeys-100b-expire-use-case:totals |279110.06| 263714.34|-5.5% | |memtier_benchmark-1mkeys-10b-expire-use-case:dels | 66342.19| 65712.19|-0.9% | |memtier_benchmark-1mkeys-10b-expire-use-case:gets | 66342.29| 65712.26|-0.9% | |memtier_benchmark-1mkeys-10b-expire-use-case:setexs | 66342.46| 65712.41|-0.9% | |memtier_benchmark-1mkeys-10b-expire-use-case:sets | 66342.38| 65712.35|-0.9% | |memtier_benchmark-1mkeys-10b-expire-use-case:totals |265369.32| 262849.21|-0.9% | |memtier_benchmark-1mkeys-1kib-expire-use-case:dels | 65474.00| 67571.10|3.2% | |memtier_benchmark-1mkeys-1kib-expire-use-case:gets | 65474.11| 67571.19|3.2% | |memtier_benchmark-1mkeys-1kib-expire-use-case:setexs | 65474.25| 67571.36|3.2% | |memtier_benchmark-1mkeys-1kib-expire-use-case:sets | 65474.17| 67571.29|3.2% | |memtier_benchmark-1mkeys-1kib-expire-use-case:totals |261896.52| 270284.94|3.2% | |memtier_benchmark-1mkeys-4kib-expire-use-case:dels | 69943.17| 64732.67|-7.4% | |memtier_benchmark-1mkeys-4kib-expire-use-case:gets | 69943.26| 64732.74|-7.4% | |memtier_benchmark-1mkeys-4kib-expire-use-case:setexs | 69943.43| 64732.90|-7.4% | |memtier_benchmark-1mkeys-4kib-expire-use-case:sets | 69943.36| 64732.84|-7.4% | |memtier_benchmark-1mkeys-4kib-expire-use-case:totals |279773.23| 258931.14|-7.4% | |memtier_benchmark-1mkeys-hash-hget-hgetall-hkeys-hvals-with-100b-values:hgetalls| 68454.63| 70134.25|2.5% | |memtier_benchmark-1mkeys-hash-hget-hgetall-hkeys-hvals-with-100b-values:hgets | 68453.78| 70133.44|2.5% | |memtier_benchmark-1mkeys-hash-hget-hgetall-hkeys-hvals-with-100b-values:hkeyss | 68454.38| 70134.01|2.5% | |memtier_benchmark-1mkeys-hash-hget-hgetall-hkeys-hvals-with-100b-values:hvalss | 68454.08| 70133.72|2.5% | |memtier_benchmark-1mkeys-hash-hget-hgetall-hkeys-hvals-with-100b-values:totals |273816.88| 280535.42|2.5% | |memtier_benchmark-1mkeys-list-lpop-rpop-with-100b-values:lpops |138687.46| 140805.72|1.5% | |memtier_benchmark-1mkeys-list-lpop-rpop-with-100b-values:rpops |138686.86| 140805.18|1.5% | |memtier_benchmark-1mkeys-list-lpop-rpop-with-100b-values:totals |277374.32| 281610.90|1.5% | |memtier_benchmark-1mkeys-load-hash-5-fields-with-1000b-values-pipeline-10:hsets |135387.58| 128134.92|-5.4% | |memtier_benchmark-1mkeys-load-hash-5-fields-with-1000b-values-pipeline-10:totals|135387.58| 128134.92|-5.4% | |memtier_benchmark-1mkeys-load-hash-5-fields-with-1000b-values:hsets | 88446.85| 91954.06|4.0% | |memtier_benchmark-1mkeys-load-hash-5-fields-with-1000b-values:totals | 88446.85| 91954.06|4.0% | |memtier_benchmark-1mkeys-load-list-with-100b-values:lpushs |199970.61| 206434.21|3.2% | |memtier_benchmark-1mkeys-load-list-with-100b-values:totals |199970.61| 206434.21|3.2% | ### overall achievable ops/sec per testcase/command on a standalone deployment (tls enabled) | test-case |unstable |panjf2000/use-writev|%% diff| |------------------------------------------------------------------------------------|--------:|-------------------:|-------| |memtier_benchmark-10mkeys-load-hash-5-fields-with-100b-values-pipeline-10-tls:hsets |115093.05| 123563.30|7.4% | |memtier_benchmark-10mkeys-load-hash-5-fields-with-100b-values-pipeline-10-tls:totals|115093.05| 123563.30|7.4% | |memtier_benchmark-10mkeys-load-hash-5-fields-with-100b-values-tls:hsets |114486.81| 115852.08|1.2% | |memtier_benchmark-10mkeys-load-hash-5-fields-with-100b-values-tls:totals |114486.81| 115852.08|1.2% | |memtier_benchmark-10mkeys-load-hash-5-fields-with-10b-values-pipeline-10-tls:hsets |280377.81| 283106.18|1.0% | |memtier_benchmark-10mkeys-load-hash-5-fields-with-10b-values-pipeline-10-tls:totals |280377.81| 283106.18|1.0% | |memtier_benchmark-10mkeys-load-hash-5-fields-with-10b-values-tls:hsets |133307.04| 127968.33|-4.0% | |memtier_benchmark-10mkeys-load-hash-5-fields-with-10b-values-tls:totals |133307.04| 127968.33|-4.0% | |memtier_benchmark-1mkeys-100b-expire-use-case-tls:dels | 43154.22| 45558.85|5.6% | |memtier_benchmark-1mkeys-100b-expire-use-case-tls:gets | 43154.29| 45558.91|5.6% | |memtier_benchmark-1mkeys-100b-expire-use-case-tls:setexs | 43154.44| 45559.10|5.6% | |memtier_benchmark-1mkeys-100b-expire-use-case-tls:sets | 43154.38| 45558.99|5.6% | |memtier_benchmark-1mkeys-100b-expire-use-case-tls:totals |172617.33| 182235.85|5.6% | |memtier_benchmark-1mkeys-10b-expire-use-case-tls:dels | 46602.27| 42953.74|-7.8% | |memtier_benchmark-1mkeys-10b-expire-use-case-tls:gets | 46602.38| 42953.82|-7.8% | |memtier_benchmark-1mkeys-10b-expire-use-case-tls:setexs | 46602.54| 42954.01|-7.8% | |memtier_benchmark-1mkeys-10b-expire-use-case-tls:sets | 46602.46| 42953.91|-7.8% | |memtier_benchmark-1mkeys-10b-expire-use-case-tls:totals |186409.65| 171815.48|-7.8% | |memtier_benchmark-1mkeys-1kib-expire-use-case-tls:dels | 44389.86| 44032.11|-0.8% | |memtier_benchmark-1mkeys-1kib-expire-use-case-tls:gets | 44389.94| 44032.19|-0.8% | |memtier_benchmark-1mkeys-1kib-expire-use-case-tls:setexs | 44390.09| 44032.35|-0.8% | |memtier_benchmark-1mkeys-1kib-expire-use-case-tls:sets | 44390.02| 44032.26|-0.8% | |memtier_benchmark-1mkeys-1kib-expire-use-case-tls:totals |177559.91| 176128.92|-0.8% | |memtier_benchmark-1mkeys-4kib-expire-use-case-tls:dels | 47189.52| 47031.52|-0.3% | |memtier_benchmark-1mkeys-4kib-expire-use-case-tls:gets | 47189.58| 47031.57|-0.3% | |memtier_benchmark-1mkeys-4kib-expire-use-case-tls:setexs | 47189.77| 47031.76|-0.3% | |memtier_benchmark-1mkeys-4kib-expire-use-case-tls:sets | 47189.66| 47031.70|-0.3% | |memtier_benchmark-1mkeys-4kib-expire-use-case-tls:totals |188758.54| 188126.55|-0.3% | |memtier_benchmark-1mkeys-hash-hget-hgetall-hkeys-hvals-with-100b-values-tls:hgetalls| 44342.74| 45790.97|3.3% | |memtier_benchmark-1mkeys-hash-hget-hgetall-hkeys-hvals-with-100b-values-tls:hgets | 44341.92| 45790.08|3.3% | |memtier_benchmark-1mkeys-hash-hget-hgetall-hkeys-hvals-with-100b-values-tls:hkeyss | 44342.46| 45790.68|3.3% | |memtier_benchmark-1mkeys-hash-hget-hgetall-hkeys-hvals-with-100b-values-tls:hvalss | 44342.16| 45790.37|3.3% | |memtier_benchmark-1mkeys-hash-hget-hgetall-hkeys-hvals-with-100b-values-tls:totals |177369.27| 183162.10|3.3% | |memtier_benchmark-1mkeys-list-lpop-rpop-with-100b-values-tls:lpops | 89297.76| 93128.32|4.3% | |memtier_benchmark-1mkeys-list-lpop-rpop-with-100b-values-tls:rpops | 89297.20| 93127.78|4.3% | |memtier_benchmark-1mkeys-list-lpop-rpop-with-100b-values-tls:totals |178594.96| 186256.10|4.3% | |memtier_benchmark-1mkeys-load-hash-5-fields-with-1000b-values-pipeline-10-tls:hsets | 56357.19| 56344.07|-0.0% | |memtier_benchmark-1mkeys-load-hash-5-fields-with-1000b-values-pipeline-10-tls:totals| 56357.19| 56344.07|-0.0% | |memtier_benchmark-1mkeys-load-hash-5-fields-with-1000b-values-tls:hsets | 48555.22| 49139.47|1.2% | |memtier_benchmark-1mkeys-load-hash-5-fields-with-1000b-values-tls:totals | 48555.22| 49139.47|1.2% | |memtier_benchmark-1mkeys-load-list-with-100b-values-tls:lpushs |143990.62| 147217.18|2.2% | |memtier_benchmark-1mkeys-load-list-with-100b-values-tls:totals |143990.62| 147217.18|2.2% |",0,0,0,0.8692692518234253,0.9325347542762756,0.9707983136177064,0.0,accept,unanimous_agreement
1034129564,9934,"from these benchmarks it looks like this pr doesn't do any good. so just to put things in proportions: this pr: [code block] merge-base (b93ccee): [code block] this is because back then, command command was a heavy user of addreplydeferredlen, which adds ton of nodes to the reply buffer. if we want to find something equivalent in normal data commands, we can use a pipeline of zrange, or zinter. or maybe just something with many big objects like hgetall on hash with several values of 50kb each. but putting that aside, it should be easy to find the places where this pr gives great value, even if not common in redis, modules can have more of these, what's bothering me is why we appear to have a regression on common cases. maybe you can reproduce one of these, profile and and realize where the regression comes from?",0,0,0,0.8424333930015564,0.9460296034812928,0.942464292049408,0.0,accept,unanimous_agreement
1034429060,9934,"ok, i will take time to reproduce it and try to find out the root cause.",0,0,0,0.9778138399124146,0.9744612574577332,0.9896983504295348,0.0,accept,unanimous_agreement
1039912051,9934,"any news or plans when you can look into it? if we're gonna merge this, i'd rather it'll be part of 7.0 rc2 (possibly around a week from now). don't wanna introduce that late before the 7.0.0 release.",0,0,0,0.978600800037384,0.9643502235412598,0.9793460369110109,0.0,accept,unanimous_agreement
1039914493,9934,"sorry, i've been a little busy lately and am about to diagnose this issue today, get some profiling results.",-1,-1,-1,0.9876553416252136,0.9913183450698853,0.9832417964935304,-1.0,accept,unanimous_agreement
1040001259,9934,"a funny fact is that i ran the benchmark on a [a link] manually and got the results which indicate that there was no performance difference between `unstable` and `use-writev`, they had almost the same performance. [code block] unstable: [code block] use-writev: [code block] besides, they basically share the same hierarchical structure of flame graph: unstable: ![a link] user-writev: ![a link] am i doing anything wrong about the benchmarks?",-1,0,-1,0.9869235157966614,0.7406793236732483,0.6557962894439697,-1.0,accept,majority_agreement
1040007112,9934,"it seems that there were not many calls to `writev()` in this common case, which should also be as expected.",0,0,0,0.9790430068969728,0.9941927790641784,0.989123284816742,0.0,accept,unanimous_agreement
1040007713,9934,"did you test unstable? or the merge-base (b93ccee45136992fe08398cc9058f9546708062b)? since unstable contains other changes, let's be sure to test the merge base to eliminate them, so if you did test unstable, and you repeat that test?",0,0,0,0.9878134727478028,0.9927853345870972,0.991666615009308,0.0,accept,unanimous_agreement
1040008835,9934,"yes, i created a new branch from commit [a link] and i've repeated the benchmark multiple times, it gets the same results.",0,0,0,0.9828149676322936,0.972366988658905,0.9907982349395752,0.0,accept,unanimous_agreement
1040013976,9934,"while we're waiting for filipe to explain this, can you maybe do some simple benchmark comparing the command command with tls? i.e. similar to what i did here: [a link] so we can decide if we wanna keep the current code we have for tls, or take yossi's advise and revert that in some way if we see a negative or neutral impact.",0,0,0,0.9757838845252992,0.9867362380027772,0.9884178042411804,0.0,accept,unanimous_agreement
1040029452,9934,"i assume that i should use `memtier_benchmark` instead of `redis-benchmark`? cuz the former support tls and the latter doesn't? but i still don't know how to send `command` command via `memtier_benchmark`, i only ran the general test and the result shows below: unstable: [code block] use-writev: [code block]",0,0,0,0.8973095417022705,0.9909924268722534,0.988508403301239,0.0,accept,unanimous_agreement
1040038251,9934,"sorry, i think i've already known how to do that, working on it.",-1,-1,-1,0.9880492091178894,0.9923985600471495,0.9896113872528076,-1.0,accept,unanimous_agreement
1040038781,9934,"hi in memtier-benchmark you have the option to send an arbitrary command. for that please use the `--command` option. for ex': memtier-benchmark --command=""command"".",0,0,0,0.9887477159500122,0.9924570322036744,0.9924324750900269,0.0,accept,unanimous_agreement
1040043226,9934,unstable: [code block] use-writev: [code block],0,0,0,0.979062795639038,0.9934303164482116,0.9947010278701782,0.0,accept,unanimous_agreement
1040464720,9934,"since i've merged the latest commits of `unstable` into this pr, we might want to trigger a new benchmark test for all variations to get the latest result, would you please run the new benchmark based on the latest `unstable` and this pr? thanks~",1,1,1,0.9139408469200134,0.90350604057312,0.9668492078781128,1.0,accept,unanimous_agreement
1040504480,9934,"will do . in the meantime, i've run manually the 4kib expire use case ( notice that you missed the prepopulation ) on 2x aws m6i.8xlarge vms for: - panjf2000/use-writev: 7b59cffe075e4adda8832b84f8f087b6bc637a5c . 265k ops/sec. p50=0.383 - unstable: 3881f7850f9f81720315bd4f33f2f9dedcc242bb . 276k ops/sec. p50=0.367 [code block] and noticed the exact same pattern of regression ### unstable 3881f7850f9f81720315bd4f33f2f9dedcc242bb [code block] ### panjf2000/use-writev: 7b59cffe075e4adda8832b84f8f087b6bc637a5c [code block] it seems we're detecting some slight regression. i'm fixing some automation issues that will allow us to re-run all tests with no manual work. give me some hours.",0,0,0,0.96443372964859,0.9915094375610352,0.989138662815094,0.0,accept,unanimous_agreement
1040519945,9934,"please try again to reproduce it (with pre-population maybe), so that you can try to profile it and realize where it comes from, or we need to keep trying to figure out what's different between these two benchmarks.",0,0,0,0.9737069606781006,0.9850870370864868,0.99052631855011,0.0,accept,unanimous_agreement
1040971890,9934,i will try it today and see if that makes a difference.,0,0,0,0.9611587524414062,0.9798274040222168,0.9732043743133544,0.0,accept,unanimous_agreement
1041136298,9934,"i've run the benchmark based on the latest commit of both `unstable` and `use-writev`, with pre-population, and this is the result: unstable: [code block] use-writev: [code block]",0,0,0,0.9855473041534424,0.9897542595863342,0.9900087118148804,0.0,accept,unanimous_agreement
1044682329,9934,"i still failed to reproduce the regression results on my own machine, it seemed that the performance data of `unstable` and `use-writev` is almost the same across generic scenarios, and the profiling results of two branches are basically the same, besides, `writev()` system call didn't even show up in the flame graph of `use-writev`, which could mean that most of the time it used `write()` like `unstable` did, so i think there shouldn't be a performance gap between these two branches in theory. ## unstable ## use-writev is there any chance i can make a program profiling on your aws vms? since the regression results seem to be reproduced on those vms easily.",0,0,0,0.8017516732215881,0.9300475120544434,0.9808821082115172,0.0,accept,unanimous_agreement
1045006210,9934,"maybe the difference in the networking of tcp/ip stack. i.e. the difference is not in cpu time so will not be visible in flame graph, but rather some difference or side effect of using writev on tcp / network behavior. i'm not sure how filipe's benchmarks are run, but i assume andy's are local. it could also be some issue with aws hypervisor, but maybe the first step is to try comparing them on a real network (commonly the actual use case for redis) .",0,0,0,0.9704148769378662,0.9550130367279052,0.9742910861968994,0.0,accept,unanimous_agreement
1046117934,9934,"/ , wrt to: the benchmarks are using 2 vms ( 1db and 1 client virtual machines (kvm) on aws ). wrt to benchmark automation, i noticed some variance on multiple runs of the same benchmark for unstable and the comparison branch. apart from 4 unstable benchmarks ( let's discard them if you agree there is no change that can impact it ), we have 9 benchmarks with no change. [code block] and still about the impact of commands with deferred len i've tested a redistimeseries module use-case, that uses deferred replies (cc ). i was expecting higher impact on the numbers. even thought we've got a slight improvement over unstable, the change is not as meaningful on the `command` numbers shared (that i can indeed reproduce ). tldr, the deferred positive impact is only ""measurable/meaningfull"" on a really large number of deferred writes per reply (otherwise the impact will be faded away like the one bellow ). unstable [code block] panjf2000/redis [code block] ### to conclude tldr i was expecting a larger impact, but on the ""common case"" this is not happening. nonetheless, there are indeed some ""not-so-usuall"" use-cases that are improved. i see reason to merge it :)",0,0,0,0.9715027809143066,0.915213704109192,0.9516241550445556,0.0,accept,unanimous_agreement
1046173772,9934,"i'm not sure how redisrimeseries uses deferred reply, if it is just one per command, or inside a loop (like command command and cluster slots used to do, see #10056, #7123). if it's just one per command, same as command command still does, it should not have a high impact. did you reproduce this on the latest? or the old copy of this branch and it's merge-base? so we now conclude that this pr doesn't do any damage in the common use cases? (even over real network). just to point out again, this pr does get some 300% performance improvement for the old code of command command, see: [a link] and about 10% performance improvement with tls, see: [a link] please ack.",0,0,0,0.8804875016212463,0.9670530557632446,0.9721473455429076,0.0,accept,unanimous_agreement
1047593997,9934,any updates on this?,0,0,0,0.9801620244979858,0.99077308177948,0.9843987226486206,0.0,accept,unanimous_agreement
1047600393,9934,we discussed and approved this today in a core-team meeting. maybe the the last possible pending concern is to realize why it didn't have the expected impact on #10310. but maybe none of that is gonna affect this pr. so anyone has any other concerns or something i forgot before i merge it?,0,0,0,0.9274181723594666,0.9885075092315674,0.9516497850418092,0.0,accept,unanimous_agreement
1047606503,9934,"i just took a quick look at #10310, and i wonder if the root cause for the regression of v6.2.6 compared to v5.0.2 has been found?",0,0,0,0.9196217060089112,0.9908814430236816,0.9801387786865234,0.0,accept,unanimous_agreement
1047611962,9934,"the root cause is that in 6.2.x we started using deferred replies [a link] filipe is suppose to try to ""revert"" this on unstable and see if this explains the whole regression, or just part of it (which would then explain why writev didn't completely undo the regression).",0,0,0,0.983346462249756,0.9866244792938232,0.9918504357337952,0.0,accept,unanimous_agreement
1047619015,9934,"is there any chance that those deferred replies are way larger than iov_max=1024, which might explain why `writev()` only alleviates rather than completely cures the regression?",0,0,0,0.974965274333954,0.983814001083374,0.9910224676132202,0.0,accept,unanimous_agreement
1047643692,9934,"filipe reproduced two cases (i assume the sizes of the elements in the zset are small): [a link] without pipeline, in which case writev didn't help. not sure if this regression is from the fact there are two writes instead of 1, or maybe another change. [a link] with pipeline of 16, so there should be 32 nodes in the reply list (i assume they're all small). and writev improved the case, but not nearly to what it was in the past. i suppose we can merge this pr, and keep discussing this in the other issue. either we'll realize there's an additional regression, or we'll figure out why writev can't solve it entirely. either way, i don't see how the conclusion could affect this pr.",0,0,0,0.9372374415397644,0.9669259786605836,0.9624165892601012,0.0,accept,unanimous_agreement
1047705557,9934,"i edited the top comment (to be used for the squash-merge commit comment). let me know, or just fix if you see any issues or missing info.",0,0,0,0.9866484999656676,0.9802083969116212,0.962312638759613,0.0,accept,unanimous_agreement
1047711460,9934,it looks good to me.,1,1,1,0.9396284222602844,0.9432727694511414,0.9140633344650269,1.0,accept,unanimous_agreement
874740404,9202,"thanks sure, will change that the hiredis parser checks for protocol errors, here the parser assumes there are no protocol errors (because the reply was generated by redis itself) and it can be faster. and in general, this pr is not adding one more parser, its actually removes one extra parser :)",1,1,1,0.9748442769050598,0.9920267462730408,0.993033766746521,1.0,accept,unanimous_agreement
875635240,9202,"please improve the top comment to include the intensive for doing this pr (share code between lua, modules, and the future functions). and if you can, elaborate a bit more on what you did in this pr, i.e. the exact roles of the new units, and any other change you did, so that it'll be easier to review. p.s. you say ""call_reply wraps the redismodulecallreply object"" but isn't it the other way around? or actually that it replaces (renames) it?",0,0,0,0.9713013172149658,0.9844731688499452,0.98801451921463,0.0,accept,unanimous_agreement
878807802,9202,"i realize we have another issue with `rm_replywithcallreply`. after this pr call reply might contain resp3 formats and if you use it with `rm_replywithcallreply` we might send resp3 formats to resp2 clients. i suggest taking the same approach as suggested here [a link] and return error if the reply contains resp3 format and try to reply it to resp 2 client, wdyt?",0,0,0,0.9576611518859864,0.9776920080184937,0.9885064363479614,0.0,accept,unanimous_agreement
878936618,9202,"we can do that (error), but i don't think that's sufficient. maybe we also need to automatically fallback to resp2 in some way (i.e. ignore the `3` char). this is really a not good idea since in some cases the module intends to parse the response, and in some it intends to forward it to the user. so maybe we need two modes (two chars), and soft `3` and a bold `3`. or have `3` explicitly refer to resp3, and another char (e.g. `0`) refer to an automatic mode that uses the real client's preference.",0,0,0,0.8852896094322205,0.7724404335021973,0.90470951795578,0.0,accept,unanimous_agreement
878945275,9202,but then if you use `0` you do not know what to expect. why not just give information on the current protocol used by the client (using context flags maybe)? this way the module can choose what to do and he has all the information he needs. if he wants to forward the reply he can check the protocol used by the client and use the same on `rm_call`. and regardless i think we also need the error in case of a mistake. wdyt?,0,0,0,0.9759376049041748,0.9201474785804749,0.9860246777534484,0.0,accept,unanimous_agreement
878952417,9202,"i agree the error is needed anyway, and i agree that we need to expose a context flag. but i think that in cases where all the module wishes to do is forward the request, forcing it to write code to check the context flags and decide whether or not it should add `3`, is awkward, so i thought of suggesting some auto mode.",0,0,0,0.8736811876296997,0.9681721925735474,0.9183939099311828,0.0,accept,unanimous_agreement
878964359,9202,"ok, i can add the auto mode, should i add the new context flag on this pr?",0,0,0,0.98909330368042,0.9831386804580688,0.9950276017189026,0.0,accept,unanimous_agreement
878998541,9202,i commented here: [a link] let me know what you think,0,0,0,0.957576870918274,0.9136586785316468,0.9172555804252625,0.0,accept,unanimous_agreement
879155274,9202,"/core-team please approve this pr. it started as a refactoring preparation for functions, but it ended up also improving a few things along the way. top comment has all the details.",0,1,0,0.9608983397483826,0.7135215997695923,0.9540306329727172,0.0,accept,majority_agreement
885090670,9202,"/core-team please approve the new api (till now we were focused on the refactoring and doc/comments). please see the top comment for the full list. it's basically 3 things: 1. new module apis for parsing resp3 responses that come back from rm_calll 2. new arguments for rm_call: `3` for resp3, and `0` for auto 3. new flag for rm_getcontextflags for detecting the protocol of the current client 4. new lua capabilities for parsing resp3 features unused till now (bignum, verbatim, attributes) some of these will be needed if rm_call and lua will call module functions that use resp3 features that redis doesn't yet yet (attributes, bignum), i.e. #8521",0,0,0,0.90188866853714,0.9508950710296632,0.8633543252944946,0.0,accept,unanimous_agreement
998869058,9974,"scard processes the data to return the cardinality. i'm a bit uncomfortable with calling list length ""metadata"" in lpush, and the cardinality in scard ""data"" in that sense is llen data or metadata? i think we should clearly state where incr, and setnx falls into, and maybe even a hypothetical case where incr would have returned +ok (i.e the new data is a derivative of the existing one). in that case (ignoring the return value), if we mark incr as r+w, would that mean that append is r+w too? i think that in the past, the `read` / `write` command flags where about getting data out of redis (i.e. that's why rename is `read`), but now for acl and keyspace restrictions, we must consider the source key as a read one. so the key-spec flags can say the first key is has `read` flag, even if the command flag doesn't have the `read` flag. i think this is just a bug (missing read flag). p.s. in the distinct the `r` meant ""will never modify the key space"" (so select had it too), and in v3.2 i pushed to clean this and change the meaning of the flag. but it still refers to keyspace, i.e. dbsize is read. that's an interesting downside (maybe not too late to change). another bad option is to add a ""write-only"" variant, like we have for bitfield_ro. i'm not sure what do do. i'm uncomfortable with changing it now, but also think it's a serious limitation. maybe the way out is to perform some acl permission checks inside the command proc itself? i.e. it'll not get blocked by processcommand, and instead fail internally? regarding aclcheckkey, let's make that change asap (api not released yet)",-1,-1,0,0.9503366947174072,0.6944037079811096,0.5916844606399536,-1.0,accept,majority_agreement
1000512833,9974,"i'm sorry. i was so happy that i finally finished reviewing the code, that i posted my comments without a summary. all in all it looks great. i think most of the comments are minor (or a few major ones that are easy to resolve). i have a feeling that the tests should be enhanced, maybe test some odd commands and corner cases. as well as acl save and load",-1,-1,-1,0.9804847836494446,0.8384886980056763,0.9893925189971924,-1.0,accept,unanimous_agreement
1000514768,9974,"thanks for looking through, it's not as polished as i normally like to put up prs but knew i was going to be busy this week, so probably worth getting feedback. tests was one thing i mentioned needs more work, so i'll definitely revamp them.",1,1,1,0.8961780071258545,0.8659639358520508,0.9708206653594972,1.0,accept,unanimous_agreement
1003144015,9974,"as a chore/aside to this, would it make/be against common sense to add a capability to introspect the effective acl permissions for a given user? ideally, this would be supported by an interface that, for a given user, would: 1. report the verbatim allowed commands (and subcommand) 2. report the allowed key patterns 3. optionally, allow a ""dry run"" to test the possible ability to execute a specific command in terms the context's permissions alone mock api: [code block]",0,0,0,0.9752139449119568,0.9919863939285278,0.9903203845024108,0.0,accept,unanimous_agreement
1003668517,9974,"i have a dry-run command actually implemented locally for testing, i've been flip flopping if it's worth committing. my variant has the form: [code block] will add it as a follow-up. not sure about the super detailed listing of commands though.",0,0,0,0.8196523189544678,0.8630496859550476,0.8291304707527161,0.0,accept,unanimous_agreement
1011844584,9974,"note: i didn't fix all the db.c stuff, i still need to spend some more time to fully understand the right way to fetch the keyspecs. however, i don't think the prevents reviewing the rest of the decisions.",0,0,0,0.9420411586761476,0.9411879777908324,0.9772858023643494,0.0,accept,unanimous_agreement
1012148711,9974,"i'd rather avoid rebase and force-push (please just merge from unstable), it makes it hard to review changes. if we have to rebase (like in the original functions pr that was merged without squash), we must use a separate force-push for the rebase from any other commit pushes. anyway, for now, i'll assume the fist commit of this pr is identical to what i already reviewed. and i should review the two after it. right?",0,0,0,0.896293044090271,0.9696689248085022,0.9384957551956176,0.0,accept,unanimous_agreement
1012487709,9974,"sorry about the rebase, i wanted to pull in the changes from the pubsub v2 pr which included shard channels. there was quite a bit of changes all over the place, so i just decided to rebase. i normally agree, and wait until the end to rebase.",-1,-1,-1,0.9875867366790771,0.9912735819816588,0.9893695116043092,-1.0,accept,unanimous_agreement
1013843571,9974,"something i'm missing or forgot. the spec in [a link] mentioned `%(r|w|d)~ ` and the top comment here says `read + write + delete`, but later only specifies `%(r|w)~ ` i do remember that we decided to treat delete as write, but i can't find any written reference for that (and also the top comment may be outdated)",0,0,0,0.9129075407981871,0.9818853735923768,0.9566394686698914,0.0,accept,unanimous_agreement
1014080990,9974,"in one of the meetings we discussed that anything more fine grain than read and modify was likely too hard to conceptualize. so i dropped it from the implementation here. i can post that comment back on the main cr. i was trying to convey that if the root segment had two key permissions, `%r~*` (read permission on all keys) and `~%r~*` (all permissions on `%r~*`) they would show up as the same string, namely ""%r~*"". so i was trying to differentiate the two by only showing the full key permissions in the selector field.",0,0,0,0.9224767684936525,0.96400648355484,0.9811804294586182,0.0,accept,unanimous_agreement
1014263219,9974,"when you say do you mean `%r~*` is key name pattern (keys starting with ""%r~"" in their name)? do you mean they'll show the same because of the de-duplication? i.e. one of them is a subset of the other? (i don't think that's the case). or they'll show the same because all permission would show as `~ ` rather than `%rw~ `? in which case i don't think that will happen since one will be shown as `%r~*` (read on all keys), and the other as either `~%r~*` (simple / old notation) or `%rw~%r~*` (move verbose notation, but still not the one the `verbose` arg in your code will use)",0,0,0,0.9837189316749572,0.990510880947113,0.9879794120788574,0.0,accept,unanimous_agreement
1015014056,9974,"i didn't follow your question, so i'll try to take a step back and explain again. the 6.2 implementation of `acl getuser` returns just the string value of the pattern. if the acl user looked like, `user foo ~foo*`, the keys field of the getuser response would just have ""foo"". now that we have key based permissions, we want to make the distinction between a keypattern that is read-write and a keypattern that is read only or write only. we could prepend the permission information onto the getuser response, in our example transforming it from `foo` to either `~foo` or `%rw~foo`. this seemed like a backwards breaking change to me, so i wanted to avoid it. we could also prepend the permission information on just the key patterns that aren't rw, so our example would stay as ""foo"", and if we added a key pattern using the new permissions, say `%r~bar`, getuser would return a list of two values: [`%r~bar`, `foo`]. the problem i was trying to describe was the case of if instead of ""foo"" the original key pattern was the literal string ""%r~bar"". then the getuser response would have returned [`%r~bar`, `%r~bar`], which is describing two separate permissions. my proposal, which is still implemented, was to show the literal string patterns in the top level of get the getkeys response. in the individual selectors, since this is net new, it returns the string patterns prepended with their permissions. if we don't think it's a backwards breaking change to prepend the permission information, that is going to be a simpler change.",0,0,0,0.9723201990127563,0.9868013858795166,0.9635633230209352,0.0,accept,unanimous_agreement
1015318619,9974,"ohh, now i understand what i was missing. for some reason i thought that `acl getuser` returns a dsl like `acl setuser`, but in fact it returns a resp response with an array of literal key name patterns the user can access (without the `~` dsl prefix). so anything we do there will be either a breaking change (adding `~` or `%` prefix), or be disambiguate since both `~` and `%` could be part of a key name / pattern. i think we can't afford the disambiguity, and unless i'm still missing something, it exists with your non-verbose. i think we should do one of the following: 1. remove this `keys` field from the map, and introduce another one like `key_rules` which is always verbose (always emits the `~` prefix and possibly the `rw` flags (can be omitted if the key has full access) 2. keep the `keys` map field, but let it include only keys that are fully accessible, and in addition to it add a `keys_r` and `keys_w` map fields with list of keys that are either read only or write only. 3. keep the `keys` map field and let it include any key that is in some way accessible (ignoring the r and w flags). and add 3 new map fields: `keys_r`, `keys_w` and `keys_rw` i.e. the reason i prefer 2 or 3 is because the `keys` field, and this entire acl getuser response is designed to be human friendly (not using the acl dsl). the difference between 2 and 3 is that 3 is in some way a bit more backwards compatible (clients will still get a list of all the keys that could be accessible). but on the other hand, if the client isn't aware of the new `selectors` map field, it'll miss some granted accesses anyway, right? so in that sense, maybe there's no need to keep the old `keys` field at all (unless maybe a case where both the new selectors feature and the new rw key access features are in use)",0,0,0,0.849669873714447,0.8583652377128601,0.7886878848075867,0.0,accept,unanimous_agreement
1015660610,9974,"i agree with most of what you said about the getuser response, except the getuser response already includes `acl dsl` for the commands response. [code block] which is why i thought moving dsl into the getuser response was reasonable. if the concern is about human readability, i'd like to propose the following: 1. the key field becomes all accessible keys, the same as your option 3. 2. we add a new key-permissions field, which is a map of the different permissions supported by the user along with the list of keys that have that permission. e.g. [code block]",0,0,0,0.9624705910682678,0.9555317163467408,0.9309488534927368,0.0,accept,unanimous_agreement
1015672723,9974,"your last suggestion is very similar to my 3rd one, considering the situation i think it's a good compromise. in any case, i think the purpose of the`keys` map field was to be human readable, so i feel that adding dsl flags into is is wrong.",0,0,0,0.7377399802207947,0.8667719960212708,0.7039220333099365,0.0,accept,unanimous_agreement
1016069265,9974,"i ran the test verifying the keys are the same between the previous implementation and the new one, see [a link] i found a couple of bugs which have been fixed, including one in your update which i submitted here: [a link] so i feel good it will have caught most things. i think all of my planned items are done now, ptal and let's see if we can close this before the end of the week. :d",1,1,1,0.987901210784912,0.9881835579872132,0.9936283230781556,1.0,accept,unanimous_agreement
1017096736,9974,/core-team i ended up with the following type of output for acls: [code block] the breaking change is that channels and keys are now shown in dsl instead of just as their patterns as flat string instead of as an array. i think it's useful to also update the channels so that it's all consistent.,0,0,0,0.9777041673660278,0.9870102405548096,0.9801778197288512,0.0,accept,unanimous_agreement
1017561277,9974,"i've reviewed the whole thing again top to bottom, found a few bugs and fixed them, please cr the last commit. i've also re-run the tests with the code that asserts both key extraction functions give the same results (put the assert in processcommand, since putting it in `call` will miss many of the test suite calls) i have a feeling that the `test various odd commands for key permissions` could be extended for more edge cases (we don't have anything that verifies the keyspecs and flags), but i don't wanna dive into it now. i think this is ready for merge.",0,1,0,0.6597010493278503,0.6878994703292847,0.4974597990512848,0.0,accept,majority_agreement
1017754798,9974,"just wanted to clarify the shard-channel keyspec stuff. in an earlier revision there was code to support both channels and sharded-channels in the keyspecs, but there wasn't clarity of it we wanted to do that, so i removed support for both. right now modules have to do their own manual acl checks for channels and there is an item in the follow up list to think this behavior through more. the issues that i saw with modules support: 1. should modules be able to also exempt themselves from acl checks, like unsubscribe and sunsubscribe does. 2. should we be able to differentiate channels and shard channels in key specs so modules can define either. shard channels route clients while regular channels can be sent to any node (for stuff like keyspec notifications).",0,0,0,0.9063872694969176,0.988110899925232,0.9659427404403688,0.0,accept,unanimous_agreement
1017769385,9974,"let's discuss modules later. what i didn't like (or actually didn't understand) in your previous attempt, was that you added key-spec flags for the old pubsub, but these commands don't declare key-specs (and i think they shouldn't, since it'll confuse cluster clients). so it's only acl that has that requirement, and i think acl should be the one to handle that (not key-specs) i don't like to see key-spec flags to suppose non-key-spec usages.",0,0,0,0.6368028521537781,0.6989076733589172,0.9420884251594543,0.0,accept,unanimous_agreement
1107745718,10273,"/core-team please have a look and comment. the design is now complete, and the ""coding"" for now covers only part of the command set (the ones that are important in order to judge this feature and run into all the complicated cases). i'd like to get a conceptual approval for that, and then merge it into unstable after 7.0 is stable, and before starting to merge any command 7.2 related changes into unstable.",0,0,0,0.9776756763458252,0.9576640129089355,0.8091878890991211,0.0,accept,unanimous_agreement
1186572454,10273,"just my 2 cents from a first glance. overall i think this will help to generate type safe interfaces for client libraries. the only things i would nag about are: #### variants to get a clean high-level api i would generate one method for each command (maybe this would be considered a medium-level api)) and if a subset of arguments causes a specific reply schema to be used, i would create a method for that too (this would be the high-level api). if i am only interested in one kind of reply schema variant i most certainly will always pass option::none or null to the arguments that are not needed for this reply schema variant. so for a very ergonomic high level interface a single method for each of those argument => response variant combinations would be desired. the questions is how one could resemble these dependencies between arguments and reply schema variants. getting to my favorite example: client kill here we have a clear mapping of old-format vs new-format (now a top-level oneof argument type) to one of the two reply schema variants. basically we have two options: annotate the arguments or annotate the reply schema top level oneof variants. while annotating the argument for the client kill command seems to be the easier solution as you can simply say e.g. ""yields: variant a of response_schema top level oneof"", this could get messy if the dependency is not as clear as with client kill. e.g. there is no top level oneof argument type. thus, i guess augmenting the reply schema would be the better (more general) solution as you can directly annotate which arguments in which combination yield the specific variant. there are a couple of open questions though. how would you address the specific arguments (they currently do not have unique identifier). what is the most powerful argument dependency for a response variant (something that is not a simple ""if argument x is passed, this is the result""). i mean in the end we (as in client library authors) can aggregate these information ourselves and feed that into the code generator but i think having this upstream would be a benefit to all client lib generators. maybe, to not block this pr, a new issue to discuss different solutions for this problem is desired? #### unique ids another thing i would like to see would be `$id`s for the different responses. this could help with associating generated types to resources in the reply schema (putting that in the docs of that type and maybe use this to derive a name for the data type)",1,0,1,0.3741374313831329,0.742925226688385,0.4919205009937286,1.0,accept,majority_agreement
1189896926,10273,"i agree we better solve this mapping of arguments to reply types. i think it could be nice to give a unique name (id) to both args and reply types, but i fear that, specifically for the args it could be a lot of work, so maybe it should be optional and we'l add it only to the ones needed. the ids for the reply type could just be indexes (like we have for key-specs), since they're not nested. what worries me is that it's hard to add a reference on each and every arg or combination as to what reply type is generates. maybe we should have some way to define a default and a list of exceptions?",-1,0,-1,0.9580799341201782,0.6524770259857178,0.904276728630066,-1.0,accept,majority_agreement
1346014720,10273,in order to solve the problem of strongly-typed languages and anyof/oneof schema i propose adding a new `condition` field to any sub-schema inside oneof/anyof. this new field will use c-style syntax with regard to the existence of certain arguments that cause this specific reply example for `set`: [code block] wdyt?,0,0,0,0.988050639629364,0.9896532893180848,0.9930588006973268,0.0,accept,unanimous_agreement
1370973068,10273,ping^,0,1,0,0.8943549990653992,0.7625365257263184,0.9891631603240968,0.0,accept,majority_agreement
1370993256,10273,"hey, thanks for the reminder and your work on this matter. nothing immediate comes to mind problem-wise regarding this solution. i have some remarks, though. i can see that the first oneof variant is the error case. it might be hard to generate good types for languages which commonly use error monads. i would appreciate it, if we could tag the error variant in some way. this should also reduce the needed amount of conditions for the error variant in some cases. on the other side, it could be hard if there are multiple error cases with different reply schemas, but they would have problems with your solution right now as well. i am uncertain if there are currently such cases in redis.",1,1,1,0.952534019947052,0.966584086418152,0.974488079547882,1.0,accept,unanimous_agreement
1371144589,10273,"parsing `condition` will make generating a client a lot harder, what do you think about something like: [code block]",0,0,0,0.9464336633682252,0.9168955087661744,0.9607110619544984,0.0,accept,unanimous_agreement
1371211255,10273,i feel this is a bit too verbose and has problems with the descriptions. if we want to remove the need to parse the condition from a different format i would change your example to something like this: [code block] with condition roughly in the following format: [code block],-1,-1,-1,0.9620893597602844,0.5976008176803589,0.7171750068664551,-1.0,accept,unanimous_agreement
1371249460,10273,i tried to mimic the original [a link].. i think we should try to stay as close as possible to the spec.,0,0,0,0.9605317115783693,0.9639431238174438,0.9869447350502014,0.0,accept,unanimous_agreement
1430192931,10273,full ci [a link],0,0,0,0.9864878058433532,0.9343080520629884,0.9955262541770936,0.0,accept,unanimous_agreement
1455713531,10273,"/core-team fyi, as discussed, we're going to merge this to unstable soon in order to proceed perfecting the schemas gradually in unstable (could take a long time, require collaboration, which will be hard to do in a side branch). for now this should have no visible interfaces, and it doesn't add any refactory (just some new code that we can delete if we someday regret it).",0,0,0,0.975442349910736,0.970935046672821,0.9621394276618958,0.0,accept,unanimous_agreement
1460695875,10273,daily: [a link],0,0,0,0.9764238595962524,0.9832409024238586,0.990952491760254,0.0,accept,unanimous_agreement
1460782643,10273,i see we have an issue with 32bit build and one of the new geo tests. the reclaim and freebsd failures are probably unrelated. waiting for valgrind but i don't expect anything to fail there.,0,0,0,0.9585399627685548,0.6728839874267578,0.9783830046653748,0.0,accept,unanimous_agreement
1462003838,10273,with fix to geo on 32bit: [a link],0,0,0,0.9884591698646544,0.9861282110214232,0.9958820343017578,0.0,accept,unanimous_agreement
1462219668,10273,"/core-team fyi, i would like to merge this one tomorrow.. let me know if you have any concerns.",0,0,0,0.7686415314674377,0.8923129439353943,0.9391993284225464,0.0,accept,unanimous_agreement
1462868398,10273,"i have no concerns, the actual change to the core seems small.",0,0,0,0.93768310546875,0.9313384294509888,0.6579002141952515,0.0,accept,unanimous_agreement
1463381272,10273,"didn't cr, only a question about the test part: does it validate the key_specs and other sections?",0,0,0,0.9876240491867064,0.980948805809021,0.986505687236786,0.0,accept,unanimous_agreement
1463585389,10273,"no, it only validates the reply schema validating the reply schemas is kinda easy, because the sot is the existing redis replies, so we just have to adjust the schemas according to them (the replies themselves don't rely on the reply-schema) validating the key specs is a bit harder because we don't have an independent sot (command getkeys relies on the key-specs)",0,0,0,0.8886476159095764,0.9427607655525208,0.9871676564216614,0.0,accept,unanimous_agreement
1463729776,10273,we can log key lookups of each command and then try to match these. but it's a topic for a different campaign..,0,0,0,0.9727112054824828,0.9723588824272156,0.9934509992599488,0.0,accept,unanimous_agreement
1907552539,10273,do we need to add reply_schema for command.json? (the same reply as command info),0,0,0,0.9889575839042664,0.9945658445358276,0.9957186579704284,0.0,accept,unanimous_agreement
1907588314,10273,"i guess that technically we do. but: 1. it's a huge pile of lines to clone 2. i hate that command command (the fact it has sub-commands but also works as a plain command), and wish we could someday deprecate it. 3. maybe the fact that we don't need it (yet, for the tests), we can hold this back for now? maybe you remember some other reason why we didn't do that?",-1,-1,-1,0.9859991669654846,0.9812204837799072,0.978846549987793,-1.0,accept,unanimous_agreement
903669447,9406,i think about three ways: * we add `dictfind` that's only used if there's some module subscribed to that event * `unlink2` is used for module types. whether we can use it for other native types? * add `notify_callback2` take addition value for module.,0,0,0,0.9876020550727844,0.9928632378578186,0.9865304827690125,0.0,accept,unanimous_agreement
903723138,9406,the last two suggestions are only valid if we're only talking about module values (specifically the module that created that key). i think the request was about keyspace notification since people may want to use it on other types of keys.,0,0,0,0.9878745675086976,0.9895582795143129,0.9924444556236268,0.0,accept,unanimous_agreement
904230785,9406,"when this notification triggered by lpop hdel, etc, the object is zero length. i suspect whether this is useful. if notification callback can access the object to be deleted, it can delete it or do something else. i think it's dangerous. if do it, i think we can add two function [code block]",-1,0,-1,0.8932123184204102,0.8934752941131592,0.783279299736023,-1.0,accept,majority_agreement
904337520,9406,"interesting idea. so it's like when we broke dictdelete into two: dictunlink and dictfreeunlinkedentry. but this time we break it in a different place (before the unlinking). you're right, that this api is becoming more dangerous the more we think of it: 1. as you said, the key the module may access may be in an odd state (like empty list) 2. it can even be in a state that can lead to a crash, maybe `o->ptr` was already released, but not set to null. 3. obviously the module can't modify the key 4. the module can't even do any other database modifications since that can get `plink` out of sync. 5. even if we only expect the module to do read-only database lookup, we have to disable the incremental dict rehashing in that sensitive time (otherwise dictfind can mess things up). * 1, 3 and 4 don't worry me too much, we'll document these and if the module violates that, that's they're mess to handle. * 5 is messy but actually easily solvable. * currently 2 worries me the most. any advise?",0,0,1,0.8797104954719543,0.4979147017002105,0.8533931374549866,0.0,accept,majority_agreement
904390849,9406,can you give an example of (2)?,0,0,0,0.9861401319503784,0.990628182888031,0.9939915537834167,0.0,accept,unanimous_agreement
904403860,9406,"no, i can't find any.. maybe i'm wrong and it doesn't exist.. it'll eventually do `decrrefcount`, which attempts to release `o->ptr` again, so for sure what i said about freeing it and not setting it to null doesn't exist, and i can't currently find anyone that frees and sets it to null either. btw, another interesting case is the rename command. it does call dbdelete, but it adds the same object (and increments the refcount) before the dbdelete call. i suppose that in this case there's a logical deletion and addition, but we're not gonna send the removed notification in that case, and we assume the module will explicitly handle that.",0,0,0,0.8217390179634094,0.969024658203125,0.9539047479629515,0.0,accept,unanimous_agreement
904474408,9406,"why aren't we gonna send the remove notification in that case? even though we don't actually release the memory, `modulenotifykeyunlink` is still called (both for the src and the dst, if exists) or maybe i'm missing something?",0,0,0,0.9820649027824402,0.9798929691314696,0.9835304617881776,0.0,accept,unanimous_agreement
904483037,9406,i don't see any call to modulenotifykeyunlink in rename,0,0,0,0.983671486377716,0.9809696674346924,0.9874935746192932,0.0,accept,unanimous_agreement
904486653,9406,it's inside `dbdeletesync` called by `dbdelete` (which is called up to two times in renamecommand),0,0,0,0.9889419078826904,0.9937166571617126,0.9950248599052428,0.0,accept,unanimous_agreement
904496903,9406,"the first call is the overwrite, so that's a plain deletion and not part of this discussion. the second deletion will indeed send that notification (don't recall what was going though my head earlier). so anyway, i guess we can consider that second notification a problem, and may wanna hide it. on one hand, logically we're deleting one key and adding another, so the removed notification is due. but on the other hand, that same object is already referenced in the dictionary elsewhere. so either we make sure to disable the notification in that case and let the module handle rename notification differently, or maybe we wanna incrrefcount of the robj, then do the deletion, and only last add it to the new key. whatever we do, we may need to handle move too.",0,0,0,0.9649108648300172,0.982155740261078,0.9494009613990784,0.0,accept,unanimous_agreement
907626587,9406,the function name maybe should change. i still wonder whether this is ok. it is a little ugly and potential dangerous.,-1,-1,-1,0.978326976299286,0.972274899482727,0.983939290046692,-1.0,accept,unanimous_agreement
957827295,9406,-house can we proceed with this? what's the status here?,0,0,0,0.9854552745819092,0.9780941605567932,0.98115736246109,0.0,accept,unanimous_agreement
958474760,9406,i'm done with it.,0,0,0,0.941129505634308,0.9399144053459167,0.6883286833763123,0.0,accept,unanimous_agreement
1217768384,9406,"it didn't make the cut for 7.0, but i'm hoping to put it in 7.2. if you can, please refresh the pr and i'll try to promote it.",0,0,0,0.8905453681945801,0.9701383709907532,0.9160299897193908,0.0,accept,unanimous_agreement
1217836614,9406,let me take some time to recall it.,0,0,0,0.9703210592269896,0.9776480197906494,0.9877232313156128,0.0,accept,unanimous_agreement
1221684054,9406,"i found a problem. in module if we call `rm_stringdma`, if the key is a string and not raw, `dbunsharestringvalue` will be called. the `val` will be freed. but we still access the `val` in `modulenotifykeyunlink`. same problem happens in `set` command. `setcommand -> setgenericcommand -> notifykeyspaceevent -> modulenotifykeyspaceevent -> rm_stringdma`. the value will be freed, but we still use it in `rewriteclientcommandvector`.",0,0,0,0.9643815159797668,0.992969810962677,0.965222418308258,0.0,accept,unanimous_agreement
1225293625,9406,"i'd like to chime in: i'm against adding a special notification for modules. i think ""loaded"" was a mistake and we should treat these things as module events.",0,0,0,0.70699143409729,0.9259313344955444,0.8760096430778503,0.0,accept,unanimous_agreement
1225302767,9406,"i think you're right about this one (being an event or some other hook, and not a key-space notification). i'm not sure it applies for `loaded` as well, but that's water under the bridge.",0,0,0,0.9562646746635436,0.9704181551933287,0.9614420533180236,0.0,accept,unanimous_agreement
1225303224,9406,"sorry for not responding to your comment, i need to dive it (on my todo list)",-1,-1,-1,0.9878551363945008,0.9931746125221252,0.9931010007858276,-1.0,accept,unanimous_agreement
1229409811,9406,"sorry for the delay, and thank you for exposing this issue. i'm guessing that it we should be rejecting rm_stringdma from within keyspace notifications, or maybe specifically if the string is now raw. the main reason to use dma if for efficiency for huge strings, so other than the inconvenience for the module to work around it (resort to other apis to read from the string without a dma), there's no real performance penalty. p.s. regardless, i've also recently discussed with about discouraging or even disallowing writes from within keyspace notifications, so maybe we need some global flag to know that we're inside a ksn? please see [a link] and let us know if you can think of another solution besides rejecting the rm_stringdma call.",-1,-1,-1,0.9852112531661988,0.9909327030181884,0.9805341362953186,-1.0,accept,unanimous_agreement
1229415152,9406,maybe we should just document that `rm_stringdma` should not be used inside key space notification (just like we plane to do with write commands) because it has a potential of changing the data?,0,0,0,0.9876227378845216,0.994242250919342,0.9825389385223388,0.0,accept,unanimous_agreement
1229423036,9406,"in some cases (reading huge strings), there's no reason not to use it. i'd rather just document (and enforce) that it can fail and the module should handle it.",0,0,0,0.9528574347496032,0.9427772760391236,0.9531151056289672,0.0,accept,unanimous_agreement
1233905416,9406,are we going with the event/hook approach?,0,0,0,0.9876976609230042,0.9923821091651917,0.9918335676193236,0.0,accept,unanimous_agreement
1233931564,9406,"yes, let's do that. please look into defining an interface for that that's separate from the ksn hooks. also, please solve the problem with stringdma, by returning an error in that case, and fixing the test module to handle it.",0,0,0,0.9873375296592712,0.9834190607070924,0.9834365844726562,0.0,accept,unanimous_agreement
1234137836,9406,you mean use `modulefireserverevent` to support this notification? fixing stringdma is also break change.,0,0,0,0.9876627922058104,0.993712842464447,0.981138288974762,0.0,accept,unanimous_agreement
1234178680,9406,"well, i was thinking of an new completely separate callback, but come to think of it the event and sub-event mechanism can maybe fit here nicely. i.e. we add redismodule_event_key and a redismodule_subevent_key_deleted (and maybe expired, and evicted too). in the future we can add other key-related events... wdyt? regarding stringdma want to fix that only for the cases where it's broken (corrupting memory), so it'll be a fix (for something that was completely broken before), not a breaking change. e.g. in some way reject rm_stringdma when used inside ksn or event hooks, on a string that's not raw encoded.",0,0,0,0.971702754497528,0.9659993648529052,0.9688296914100648,0.0,accept,unanimous_agreement
1247598935,9406,"i like the idea of using the events mechanism, i do not see a need to add a new mechanism.",1,1,1,0.8384984135627747,0.8783266544342041,0.9100540280342102,1.0,accept,unanimous_agreement
1247768075,9406,i think you can proceed with the above plan,0,0,0,0.97353595495224,0.9001743793487549,0.9837852716445924,0.0,accept,unanimous_agreement
1247780987,9406,ok,0,0,0,0.9667208194732666,0.8787186145782471,0.9233372807502748,0.0,accept,unanimous_agreement
1249985922,9406,"we simply pass a key to the events or pass the value too? without value, it is more simple.",0,0,0,0.9777612686157228,0.9925884008407592,0.9905431866645812,0.0,accept,unanimous_agreement
1250242584,9406,"setcommand -> setgenericcommand -> notifykeyspaceevent -> modulenotifykeyspaceevent -> rm_stringdma. the value will be freed, but we still use it in rewriteclientcommandvector. i tried to reproduce this scenario you describe but i could not, i could not see where we use this freed value on `replaceclientcommandvector` modified: ok found it, its when you use it with expiration option :+1:",0,0,1,0.5590083003044128,0.7457709312438965,0.5572699308395386,0.0,accept,majority_agreement
1250246284,9406,"so this is not really an issue because the value is not really freed, it is refcounted and client arguments array is keeping the refcount on it. i verified with valgrind and there is no memory violation.",0,0,0,0.9846515655517578,0.9863964319229126,0.991087019443512,0.0,accept,unanimous_agreement
1251063817,9406,after consulting with we believe that we can solve this stringdma issue in your case by increasing the `val` refcount so even if stringdma is used the value will not be freed and will still be usable. wdyt?,0,0,0,0.988255262374878,0.9757409691810608,0.990362286567688,0.0,accept,unanimous_agreement
1251692000,9406,we need `incr` before `rm_stringdma` and `decr` after `rm_stringdma` by ourself?,0,0,0,0.9895081520080566,0.9942218661308287,0.9941202998161316,0.0,accept,unanimous_agreement
1254068440,9406,"no, i actually meant that you will do it on the value that you might want to use after the unlink event finishes.",0,0,0,0.9868327379226683,0.9867578148841858,0.9910974502563475,0.0,accept,unanimous_agreement
1257118781,9406,simple change. we need to discuss that what `redismodulekeyinfo` should be. [code block],0,0,0,0.9860661625862122,0.9902141690254213,0.9921669960021972,0.0,accept,unanimous_agreement
1264586742,9406,i'm not done. still with this problem [a link],-1,0,-1,0.7530060410499573,0.5614553093910217,0.7236183285713196,-1.0,accept,majority_agreement
1264588918,9406,"that's odd, in many other places (e.g. `rm_retainstring`) we simply pass them around as equivalent, without even a casting attempt. maybe you can try to remove the struct initializer `{}` and just set the fields one by one to resolve it.",0,0,0,0.9431578516960144,0.864908754825592,0.8991748690605164,0.0,accept,unanimous_agreement
1265035001,9406,do you mean we `incr` `val` before `modulenotifykeyunlink` in `dbgenericdelete`?,0,0,0,0.9883620142936708,0.9951334595680236,0.9953468441963196,0.0,accept,unanimous_agreement
1265330269,9406,"yes, i think that that's what he meant. it'll cause `dbunsharestringvalue` to create a new one for the module, but will not destroy the original which `dbgenericdelete` holds",0,0,0,0.9840776920318604,0.986831784248352,0.9884063005447388,0.0,accept,unanimous_agreement
1265423687,9406,"i still can't resolve this problem. because in core `redismodulestring` is `robj`, but we can't use `robj` in `redismodulekeyinfov1`, we need to define `redismodulekeyinfov1` in both core and module.",0,0,0,0.942838728427887,0.9154946804046632,0.9312999844551086,0.0,accept,unanimous_agreement
1266272471,9406,"another question. why do we put this event to `modulenotifykeyunlink`. if we need expire evict, we'd better put this in front of `dbgenericdelete` and `dbgenericdelete`. then we don't need `dicttwophaseunlinkfind`, so we need not to handler `stringdma`.",0,0,0,0.9851539134979248,0.9940991401672364,0.984639048576355,0.0,accept,unanimous_agreement
1266566445,9406,"i'm not sure i understand your last question. we wanted to let the module get notified on any key that's removed from the database, before it is removed, but only if the key exists. afaik for that we either need another dictfind to make sure the key exists, or a two-phase removal.",0,0,0,0.9641191363334656,0.5628399848937988,0.7083966135978699,0.0,accept,unanimous_agreement
1266582418,9406,"sorry, i made a mistake. in `dbgenericdelete` we not always know we will remove an entry. if we want to add expire and evcit, will they be mutual exclusion with `removed`? two choices: * a key expired, we fire two event, `removed` and `expired`; * one event, `expired`?",-1,-1,-1,0.9827619194984436,0.9766070246696472,0.9854530692100524,-1.0,accept,unanimous_agreement
1266913284,9406,i think we can add another flags / hint argument to dbgenericdelete and use it in evict.c and expire.c then we pass that hint to modulenotifykeyunlink and use it as the sub-event.,0,0,0,0.9887471795082092,0.9927136301994324,0.9880996346473694,0.0,accept,unanimous_agreement
1279921623,9406,"what's the status here? we're almost done, right? any reason not to mark it as ready and ask for core-team approval? anything missing or needs a decision? other than minor things, the only major one i see is the one about evicted and expired indication.",0,0,0,0.9558899402618408,0.9712706208229064,0.9748347401618958,0.0,accept,unanimous_agreement
1279958257,9406,"we are almost done. only one left is evicted and expired, i will commit it these days. one thing is should we support `overwrite`? and something about overwrite is wrong, should we fix here or another pr?",0,0,0,0.7635284662246704,0.9495449662208556,0.8090097308158875,0.0,accept,unanimous_agreement
1279971681,9406,what do you mean about overwrite? what's wrong? i'm not sure the module needs a different indication between delete and overwrite. anyone thinks otherwise?,0,0,0,0.5486690998077393,0.5384616851806641,0.969636082649231,0.0,accept,unanimous_agreement
1279975293,9406,you metion it or we don't need it. `dboverwrite` in spopwithcountcommand is wrong.,-1,0,0,0.628116250038147,0.9146026372909546,0.9595867991447448,0.0,accept,majority_agreement
1280005030,9406,"ohh, i already forgot. well, thinking of it again, i think that semantically, unlike evict and expire, in which the deletion is not a direct result of a user action, for overwrite there's no real difference from delete + write. in fact, iirc some places in the code use dboverwrite, and others use dbdelete and dbadd. i'm not in front of the code right now, maybe we already sorted these out to be consistent. on the other hand, i guess that if we can easily provide that indication, some modules will find is useful, and others can just handle it together with normal deletion, so let's look into that. any thoughts about that?",0,0,0,0.865866482257843,0.9154641032218932,0.5279964804649353,0.0,accept,unanimous_agreement
1280455895,9406,"i think maybe the diff will look better if we don't add an argument to dbdelete, and have an implicit deleted, then in the few places that need expired and evicted, use dbdeletegeneric. regarding overwrite, i looked into it a bit and it seems complicated. for instance spopwithcount uses dboverwrite in one of its cases, although logically it's not an overwrite.",0,0,0,0.8368440866470337,0.9774453639984132,0.9704943895339966,0.0,accept,unanimous_agreement
1280466999,9406,please add documentation for the new sub-events in module.c,0,0,0,0.9789784550666808,0.9931746125221252,0.9944517016410828,0.0,accept,unanimous_agreement
1281423824,9406,"looking into the overwrite topic deeper this time, i realize is was wrong to look at `dboverwrite`, it's actually `setkey` and in fact our projects isn't actually complete without hooking into it. without that, when sunionstore completely replaces (deletes) the target key (including the ttl), the module would miss it. in the past there was a mix, some places used `setkey` and others `dbdelete` and `dbadd`, but i think i sorted this out long ago, and with the exception of restore and alike, all of the overwrite commands now use `setkey`. bottom line, i think we should add the overwrite indication, and use it in `setkey`",0,0,0,0.9109996557235718,0.9831151366233826,0.960290253162384,0.0,accept,unanimous_agreement
1281672891,9406,why module would miss sunionstore event? it calls `dboverwrite`.,0,0,0,0.9833305478096008,0.970439612865448,0.989302694797516,0.0,accept,unanimous_agreement
1281883415,9406,"i'm sorry, i think i missed something again, but then again, i still see some issues (pre-existing ones) that need to be sorted out. reviewing all the calls to `setkey`, it seems that these are all a case of overwrite (and we can add a new sub-event type for it), but some (actually **all**) of the calls to `dboverwrite` in which we already call `modulenotifykeyunlink` seem to be wrong. e.g. `spopwithcountcommand`, `incrdecrcommand`, `incrbyfloatcommand`, `dbunsharestringvalue` (e.g. on behalf of `setrangecommand`, `lookupstringforbitcommand`, etc) i.e. in all these cases, we don't really want to signal a deleted event. what i think we should do: 1. move out the call to `modulenotifykeyunlink` and also the call to `signalkeyasready` from `dboverwrite` to `setkey` 2. i think in that case we want to issue an overwrite sub-event. 3. let's rename `dboverwrite` to `dbreplacevalue` or alike, to indicate it works on the value, not the key (unlike `setkey` it doesn't mess with the ttl, and doesn't logically replace the key, just modifies the assigned value) i'd like a reassurance.",-1,-1,-1,0.9836903214454652,0.9644855260849,0.9838035106658936,-1.0,accept,unanimous_agreement
1282490418,9406,need a deep review,0,0,0,0.9484752416610718,0.9715347290039062,0.9505242109298706,0.0,accept,unanimous_agreement
1282736376,9406,conceptually approved in a core-team meeting.,0,0,0,0.9766377210617064,0.992580771446228,0.9919010996818542,0.0,accept,unanimous_agreement
1285219991,9406,will we do it in this pr?,0,0,0,0.9851129055023192,0.99372535943985,0.9945911765098572,0.0,accept,unanimous_agreement
1285757077,9406,"not sure what you're asking. i just noted that the core team discussed the general concept of the pr and okay-ed it. there's still a bit of work to do before merging it, which is to implement the [a link] i asked regarding overwrite (please go ahead and add them to this pr), and get at least one more set of eyes on both the api, and the overwrite related concerns.",0,0,0,0.9402204155921936,0.948639750480652,0.951159417629242,0.0,accept,unanimous_agreement
1293419632,9406,i just finished it and fixed some bugs.,0,0,0,0.9655287265777588,0.9745176434516908,0.9227470755577089,0.0,accept,unanimous_agreement
1296118406,9406,full ci: [a link],0,0,0,0.9845400452613832,0.8597871661186218,0.9959890246391296,0.0,accept,unanimous_agreement
1296261072,9406,i'd like another pair of eyes on this one before it's merged.,0,0,0,0.7999932169914246,0.9625135660171508,0.9512056112289428,0.0,accept,unanimous_agreement
1331902280,9406,merged (more than a year after the initial version). thank you for the patients and for following all the requests.,1,1,1,0.8977354764938354,0.8418003916740417,0.9149301052093506,1.0,accept,unanimous_agreement
913901295,9462,"for the sake of completeness, can you run your perf test again with an r values greater than 100k. i'd like to make sure the benchmark holds up when basically everything isn't in some level of the cpu cache you probably tested on. i think this is really exciting!",1,1,1,0.989589512348175,0.9945093989372252,0.994284689426422,1.0,accept,unanimous_agreement
938234485,9462,"the size of `hdr_histogram` struct from [a link] is `104 bytes` itself. the `hdr_histogram` count array is stored in `int64_t* counts;` and is of size `counts_len` when initialized. initializing a histogram with the values from [a link] i.e., [code block] would lead to the creation of a histogram with a `counts_len` of `11264`. overall the overhead size of a single histogram, according to [a link] would be `(11264 * 8) + 104 = 90216 bytes`",0,0,0,0.9838836789131165,0.99517160654068,0.9911983609199524,0.0,accept,unanimous_agreement
957879661,9462,do you wanna pick this up?,0,0,0,0.9839484095573424,0.9736962914466858,0.98969566822052,0.0,accept,unanimous_agreement
984537461,9462,this weekend will focus on it. :+1:,1,1,1,0.8643606305122375,0.9704996943473816,0.9935223460197448,1.0,accept,unanimous_agreement
999089804,9462,"and i've picked this one again, and now all tests pass, and added the ability to enable/disable this at runtime, and to configure the exported percentiles. imo missing extra testing, and extra benchmarks, that i will address in the following days.",0,0,0,0.9842814803123474,0.74712073802948,0.9599676132202148,0.0,accept,unanimous_agreement
1001717449,9462,"i've updated the benchmark results with the latest unstable vs this feature branch ( with the feature enabled disabled ) and a larger keyspace len as requested ### overhead test using redis-benchmark on unstable vs commands.latency.histogram i've tested using ping and set commands ( fast ones ) to assess the largest possible overhead of histogram latency tracking. as seen below there is no measurable overhead on the achievable ops/sec or full latency spectrum on the client. this matches the expected behaviour, given on previous microbenchmarks of the hdr_record_value (the function used) [a link] took on average 5-6 ns/op. #### results on unstable ( commit hash = af0b50f83a997ca3eb2e21fd9ee823ef15e12183 ) used redis-server command [code block] [code block] #### results on commands.latency.histogram branch with latency track disabled used redis-server command [code block] [code block] #### results on commands.latency.histogram branch with latency track enabled used redis-server command [code block] [code block]",0,0,0,0.9790505766868592,0.9905799031257628,0.9798021912574768,0.0,accept,unanimous_agreement
1001748424,9462,and i believe all requested changes have been addressed. when you have the time can you check it?,0,0,0,0.9152979850769044,0.9418650269508362,0.9441781044006348,0.0,accept,unanimous_agreement
1003905690,9462,/core-team please approve (see top comment),0,0,0,0.9820288419723512,0.9752839207649232,0.9879403114318848,0.0,accept,unanimous_agreement
1004624983,9462,does `config resetstat` zero the latency stats?,0,0,0,0.9862375259399414,0.993143916130066,0.9928820133209229,0.0,accept,unanimous_agreement
1004641564,9462,yes. on [a link],0,0,0,0.9863073229789734,0.9895840287208556,0.9923156499862672,0.0,accept,unanimous_agreement
1004728979,9462,", a concern raised by is that hdrhistogram us using malloc directly, and will not be accounted for in used_memory. we would like to change that. can you look into it?",0,0,0,0.964624285697937,0.980079710483551,0.9748329520225524,0.0,accept,unanimous_agreement
1004733408,9462,"yes, will raise it in [a link] and address the change with a pr to the project while also including the minimum changes to our deps folder :+1:",0,1,1,0.8580948114395142,0.5494624972343445,0.621390163898468,1.0,accept,majority_agreement
1005213570,9462,i've added the runtime allocator change to the hdrhistogram on the deps folder: [a link] notice that as discussed we need the additional `zcalloc_num` given the zmalloc's zcalloc is not with the right signature as calloc: [a link],0,0,0,0.9876031279563904,0.9895126223564148,0.9950302839279176,0.0,accept,unanimous_agreement
1005622797,9462,"for the record, the core-team approved this pr on a voice meeting.",0,0,0,0.971544086933136,0.991683840751648,0.9931463003158568,0.0,accept,unanimous_agreement
1005625801,9462,thank you. can you please make a redis-doc pr for latency histogram and info,1,1,1,0.9424136877059937,0.8954635858535767,0.8984563946723938,1.0,accept,unanimous_agreement
1005635935,9462,thank you all the review cycles! will do so,1,1,1,0.9774296283721924,0.9909250736236572,0.9936471581459044,1.0,accept,unanimous_agreement
1005719877,9462,-steinberg redis-doc pr: [a link],0,0,0,0.97284734249115,0.9712681770324708,0.9840098023414612,0.0,accept,unanimous_agreement
919892541,9504,modules: maybe we want something like rm_createcontainercommand + rm_createsubcommand ?,0,0,0,0.986900508403778,0.9949418902397156,0.992089867591858,0.0,accept,unanimous_agreement
919895275,9504,"please go over all subcommands' flags: i've put some thought in some (config's subcommand) but in others, i just copied the flags from the parent command (cluster)",0,0,0,0.9880790114402772,0.9911779761314392,0.9946574568748474,0.0,accept,unanimous_agreement
920655900,9504,"i didn't go deep into this pr, the subcommand idea is good, but a problem that came to my mind is about module, how to let module to register subcommands?",0,0,0,0.8131985068321228,0.7673653960227966,0.8043408989906311,0.0,accept,unanimous_agreement
920661463,9504,"i guess we should introduce a new api `rm_createsubcommand` which is identical to rm_createcommand except that it gets `const char* container_name` (and maybe also drop the old [first,last,step] scheme, not sure)",0,0,0,0.9866893887519836,0.9936176538467408,0.9873788952827454,0.0,accept,unanimous_agreement
921001125,9504,todo: 1. module api 2. `commands list [filterby (module |aclcat |group |pattern )]` 3. different acl command-id per subcommand. pay special attention to backward-compatibility. please ack,0,0,0,0.9698984622955322,0.9883861541748048,0.9935548901557922,0.0,accept,unanimous_agreement
943590840,9504,/core-team please approve. have a look at the the top comment for a comprehensive list of the changes and their implications.,0,0,0,0.970946729183197,0.9490845203399658,0.973815143108368,0.0,accept,unanimous_agreement
833697827,8887,"one more thing, we need an option to convert the encoding at rdb loading time. either when doing full sanitization, since in that case we already do o(n) operation anyway, but also maybe some people will want to do that conversion at upgrade time (slave will take longer to go online), maybe we'll even make it the default one day. i think it would also be a good idea to benchmark such a conversion, and get a feeling of how much longer it takes to load an rdb file if we do a conversion during loading vs one that just loads the ziplists as they are. can you try doing such a benchmark?",0,0,0,0.7349364757537842,0.9310889840126038,0.9250307679176332,0.0,accept,unanimous_agreement
834000583,8887,"i also do not like ""list container"", it is too difficult to come up with a good name. maybe we can convert ziplist to listpack when call hashtypeinititerator. i have write corrupt-dump-fuzzer test in #8761, i will complete these tests.",-1,0,-1,0.952961564064026,0.6718470454216003,0.5194092988967896,-1.0,accept,majority_agreement
835439285,8887,"i tested the speed of rdb loading, before(keep ziplsit) and after(convert ziplist) using the same dump.rdb, creating 100000 keys per test and filling value with random strings. it looks like the speed becomes 3 times of the original. | entries num of one ziplist | max value size| rdb loading time without convert | rdb loading time with convert| | ------ | ------ | ------ | ------ | | 256 | 64bytes | 6.888s | 18.078s | | 256 | 32bytes | 6.849s | 17.869s | | 256 | 16bytes | 6.991s | 18.0842s | | 128| 64bytes | 3.553s | 8.983s | | 128| 32bytes | 3.521s | 9.0222s | | 128| 16bytes | 3.558s | 8.982s |",0,0,0,0.9525518417358398,0.9809014201164246,0.9565141201019288,0.0,accept,unanimous_agreement
835609233,8887,"when convert ziplist to listpack, `lppushtail` is used. `malloc` will be called everytime. maybe prealloc make sense. but i'm not sure whether it can make the speed up.",0,0,0,0.9105318784713744,0.985806941986084,0.977763533592224,0.0,accept,unanimous_agreement
835637925,8887,"good idea, i try again.",1,1,1,0.879226803779602,0.9217177629470824,0.989051878452301,1.0,accept,unanimous_agreement
838405443,8887,"performance test comparison of [code block] and [code block]. data of [code block],[code block] and [code block] is 100000 entry. the main reason for the difference in [code block] speed between [code block] and [code block] is that the traversal speed of [code block] is 2/3 of [code block]. | | ziplist | listpack| | ------ | ------ | ------ | | pushtail 100000 entry | 85ms | 84.7ms | | find 2000 times from head | 1.157s | 1.851s | | index 2000 times | 1.412s| 1.458s| | validateintegrity 2000 times| 959ms| 1.739s|",0,0,0,0.9740201234817504,0.9921395182609558,0.982758104801178,0.0,accept,unanimous_agreement
838418022,8887,thanks using pre-allocated memory for listpacks results in a huge speedup in loading. | entries num of one ziplist | max value size| rdb loading time without convert | rdb loading time with convert| | ------ | ------ | ------ | ------ | | 256 | 64bytes | 3.875s| 5.675s | | 256 | 32bytes | 4.178s | 5.724s | | 256 | 16bytes | 4.087s | 5.684s | | 128| 64bytes | 1.803s | 3.237s | | 128| 32bytes | 1.747s | 2.725s | | 128| 16bytes | 1.771s | 2.695s |,1,0,1,0.5212250351905823,0.6785405874252319,0.7957485914230347,1.0,accept,majority_agreement
839862802,8887,"the conversion [a link] you showed during rdb loading looks very promising. maybe with that (rather low impact), we can afford to just always do that conversion unconditionally. if we do that, we can drop all the runtime code that allows t_hash.c to optionally work with ziplists, and the configuration that let's us run the test suite with ziplists rather than listpacks. and if we don't do that, we still need to come up with a strategy of how we gradually do that at runtime without impacting latency. /core-team wdyt?",1,1,0,0.6731116771697998,0.7463651895523071,0.7566335201263428,1.0,accept,majority_agreement
839875558,8887,i'll run test again tomorrow on a very large amount of data (millions and more).,0,0,0,0.970237135887146,0.9240272641181946,0.985341250896454,0.0,accept,unanimous_agreement
840437330,8887,"i ran another rdb load time test on a larger amount of data. due to memory limitation, only 500,000 keys are filled when entry num is 256. the data of this test, and the above test results are almost proportional, almost 50% increase in time consumption. |num of keys| entries num of one ziplist | max value size| rdb loading time without convert | rdb loading time with convert| | ------ | ------ | ------ | ------ | ------ | | 500,000 | 256 | 64bytes | 16.575s | 25.860s | | 500,000 | 256 | 32bytes | 16.710s | 26.662s | | 500,000 | 256 | 16bytes | 16.635s | 25.608s | | 1,000,000 | 128 | 64bytes | 19.541s | 28.423s | | 1,000,000 | 128 | 32bytes | 19.427s | 31.656s | | 1,000,000 | 128 | 16bytes | 17.649s | 27.367s |",0,0,0,0.9601044058799744,0.975884199142456,0.9767436981201172,0.0,accept,unanimous_agreement
841205469,8887,"i made two changes to listpack's lpfind. 1) use lpnext instead of lpskip. 2) modify entry length calculation. i conducted a test, 500 entries, lpfind 2000000 times. performance comparison: | | use lpnext | use lpskip | use lpskip and change entry cal | ziplist | | ------ | ------ | ------ | ------ |-----| | find string | 6.915s | 5.074s | 3.695s| 2.912s| | find number | 6.264s | 5.148s | 3.085s | 3.383s|",0,0,0,0.9668216705322266,0.9895125031471252,0.9674225449562072,0.0,accept,unanimous_agreement
841213594,8887,hset benchmark test. config: [code block] command: [code block] listpack(packed) [code block] ziplist(packed) [code block] ziplsit(unstable) [code block],0,0,0,0.9843809604644777,0.9935744404792786,0.9916531443595886,0.0,accept,unanimous_agreement
865357318,8887,"so using listpack seems to be slower across a lot of dimensions? since for the most part listpacks aren't really an optimization for the small hashes, do we really think this is that much better?",0,0,0,0.9596067667007446,0.9162715673446656,0.9636335968971252,0.0,accept,unanimous_agreement
865469028,8887,"yes, since [code block] uses many [code block] judgments to calculate the length of the current element, this is the main reason why it is slow. recently i've been wondering if it's possible to loop from backward to forward, maybe it can be improved to the same speed as ziplist.",0,0,0,0.963038206100464,0.9333359599113464,0.9816575050354004,0.0,accept,unanimous_agreement
866010767,8887,"we should consider the issue raised by , since listpack cannot get the size of the elements directly, this will result in listpack traversal speed being 85% of ziplist, and sometimes even less. i tried to improve listpack traversal speed in various ways, but in the end they all failed.",0,0,0,0.9651340246200562,0.9762786626815796,0.9769439101219176,0.0,accept,unanimous_agreement
866025110,8887,"please keep in mind that the reason to replace ziplists with listpacks is not in order to get better memory or speed optimization, it's actually in order to get rid of the unreadable bug-prone code of ziplist. also note that when an object (e.g. hash) is ziplist encoded, it is usually done in order to trade performance in favor of memory, i.e. we switch from an o(1) dict to an o(n) ziplist, so i don't think it really matters if listpack is 85% of the speed of ziplist. on the other hand, i don't wanna slow redis, and i'm not ready to give up (yet)! i don't (yet) agree that this performance regression is inevitable, and i think we should try to solve it (in some way or another). for instance, when i recently added the sanitization feature, i did add a performance regression, which i didn't want to live with. i couldn't find a way to avoid the regression, so instead i profiled the code and added an optimization (in a different place) to counteract the regression i added (had i added that optimization in a separate / earlier pr, my sanitization pr would have still added a regression). another thing to consider is that maybe for some reason the benchmark you're doing is unrealistic, maybe we can realize that a different, more realistic benchmark will be more forgiving to our cause.",0,0,0,0.977812647819519,0.9857043027877808,0.9833806753158568,0.0,accept,unanimous_agreement
866030487,8887,"tahnks , i'll do more testing and experimentation.",0,0,0,0.9507937431335448,0.9532877206802368,0.990026354789734,0.0,accept,unanimous_agreement
866679365,8887,"in my last commit, i made a big performance improvement to listpack, and i had been ignoring the function call overhead, which is the main reason why listpack is slower than ziplist. hset benchmark test. config: [code block] 1) command: [code block] listpack(packed) [code block] ziplist(packed) [code block] 2) command: [code block] listpack(packed) [code block] ziplsit(unstable) [code block]",0,0,0,0.9049541354179382,0.9653877019882202,0.9469186067581176,0.0,accept,unanimous_agreement
867612312,8887,"great!.. so now listpack is faster than ziplist? (at least in ""find"", on a non-realistically long list). do you know if that was mainly the interlining or skipping the call to lpstringtoint64? anyway, as i said, in normal conditions, we won't have such long lists, and on short ones the network / command processing overhead is likely to hide any differences. if we'll ever want, i'm sure we can optimize it further. but i don't think we do at this point. so the next thing to re-consider is if we can afford to force an ""upgrade"" to listpack at rdb loading, or do we need to keep the complication of supporting both formats side by side at runtime, right? do we have all the details to summon a decision about that already? or is something missing?",1,1,1,0.9338544607162476,0.9488005042076112,0.9827281832695008,1.0,accept,unanimous_agreement
868120359,8887,"mainly due to interlining. all methods of listpack will bring improvements, i will test all commands. yes, this test is still in progress. in yesterday’s test, the speed of convert listpack to ziplist has also been improved.",0,0,0,0.9595776796340942,0.9787747859954834,0.9468865394592284,0.0,accept,unanimous_agreement
869539715,8887,"the consumption of listpack to ziplist is mainly lppushtail, and it is not affected by the size of ziplsit item, the conversion time depends on the total number of all ziplsit. for example, if there are 1 million keys, each key has 128 entries, then the conversion time is proportional to the number of entries (1 million * 128). on my pc, 128 million entries are converted in about 7s. |num of keys| entries num of one ziplist | entry size | loading time without convert | rdb loading time with convert| | ------ | ------ | ------ | ------ | ------ | | 1 million | 128 | 8 bytes | 5.893s | 12.747s | | 1 million | 128 | 16 bytes | 7.263s | 14.727s | | 1 million | 128 | 24 bytes | 9.794s | 16.058s | | 1 million | 128 | 32 bytes | 11.636s | 18.537s | | 2 million | 64 | 8 bytes | 7.234s | 14.366s | | 2 million | 64 | 16 bytes | 8.759s | 16.478s | | 2 million | 64 | 24 bytes | 11.095s | 18.372s | | 2 million | 64 | 32 bytes | 14.413s | 20.607s | | 4 million | 32 | 8 bytes | 9.452s | 16.886s | | 4 million | 32 | 16 bytes | 10.685s | 18.975s | | 4 million | 32 | 24 bytes | 13.474s | 20.824s | | 4 million | 32 | 32 bytes | 16.041s | 22.712s | btw. after testing other hash-related commands, the tested hash's size is 512, listpack and ziplist have similar performance.",0,0,0,0.9678512811660768,0.9893444180488586,0.9915626645088196,0.0,accept,unanimous_agreement
870370971,8887,"/core-team we need to take a decision about conversion of ziplists to listpacks. our raw options can be: 1. don't convert old ones, just create new ones as listpacks (we can never get rid of ziplist from our codebase this way) 2. convert each key when modified for the first time (we need to keep maintaining runtime support for ziplist encoded objects, and also have the test suite able to reach that code somehow). 3. convert all ziplists to listpack on rdb loading, so that there are never any ziplists at runtime (this is an o(n) operation on upgrades) please have a look at the above benchmark and share your thoughts.",0,0,0,0.9527405500411988,0.9897820949554444,0.9636618494987488,0.0,accept,unanimous_agreement
874187999,8887,we discussed that topic in the core-team meeting. we would like to proceed with option 3 mentioned above. i.e. to convert all ziplists to listpacks on load time. i suppose it means the code (and tests) can be simplified a lot now.,0,0,0,0.9776625037193298,0.965974748134613,0.9665558934211732,0.0,accept,unanimous_agreement
874393180,8887,"great, i continue to make changes.",1,1,1,0.7589078545570374,0.9758782982826232,0.9810091257095336,1.0,accept,unanimous_agreement
874684916,8887,"i'm not sure if we need to make t_hash compatible with ziplist, if not, [code block] is not necessary.",0,0,0,0.8756583333015442,0.9177019596099854,0.9558749198913574,0.0,accept,unanimous_agreement
874732876,8887,yeah.. i'm sorry for both of us investing so much time on that.. we can drop it.,-1,-1,-1,0.98911452293396,0.9898569583892822,0.9943894743919371,-1.0,accept,unanimous_agreement
876986860,8887,"i add a new corrupt-dump test for [code block]. it doesn't error in the old code, because in the old lpskip method, [code block] returns 0 when `p` is corrupted and [code block] will return 1, which then triggers the out-of-bounds assertion.",0,0,0,0.9874868392944336,0.9941609501838684,0.99441260099411,0.0,accept,unanimous_agreement
877909422,8887,"yes, it was found thanks to fuzzer, and the assertion was added to lpentrysizeunsafe.",0,0,0,0.983667016029358,0.969825804233551,0.9906532764434814,0.0,accept,unanimous_agreement
880454690,8887,i've reviewed the recent changes. lgtm. are there any other topics left to code or decide before i do the final top to bottom review and merge this?,0,0,0,0.9842504262924194,0.9869897961616516,0.9919386506080629,0.0,accept,unanimous_agreement
880691922,8887,"there are still some leftover method names and comments that need to be modified, i can't be in front of the computer for a few days, i'll come back and make changes.",0,0,0,0.9760586023330688,0.9343952536582948,0.9440157413482666,0.0,accept,unanimous_agreement
882382397,8887,it's ready for review.,0,0,0,0.9640316367149352,0.9127739667892456,0.9692403078079224,0.0,accept,unanimous_agreement
893782051,8887,"thank you.. i added two minor comments about naming an doc comments. next thing is to merge recent unstable into it, you'll get some conflicts since some code from this pr found its way to unstable already, and then we can merge it.",1,1,1,0.968535840511322,0.957721710205078,0.9678704738616944,1.0,accept,unanimous_agreement
1131624604,8887,"i have a question. it can still support old rdb version? if we try to migrate redis 6 to redis 7 with replication, maybe redis 6 creates rdb with old version(using ziplist), does redis 7 load it(redis 6 rdb)? it is very common pattern to upgrade redis server.",0,0,0,0.9783438444137572,0.980207860469818,0.9412082433700562,0.0,accept,unanimous_agreement
1131626477,8887,"-lemontree indeed, it is backward compatible.",0,0,0,0.9805507659912108,0.9298043847084044,0.9294582605361938,0.0,accept,unanimous_agreement
1296250741,11303,"please avoid force-pushes, it's harder for me to keep track of what changed since my last review. also, if you can go over the comments and mark the resolved ones as resolved, so it'll be easier to focus on what's left.",0,0,0,0.907889723777771,0.977762758731842,0.9879014492034912,0.0,accept,unanimous_agreement
1305777263,11303,"/core-team please approve, although basically there's no real interface change here other than a new output for object encoding. it doesn't add any new config or change in the rdb format, can be thought as just an optimization for a single-node quicklist to avoid the quicklist header overhead.",0,0,0,0.982435941696167,0.9905310273170472,0.9877352118492126,0.0,accept,unanimous_agreement
1308341260,11303,pr was conceptually approved in a core-team meeting. anything left before merging it? can you please add an example case of the memory impact in the top comment. e.g. the effect on a list of 5 items of 10 chars each.,0,0,0,0.9873768091201782,0.9923513531684875,0.992381989955902,0.0,accept,unanimous_agreement
1308341980,11303,"ok, working on it.",0,0,0,0.9789299368858336,0.9321543574333192,0.959673285484314,0.0,accept,unanimous_agreement
1309185060,11303,"i added an example memory saving at the top (obtained with memory usage). the 40 bytes savings you estimated was just the quicklist struct, without the quicklist node, the saving is actually about 80 bytes.",0,0,0,0.9835280179977416,0.9920885562896729,0.9942395687103271,0.0,accept,unanimous_agreement
1309190694,11303,"regarding the benchmark using `redis-benchmark`, i recommend using at least 2 threads. otherwise, `redis-benchmark` tends to be the bottleneck rather than redis itself. in another benchmark, i noted using `top` while the benchmark was running, redis-benchmark being on 100% cpu and redis-server on ~95%.",0,0,0,0.9824130535125732,0.991956114768982,0.9881943464279176,0.0,accept,unanimous_agreement
1309196589,11303,i now notice an `!!waiting for completion` section in the top comment. so i'll wait to hear if we want to do that or skip it.,0,0,0,0.908519446849823,0.9620683789253236,0.9517459869384766,0.0,accept,unanimous_agreement
1311647716,11303,"please see the top comment again, i think the benchmarks are as expected except for the lrange. i tried testing it on another branch, and it saved 1-2% performance, the downside is that the readability got worse, not sure if i want to implement it. [a link]",0,0,0,0.9375683665275574,0.8880378603935242,0.8470536470413208,0.0,accept,unanimous_agreement
1311679135,11303,"i'm ok with applying the ""optimization"" (moving the `if` outside the loop, and repeating it). let's just add a comment that ""we're not using listtypeiterator in order to optimize the loop"". please apply and update the top comment.",0,0,0,0.9872108697891236,0.8226767778396606,0.978576123714447,0.0,accept,unanimous_agreement
1311720142,11303,"i see you added `--threads 1` to the benchmark command line. doesn't this too mean it's single threaded? anyway, i tested `./src/redis-benchmark -n 1000000 --threads 1 -r 10000000 eval 'for i=1,50,1 do redis.call(""rpush"", ""lst:""..argv[1], ""hellohello"") end' 0 __rand_int_` and i see that redis-benchmark is on 30% cpu while redis-server is on 100% cpu so that concern is not valid in this case. :-) a few more benchmarking ideas. in order to maximize the work done on the data structures rather than networking, you could use pipelining too (e.g. `-p 10`) and i'm not sure how much cpu is used by lua. without lua we can probably see even better results. i don't think it's necessary to redo the benchmark. just good to know about.",0,0,0,0.9693260788917542,0.9770127534866332,0.9924626350402832,0.0,accept,unanimous_agreement
1311730812,11303,"grateful indeed for your suggestions, i'll try again. the reason for using 1 is that redis-benchmark is almost 100% utilized and i am not sure if the vm performance is too bad.",1,-1,1,0.8342221975326538,0.948805332183838,0.9756113886833192,1.0,accept,majority_agreement
1312719876,11303,"i tried the benchmark with a new hw (intel xeon cooper lake, 8cores, 16g mem). it looks like [a link] doesn't offer a benefit, it just reduces the performance in the case of lack of performance will be a little faster, but still 20% slower than the old code of `addlistrangereply()`. please see the new benchmark results in the top comment, `lrange` performance will drop a little.",0,0,0,0.964322865009308,0.9723702669143676,0.976567804813385,0.0,accept,unanimous_agreement
1312743748,11303,"here is what is seen via perf. there is basically no difference between 1 and 2, and when i remove the `else if (o->encoding == obj_encoding_listpack)` from 2, 2 and 3 are basically the same, so the performance drop is unavoidable. 1) this pr ![a link] 2) [a link] ![a link] 3) unstable ![a link]",-1,0,0,0.5126920938491821,0.9851048588752748,0.923721194267273,0.0,accept,majority_agreement
1313551751,11303,"i re-benchmarked using memtier_benchmark with pipeline, but note that i still use `--threads=1`, otherwise redis-server would reach 100% cpu usage. the results are close to redis-benchmark, with `lrange` reaching a performance degradation of <=0.5. btw, i didn't test `lpop `, `rpop` and `lset` with memtier_benchmark, because their ops results have problems (probably a bug in memtier_benchmark).",0,0,0,0.9812160134315492,0.98887038230896,0.9837941527366638,0.0,accept,unanimous_agreement
1313616165,11303,"redis-server 100% cpu usage is what we want, isn't it?",0,0,0,0.9855834245681764,0.984745979309082,0.987272560596466,0.0,accept,unanimous_agreement
1313675650,11303,"yes, we do want the server (cpu) to be the bottle neck, otherwise, it means something else is the bottleneck and we're not measuring the throughput of redis.",0,0,0,0.9680837988853456,0.9477066397666932,0.9872129559516908,0.0,accept,unanimous_agreement
1313685137,11303,"yes, that's what i did yesterday (100% cpu usage), normally the new code would be slower than the old code, but when i didn't use `-threads=1` yesterday, the test results confused me, the new code was sometimes as fast as the old code, making me wonder if the 100% usage was causing the uncertainty. anyway, let me do it again.",0,0,0,0.9464102983474731,0.9196527600288392,0.8592071533203125,0.0,accept,unanimous_agreement
1315208649,11303,"prepare dataset : `redis-benchmark -q -n 1000000 rpush mylist hello` benchmark: `memtier_benchmark --hide-histogram -n 50000 --pipeline=10 --command=""lrange mylist 0 300""` unstable [code block] this pr with commit [code block] this pr without commit [code block] the performance degradation is mainly caused by the inline of `addlistrangereply()`. the code in the loop in `addlistrangereply()` will be expanded in the old code, but not in the new one, resulting in a lot of function call overhead. in the last commit, i split the processing of quicklist and listpack into smaller methods, making it easier to expand the iterators in each of them, which works as evidenced by the test results and perf. but i'm not sure if it works under a different compiler or platform. wdyt? ![a link] ![a link]",0,0,0,0.979895830154419,0.991738736629486,0.9903188943862916,0.0,accept,unanimous_agreement
1315244650,11303,"so the difference between your last commit and [a link] is that you've put each of these in a separate function instead of an if-else in the outer function, and that allowed for a better inlining? i don't think we wanna go deeper in that direction (being one step away from manual inlining of code, or conversion to assembly). moving the `if` outside the loop should have been enough. if it isn't then we can mark these functions as `inline` or convert them to macros, but in this case it's not applicable (unless we want to make two variants of each of these, one as macro and the other without). bottom line, if what you did was enough to convince the compiler to inline, i'd be ok with that (just ask to write a comment in the code specifying that). i'd like to check that it works on both gcc and clang (some recent version), but i don't think it'll change anything if we find out it doesn't.",0,0,0,0.9703823328018188,0.9759768843650818,0.9871812462806702,0.0,accept,unanimous_agreement
1316843960,11303,"1. (unstable + clang)(1) is slower than (unstable + gcc)(3) because gcc will inline the code in the loop, but clang will not. so 4 will also be faster than 2 2. commits [a link] is always valid, whether it's clang or gcc, it's a little faster than old code benchmark: `memtier_benchmark --hide-histogram -n 50000 --pipeline=10 --command=""lrange mylist 0 300""` ### clang 14.0.0 1) unstable [code block] 2) this pr with [a link] [code block] ### gcc 11.3 3) unstable [code block] 4) this pr with [a link] [code block]",0,0,0,0.9798451066017152,0.9626739621162416,0.9838539361953736,0.0,accept,unanimous_agreement
1316868526,11303,"so clang didn't used to do any inlining in that code, and thus unaffected by the change in this pr anyway. and for gcc, which did do inlining, your recent change convinced it to keep doing it, and thus no regression either. did i get it right? in which case i think we're good to go (i rather not dive into macros, and just let the compiler do what it can).",0,0,0,0.964896559715271,0.8747747540473938,0.8572757840156555,0.0,accept,unanimous_agreement
1316873035,11303,yes.,0,0,0,0.969875693321228,0.98186594247818,0.9851860404014589,0.0,accept,unanimous_agreement
1316966194,11303,please update the top comment and let me know when ready to be merged.,0,0,0,0.9838297963142396,0.983979105949402,0.9941674470901488,0.0,accept,unanimous_agreement
1317079082,11303,"updated, mainly the lrange part, which seems to bring a huge performance boost when list is listpack encoding, and i verified this boost on my local pc.",0,0,0,0.9723488092422484,0.9691302180290222,0.9746062159538268,0.0,accept,unanimous_agreement
1321075039,11303,wanted to take a moment thank you again for the time you invested in this project and others!,1,1,1,0.9534881114959716,0.8771839141845703,0.9725459814071656,1.0,accept,unanimous_agreement
882898143,8621,"/core-team hey, wanted to get your guy's opinions about a couple of open questions. skip to the third section if you are aware of the context. i know you're away, but take a look when you're back as well. ## high level changes - three new commands will be introduced: publishlocal, subscribelocal, unsubsribelocal that add support for ""local channels"". (feel free to suggest alternative naming now) - subscribing and publishing to a local channel works just like a regular channel, except it exists in a separate namespace and is bound to a slot. example: - client a does a localsubscribe to local channel foo - client b does a normal subscribe to channel foo - client c does a local publish to channel foo. client a receives the message. - local channels will be assigned to slots based on the crc16 mod 2^14, like they are for regular keys. - sending a local message to a server not serving the slot will be redirected to the primary that owns the slot. - subscribing to a local channel on a server not serving the slot will be redirect to the primary that owns the slot. - messages are broadcasted between nodes over the clusterbus and not via replication. this reduces write amplification from the number of nodes in the cluster to the number of nodes in the shard. - local channels are supported in cluster mode disabled, but don't provide any specific value. - patterns are not in the initial scope. ## assumptions that are staying the same for local channels 1. local messages only provide at most once delivery guarantees 2. there is no ordering guarantees between servers, if you are subscribing on a replica you might get a different order subscribing on the primary. 3. there is also no delivery guarantees for local messages originating from the same client. if you are client that sends 3 messages, the middle one can be dropped.(in this case the clusterbus can drop the message) messages will not be re-ordered though. ## open questions here are the open questions that have come up while reviewing that the requester wanted to close on before continuing. they're summarized as multiple options, i've included my recommendation. - should publishlocal be restricted just to primaries? - today only primaries own slots, replicas have a lagged view and might disagree. therefor in a cluster down scenario a replica could accept a publish it doesn't know it should redirect (this might change with some of the cluster v2). forcing publishing to happen on primaries will give us a stronger consistency model, but it's pubsub, so it's not that important. <- my preference - the reason against publishing just to primaries is that today cluster pubsub allows you to send to wherever you feel like. since it's not generating any write traffic into the keyspace, publishing on replicas will better diffuse the load across the shard. - should the channel be considered a ""key"" and be included in the key positions of ""command"". also, internal implementation detail, should it be considered a ""key"" to simplify command routing? - a channel is not a key, and should not be considered one. the `command` command will list it as having no keys. internally we will add code to getnodefromquery() for routing. <- my preference - it's ""basically a key"", so might as well re-use the existing key specification (or used/extend the keyspec that was proposed). we will need to explicitly handle the cases where getkeys() is called like client side tracking. - what is the behavior of pubsublocal commands during slot migration? (including replica migration) only one option listed here, but maybe there are others? - during migration, it is assumed that all channels are ""immediately migrated"" to the target. as soon as a slot is moved into the migrating state, all clients connected are disconnected. - the migration process needs to start with the target being put into importing before the source is put into migrating. i believe that is the required path, but worth validating. - what is the ""no longer serving slot behavior"". i.e. when a replica migrates away or when a slot migration happen, what happens to clients subscribing to the local channel. - disconnect the client after all the pending messages have been written out. clients should know how to handle this case, and seems consistent with other mechanisms we have today like acl changes. <- my preferred option - we can send a message indicating that the server is no longer serving a given slot. this requires clients to understand the message and take action. seems more complex. - can a single client subscribe to multiple different slots? - be consistent with cluster, and only allow consuming from a single slot at a given time. you can mix the global channels + local channels. <- my preferred option - allow consuming from multiple slots. this doesn't really break any consistency guarantees and allows for fewer connections. however it seems semantically wrong, since regular cluster forces you not to mix slots. - should subscribing on a replica require the `readonly` command to be executed: - yes, since replicas world view can be stale, it seems like we should maintain the assumption. ←my preference - the counter argument is that pubsub has poor guarantees today, so it would unnecessary to require the readonly command.",1,0,0,0.6091312170028687,0.517113208770752,0.6554068922996521,0.0,accept,majority_agreement
883639994,8621,"not sure how i agree about these limitations: regarding open questions: 1. i also feel that we want to only accept local publish in masters, seems that following the existing practice we won't run into new surprises. 2. i agree these are not keys, but we'll need command command to give some hints to clients one way or another (unless we expect them to handle this command explicitly). maybe with the changes we plan for this mechanism in 7.0 we can show them as keys, but also add some flag indicating that they're not. (some way of still supporting old clients with a good default). 3. i agree that the easy thing is to drop the client when the slot is migrated, it's also easier for the client maybe. but maybe we can add a feature in which the client declares ahead of time if it can handle a slot invalidation message, in which case we send one rather than drop it (more efficient in case multiple channels are used by one client), we can add such a feature in the future (not a must for the first version) 4. i think a single client should be able to subscribe to multiple slots. i also said at the top that i think this feature should be supported in non-cluster mode, i think these two go together. i feel i'm not knowledgeable enough in these topics, so take my opinion with a grain of salt.",0,0,0,0.7307726144790649,0.9626471996307372,0.7747275829315186,0.0,accept,unanimous_agreement
883721797,8621,"* i updated to cluster mode being in scope, i suppose the initial thought was that there was no value, but it's also trivial to support, so there is no harm. the name doesn't make as much sense though? * nothing explicitly requires patterns, if we feel strongly about that we can do it. * the cross slot subscribe is the position i hold the least strongly. one thought i've had in the past is that you should be able to send a command like client allow-multi-slot, which is just a signal that you are making an optimization to fetch data from multiple slots. in this case, if you want to subscribe to multiple local channels, you just send that command first (similar to readonly)",0,0,0,0.9237293601036072,0.9308455586433412,0.9666078090667723,0.0,accept,unanimous_agreement
922069361,8621,"few of the considerations made. * local channels are allocated to the same node as keys (same hashing logic). * client(s) will be redirected to the master node of the slot if they try to connect to a node not serving the slot. * client(s) can connect to replica node(s) serving the slot and perform operations. * global channels and local channels share the acl rule. * client(s) will be disconnected on slot migration, need to reconnect again. * client(s) can connect to channels across multi slot. * client(s) can connect to both local and global channels together. * client(s) would enter `pubsub` mode either on local/global channels subscription. * during the `pubsub` mode, only certain commands are allowed i.e. `pingcommand`, `subscribecommand`, `subscribelocalcommand`, `unsubscribecommand`, `unsubscribelocalcommand`, `psubscribecommand`, `punsubscribecommand` and `resetcommand`. * read_only flag doesn't impact pubsublocal feature. as pubsub operations are neither read/write. * no additional operation to be done during slot migration. local channels are migrated as part of `cluster setslot node ` please review the pr.",0,0,0,0.9792094230651855,0.992964506149292,0.9881148338317872,0.0,accept,unanimous_agreement
963677293,8621,"thanks for your feedback. i've addressed them, please have a look.",1,1,1,0.8157575726509094,0.777089536190033,0.9647810459136964,1.0,accept,unanimous_agreement
980835330,8621,can you rebase with the latest changes? i made some minor updates as well.,0,0,0,0.9880164265632628,0.987398624420166,0.9953238368034364,0.0,accept,unanimous_agreement
983808321,8621,will update it tomorrow. :thumbs_up_light_skin_tone:,1,0,1,0.7253396511077881,0.9804880023002625,0.99171781539917,1.0,accept,majority_agreement
985968499,8621,updated.,0,0,0,0.975935995578766,0.9856141209602356,0.9768958687782288,0.0,accept,unanimous_agreement
993156637,8621,thanks for the feedback. i feel this could be easily added as a follow up post 7.0. why do you think we should add it right away? would love to know your thoughts.,1,1,1,0.9600372910499572,0.983827829360962,0.9938894510269164,1.0,accept,unanimous_agreement
993414462,8621,"by ""now"" i meant redis 7.0 (so we don't end up with backwards compatibility concerns, and troll client libraries with separate changes). anyway, i don't have a concrete opinion about it, was just responding to [a link] to state my thoughts.",0,0,0,0.870853066444397,0.6524432301521301,0.9695355892181396,0.0,accept,unanimous_agreement
1003660244,8621,"the reason this wasn't merged was i found a bug while running cluster tests before merging, the keyspec was off. i patched it and am running tests again. [a link]",0,0,0,0.932918667793274,0.966658890247345,0.9913830161094666,0.0,accept,unanimous_agreement
1003810916,8621,"tests are all good now, merging: [a link]",0,1,1,0.5208027362823486,0.8145282864570618,0.5882741212844849,1.0,accept,majority_agreement
1008263338,8621,can someone look into this ci failure: [a link] [code block],0,0,0,0.9815360903739928,0.9911211729049684,0.9931334853172302,0.0,accept,unanimous_agreement
1008463803,8621,"seems like that isn't related to this issue, seems related to the [a link]",0,0,0,0.9357768893241882,0.984744131565094,0.9863806366920472,0.0,accept,unanimous_agreement
1013823779,8621,"looks like the test hung here (saw it only once, so may be a false report) [a link]",0,0,0,0.9870933890342712,0.9833819270133972,0.9868234395980836,0.0,accept,unanimous_agreement
1014072824,8621,"fyi. i'm not aware of any place it's likely to get stuck, let's keep a look out if it happens again.",-1,-1,0,0.9787065982818604,0.9091331362724304,0.9467328786849976,-1.0,accept,majority_agreement
1014404934,8621,test hung on my fork : [a link] i had some changes on this branch but don't think it effects this test.,0,0,0,0.9806484580039978,0.5182262063026428,0.979777693748474,0.0,accept,unanimous_agreement
1014421959,8621,taking a look.,0,0,0,0.9567191004753112,0.9551520943641664,0.9651147127151488,0.0,accept,unanimous_agreement
1014497331,8621,"[code block] just realized in both failures, previous test case failed. maybe, this is not related to this pr at all. maybe it's about [a link] , something is leaking to pubsub test after the failure. `node-timeout` is set to 1 hour in that test, maybe that's why pubsub test hangs? just fyi.",-1,0,0,0.5169596076011658,0.8898074626922607,0.95720773935318,0.0,accept,majority_agreement
1021633856,8621,"thanks for the work on this ! sorry for commenting on an already closed pr but i just want to point out a small inconsistency between the behaviour of redis-cli for `subscribe` and `ssubscribe`. namely, when we run `ssubscribe` we don't enter [a link] as we do with `subscribe`. let me know if you'd prefer this in an issue.",-1,1,-1,0.9809051752090454,0.7824334502220154,0.6649206876754761,-1.0,accept,majority_agreement
1021972998,8621,"-kolevska i get the below view after `ssubscribe`, based on this change [a link] is there anything else expected ? [code block]",0,0,0,0.9842960834503174,0.9863941073417664,0.9903278946876526,0.0,accept,unanimous_agreement
1022067420,8621,"i'm sorry, this was a silly (and embarrassing) mistake, i had ran redis-cli from my path by mistake, not from unstable. works as expected from unstable.",-1,-1,-1,0.9877958297729492,0.9935001730918884,0.9954804182052612,-1.0,accept,unanimous_agreement
1022172694,8621,"-kolevska ah, makes sense. np.",0,0,0,0.9688720107078552,0.9418843984603882,0.942221462726593,0.0,accept,unanimous_agreement
953141117,9656,"comments from oran: 1. remove both help.h and commands.c from makefile. document somewhere how to re-gen commands.c 2. in the command json files, i suggest to put the arity, acl, key_specs and all other short metadata before the arguments (put the short ones first and the long ones last) ( fyi - it's better than alphabetical ordering - basically have ""arguments"" as the last field, and sort alphabetically the n-1 first fields) 3. i'd rather use ""resp3"" over just ""3"" in the json files. 4. re-arrange return_types: it should be a list oj objects. each object will have ""summary"", ""type"" (which can be either a string or an object of resp2 and resp3) and ""contant_value"" (optional, only for simple string) 5. delete return_summary from jsons 6. split the doc in commands.c - the first half should be next to struct rediscommand. 7. try to avoid using the preprocessor in commands.c 8. turn all non-freetext fields into enum (command.group, arg.type and return_type.type) we will no longer need rediscommandvalueargtype (see [a link]",0,0,0,0.9671897888183594,0.9727153778076172,0.9683282375335692,0.0,accept,unanimous_agreement
954929504,9656,"(note to self: pushed fixes to all above, except (7))",0,0,0,0.9856734275817872,0.9899126887321472,0.9942441582679749,0.0,accept,unanimous_agreement
971725858,9656,"i'm addressing the open issues at the top comment: because the differences between resp2 and resp3 are only semantic, i tend to say it probably makes sense to have a single `returns` element (and description) that refers to both versions, and branch only the `type` part as you listed in the example. if this proves to be not good enough, i'd just consider the entire `returns` element version specific and include an optional `versions` field that lists applicable protocol versions (with default being `versions: [2,3]`). i think we need to choose one of these two approaches but not both. should they have a ""type""? usually, type is used to validate some free-test user input is the token itself be under ""token"" or under ""value""? if it's under ""value"" we can declare it as a mandatory attribute i think there can be a `type: flag` that indicates there are no user supplied values, and the specified value indicates that flag was set.",0,0,0,0.9567307829856871,0.9793786406517028,0.9794853329658508,0.0,accept,unanimous_agreement
974777459,9656,"i've started to work on integrating these changes into `redis-cli.c`. so far i see at least two issues: 1. `static char *commandgroups[]` is missing - this is used for outputting command help. 2. `struct rediscommand` is defined in `server.h`. this requires `redis-cli.c` to include `server.h` for the struct definition. aside from the massive header coupling this introduces (`server.h` desperately needs to be broken up regardless), it's incompatible with `redis-cli.c`, which uses the `hiredis` implementation of `sds.h` instead of the one included from `server.h`. i'm not familiar with the differences between these implementations so i don't know how important this is. but it seems to me a poor design choice to require a client library to include `server.h`. the command-related definitions should be broken out into a separate header file.",0,0,0,0.9608392715454102,0.9936322569847108,0.9710855484008788,0.0,accept,unanimous_agreement
981069745,9656,redis-cli should not depend on any structs/definitions from redis itself. it should execute command on startup and build its own structures based in what command returned,0,0,0,0.9876031279563904,0.9894368648529052,0.9909494519233704,0.0,accept,unanimous_agreement
981070055,9656,got it - that makes sense. i'll give it another shot.,0,0,0,0.8031555414199829,0.8882083296775818,0.8801869750022888,0.0,accept,unanimous_agreement
983976104,9656,"/core-team please approve. have a look at the top comment for details, as well as the pending redis-doc pr if you want.",0,0,0,0.9709007740020752,0.9518353939056396,0.9832645058631896,0.0,accept,unanimous_agreement
993163998,9656,":clapping_hands::clapping_hands::clapping_hands: i don't have any major concerns. the only thing i'm not certain about is the module api. i would almost prefer we pull that out so we can get the rest of this merged and start guiding new prs to create the new json files. chosen option sounds fine. when a module sets ""since"", should it be the debut redis version or the module version? i don't see any other option but to show the module version. definitely the f$*-it option.",1,0,1,0.727468729019165,0.634006917476654,0.7682842016220093,1.0,accept,majority_agreement
993349251,9656,"we discussed this pr in a core-team meeting. the **only** thing that's possibly an issue is the module api. we want to try and invest more time on searching for nicer alternatives (which will be easier for the module authors to use). one example is maybe expose apis that take compound structs like the ones in commands.c. we think the quick way forward is to trim the module api (and tests) from this pr, merge it, and open another one to introduce them. wdyt?",0,0,0,0.916900396347046,0.9626889824867249,0.9674682021141052,0.0,accept,unanimous_agreement
994936562,9656,full ci: [a link],0,0,0,0.9845400452613832,0.8597871661186218,0.9959890246391296,0.0,accept,unanimous_agreement
1421157765,11659,"we are stuck between discussing two options for the module authentication and how it works with blocking and would like your input. both options assume that we are creating a new authcallback function that is being registered, and it will be executed in either `auth` or `hello`. 1. have the blocking behave like module blocking, where a custom callback is executed when the client is unblocked. e.g. [code block] this option is a little bit odd since we are configuring module blocking for a normal command, but is more consistent with out module blocking. 2. have the blocking behave like the new key based blocking introduced ([a link] where the command is reprocessed when the command is unblocked. e.g. [code block] i think this type of blocking should be included as a new way for modules to block, so we can setup some of the primitives here. overall i think this is a simpler api for modules to implement against. we just need to maintain a little bit of state for whether or not we have process a given authentication handler.",0,0,-1,0.8135436177253723,0.9564915895462036,0.510637640953064,0.0,accept,majority_agreement
1422157192,11659,"i haven't thought about this too deeply, and i must say it scares me a bit that we have to block the client and do asynchronous authentication. looking at your two proposed alternatives, i think i prefer the first one, letting the module control it more closely and explicitly. unlike key locking, i think this is a lower level api that's ok to be more complicated.",-1,0,-1,0.7697792649269104,0.5889166593551636,0.9133954048156738,-1.0,accept,majority_agreement
1423107819,11659,"the question is not about control. the module still has the ability to trigger a given callback or modulate the behavior of the same function. looking back, i think the way i framed it wasn't actually helpful. the current implementation is basically adding a new type of blocking, which is specific to just authentication, which i would rather not have. the question is more about do we want this to conform more to module blocking, which has two different flows for the initial call and the unblock flow vs the new blocking code, which has the same flow for both blocking and unblocking flows. edit: we discussed more internally and came to a conclusion.",0,0,0,0.9525448083877563,0.906217098236084,0.9659643173217772,0.0,accept,unanimous_agreement
1423112042,11659,"yes, we need to make sure we do it right.",0,0,0,0.9616044163703918,0.955280900001526,0.9646568894386292,0.0,accept,unanimous_agreement
1444542535,11659,"/core-team hey, i think this is in a good enough state to pull folks in for approval. please see the top level comment for the hld. there are still some pending comments, but most of them are waiting for general consensus about the design.",0,0,1,0.7269366979598999,0.7330054044723511,0.6212630271911621,0.0,accept,majority_agreement
1469320307,11659,core group conceptually approved. what is pending is to update the naming and implement the new api to make it simple to log the events.,0,0,0,0.9719489812850952,0.9864004850387572,0.9820752739906312,0.0,accept,unanimous_agreement
1469466348,11659,please make sure to update the top comment (to be used as squash-merge commit comment) p.s. triggered a daily ci for the module tests: [a link],0,0,0,0.9838480353355408,0.984566867351532,0.9939646124839784,0.0,accept,unanimous_agreement
1470356676,11659,"yes, thanks! i updated the top comment",1,1,1,0.9767034649848938,0.9923836588859558,0.989177942276001,1.0,accept,unanimous_agreement
945072492,9572,"i think you should avoid doing `git rebase` and `push -f`, just stick to merge and incremental commits. it's hard to keep track of what's new and what was already reviewed, and since we're gonna squash-merge this anyway, there's no real need for modifying existing commits.",0,0,0,0.8455380201339722,0.9786837697029114,0.9609763026237488,0.0,accept,unanimous_agreement
951762777,9572,"/core-team not sure if we need a major decision for this, but since it's a delicate subject, i'd love for you to review.",1,-1,1,0.6573442816734314,0.4356881976127624,0.5548436045646667,1.0,accept,majority_agreement
968296729,9572,"please re-review and post if you have an example of a case you think this pr breaks. considering cases where writable replica only write to temporary keys, i think this pr only fixes things. also, fyi: [a link]",0,0,0,0.9494476318359376,0.9729385375976562,0.9717580676078796,0.0,accept,unanimous_agreement
968619479,9572,"first of all, i agree we should cleanup the functions `lookupkey*()`, but except the change about writable-replica: [code block] imho, writable-replica is a bad design, i'm strongly against it(i even wanna deprecate it in 7.0), maybe it's useful in some scenarios if people use redis as cache, but i believe more and more people use redis as database (iirc redislabs defines redis as a primary database), and data inconsistency is a very serious problem in database. back to writable-replica change in this pr, it seems try to fix a ""bug"" about expire, it also confuses me how to define a bug? 1. if we wanna allow write commands delete expired key, then how to deal with read commands? maybe you would say because users can send write commands to writable-replica, and just insert new keys and set expire time on the new keys, but we don't have any way to restrict people to use it in the ""correct way"" we expect. ok, if people really use it in right way (only use ""slave keys""), then the read commands also need delete the expired keys. 2. this change let the lazy expire work in write command on all keys, but the active expire only work on the ""slave keys"". i don't think it's a good idea that we use different approaches on expire mechanism, the active expire can only delete ""slave expired keys"" but the lazy can delete all (especially ""master"") expired keys. and i can't understand why we need ""fix"" it, is it an actual bug(i don't see any public issues, in my opinion it's an abuse case)? and i(we) need a uniform standards, to decide what is bug and if it is necessary to fix, or this change cannot convince me. bottom in line, my opinion is we should stop any development on writable-replica, just keep it as what it is currently. if we think it's a useful feature we should improve and fix lots of places about it, not only expire (and i think the ""bugfix"" in this pr is not right).",0,-1,-1,0.6292847394943237,0.7454577684402466,0.9529142379760742,-1.0,accept,majority_agreement
968768300,9572,"generally, i could argue that i don't mind not fixing bugs in writable replicas, but i do wanna clean the code and make it consistent, so that when people read it or add new commands, they know what's the right thing to do (this lookupkey thing was a problem in 6.2). if while doing that cleanup i'm affecting writable replicas, i don't mind that much since it's a bad feature. however, since people do use it, i rather not introduce new problems to it or cause it to stop serving their needs, but if such a cleanup is fixing things issues them rather than break things, i'm certainly ok with it. more to the point, if we consider for a moment that writable replicas are for now valid to be used to create temporary keys on replicas, and that the keys they write to are keys that don't exist on the master (in that case there's really no data inconsistency between the master and replica). so in that case: 1. same as zunion won't let you read from an already expired key, zunionstore+zrange shouldn't let you read from it either, so lookupkeyread should be used for source keys, and it should filter out expired keys, even if the command doesn't arrive from the master. **this is a bug this pr fixes, and iiuc that's not the one you strongly object to, right?** 2. if you consider the destination key a write command e.g. let's say it's a zunionstore followed by zpop, then the keys we write to should in theory do an expiration test. **this is the change / fix you're objecting to, right? let's discuss it below.** so for the above case [2], you are correct that we're actually it's missing the point since it doesn't check for the expiration in the right dict. i don't mind improving / fixing it (makes the internal consistency of our code better), but also since this bug was already always there, i don't mind leaving it as is either. as i said before, this is the part i really hate about writable replicas (the `slavekeyswithexpire`), and i'm willing to break it, delete it, or leave it broken. i don't want to invest much effort in fixing it. please let me know if i'm missing anything, i.e. my technical analysis of the problem is wrong, or are we just holding different opinions on what's the right thing to do.",0,-1,0,0.5939725041389465,0.5156828165054321,0.7715898752212524,0.0,accept,majority_agreement
968880012,9572,"i think it's unnecessary to affect writable-replica in this pr. that's the point i don't understand and i want to argue, this bug was already always there and if you also agree we don't need to fix it, this pr should not affect writable-replica. but why we fix it incompletely in this pr? i think we should revert the change about writable-replica in this pr, or we should check expire not only in write command but also in read command, and the key we check is in the `slavekeyswithexpire` dict. i also agree this action. my opinion is we should fix bug completely or we don't fix it, i don't know are we holding different opinions? : )",0,0,0,0.8896905779838562,0.9631357192993164,0.8917595148086548,0.0,accept,unanimous_agreement
968930747,9572,"regarding your suggestion to be clear, in this pr we ""check expire"" in both read and write, but we only delete the key in the write case. are you suggesting that we delete the expired key in both read and write cases? then we have the inconsistency described in this comment: [a link] the example doesn't include slavekeyswithexpire. are you suggesting that we delete the expired key only if it's in the slavekeyswithexpire dict? if yes, then it means we still return null in lookupkey for expired keys for read and write, even if we don't delete it? maybe this works... but a lookup for write is always followed by some write operation, so it will behave the same way as if it would be deleted already in lookup. (correct me if i'm wrong.)",0,0,0,0.9796227812767028,0.989713191986084,0.9848065972328186,0.0,accept,unanimous_agreement
968953109,9572,"sorry i didn't explain it clear, i mean we should delete expired keys only in `slavekeyswithexpire` dict in both write and read command on writable-replica. but at the same time i don't like the fix, if writable-replica is a bad design, then the expire management on writable-replica is a horrible design. i prefer much more removing the `slavekeyswithexpire` code and don't do any expire delete on writable-replica.",-1,-1,-1,0.9893006682395936,0.9929807186126708,0.9908446669578552,-1.0,accept,unanimous_agreement
969001441,9572,"please make a distinction between the following use cases: 1. a writable replica that only writes to temporary keys, and does not use expire command. 2. a writable replica that writes to keys that also exist on the master. 3. a writable replica that uses expire on the replica. the bug fix in lookupkeyread, which refers to case [1] in this list, and also [1] in my previous post should be considered separately from the change in lookupkeywrite, and separately from anything involving `slavekeyswithexpire` imho. i think i wanna insist that the code we have now in lookupkeyread is ok, and we can argue about what lookupkeywrite does. regarding lookupkeywrite, i do think it should behave the same as a write command on the master, i.e. if the key already expired, delete it. if we wanna go the extra mile and check the `slavekeyswithexpire` dict, we can do that too, but i'm also willing to overlook it.",0,0,0,0.9686938524246216,0.9759085178375244,0.9118609428405762,0.0,accept,unanimous_agreement
969032145,9572,"for the record, the reason i make this distinction is because: 1. case 1 won't cause any commands that are propagated form the master to fail. i don't feel comfortable to deprecate or break that yet, and in the meanwhile i rather fix it since it improves the code consistency and reasoning. 2. case 2, will cause commands propagated from the master to fail. so i consider it a serious abuse. 3. case 3, may in theory be ok, but it is already very buggy and the code is inconsistent and hard to reason with, so i don't care to fix it.",-1,0,-1,0.9745537638664246,0.6680570840835571,0.857827365398407,-1.0,accept,majority_agreement
969079943,9572,"i tried changing expireifneeded so that it doesn't delete expired keys for write commands. first, an assert in dbadd fails, but after fixing that, the following tests fail: [code block] note that these tests are all added in this pr. it may be possible to work around the fact that the dict entry can still exists even after lookupkeywrite returns null, but it's **much more custom code for writable replicas** in various commands, instead of only one special case for writable replicas in expireifneeded.",0,0,0,0.9810263514518738,0.9927402138710022,0.9854078888893129,0.0,accept,unanimous_agreement
969086579,9572,"i suggest we deprecate writable replicas in 7.0 and remove writable replicas in 8.0. we can't do it faster than this i think. for now, let's merge this pr so we can focus on more important things...?",0,0,0,0.9325800538063048,0.964921772480011,0.9798881411552428,0.0,accept,unanimous_agreement
969228355,9572,"i really rather not go there, if we do that then the flow on a writable replica will be different than the one on the master, and who knows what that'll lead to (other than leaks), i.e. considering this part will not be heavily tested. we need lookupkeywrite to delete the key when it returns null. i think the current core is ok, and the only thing to improve is the check for expiration in the other dict. personally, i don't feel confident enough to depreciate that feature just yet.",-1,0,0,0.9005811214447021,0.8454564809799194,0.7979492545127869,0.0,accept,majority_agreement
971392705,9572,"to be clear, i'm not talking about the cleanup on `lookupkey*()` function i totally agree with the cleanup, what i'm talking about and against is just only the expire mechanism change on writable-replica, only the case [3]. but this pr not only cleanup the `lookupkey*()` function, but also change the expire mechanism on writable-replica, before this pr: 1. ***read*** expired keys in writable-replica ***doesn't*** delete the key, just return null. 2. ***write*** expired keys in writable-replica ***doesn't*** delete the key, but return the key (which is expired from replica's pov). 3. `databasescron` only delete ***slave*** expired keys. ok, case [2] is buggy, this pr try to fix it, so after this pr: 1. ***read*** expired keys in writable-replica ***doesn't*** delete the key, just return null. 2. ***write*** expired keys in writable-replica ***would*** delete the key, and return null. 3. `databasescron` only delete ***slave*** expired keys. this pr seems fix the ""bug"", but i think case [1] and case [2] should be consistent with case [3], expire check and only check `slavekeyswithexpire` dict both in read and write command. but, i don't like any ""fix"" above, i prefer keeping the status quo, i hate waste time on improving a bad design, even if it just change one line code.",0,0,0,0.9637619256973268,0.979025661945343,0.9783158898353576,0.0,accept,unanimous_agreement
971457099,9572,"what's the point of keeping a status quo? the risk that fixing one scenario breaks another scenario? that's a valid point, but keeping a buggy feature without any plan to fix it or to remove it is the worst option. if we can't provide consistency, we can't support that feature if we want to look like a serious database. i must agree with that ""data inconsistency is a very serious problem in database"". so, if i revert the ""fix"" and keep only the refactoring, what do we reply in #6842? i think we can solve it by documentation. or do you prefer to keep status quo also for the documentation? ;-) i think we need to define exactly which usage of writable replicas we can support and then update documentation to explain that any other usage is very risky and can cause inconsistency. i think it can be very simple: ""writable replicas should only write to keys that are never used on the master."" (even the scenario where the replica does expire on a key replicated from master is going to cause inconsistency.) do you have a different documentation idea to support the changes in this pr? currently, the [a link] has no mention of inconsistency for writable replicas. it only mentions a problem that was solved in version 4.0:",-1,0,-1,0.6746822595596313,0.6076783537864685,0.8744307160377502,-1.0,accept,majority_agreement
971483449,9572,"i think this bug fix is important since it fixes a bug in the only case in writable-replica that i consider valid (i.e. the one that uses temp keys and won't cause any command from the master to fail). we can't do that, this will damage read-only replicas too. what we can do to consolidate that, is fix the lazy expire to look at slavekeyswithexpire too. but as you said, we rather not fix bugs around slavekeyswithexpire at the moment, so if we didn't introduce new ones (i.e. lazy expire never checked this data dict), then i'm ok leaving it as broken as it was.",0,0,0,0.9552854299545288,0.9606068730354308,0.624527633190155,0.0,accept,unanimous_agreement
971485294,9572,"no, i mean if this is a writable-replica, then do lazy expire check in `slavekeyswithexpire`.",0,0,0,0.9815917015075684,0.9932621121406556,0.9916757345199584,0.0,accept,unanimous_agreement
971487060,9572,"i think people who use writable replicas don't use redis as a database, and don't expect data consistency. but still we need to remember that redis has many use cases, and caching is one of them, for now not yet comfortable hurting this use case, and i'm willing to introduce minimal changes to make parts of it less broken. we certainly need to update the docs, i think they're misleading.",-1,0,0,0.5240922570228577,0.519049882888794,0.7594686150550842,0.0,accept,majority_agreement
971488521,9572,"you mean only in slavekeyswithexpire or in both? i'm ok to check both (fixing an additional bug), i was under the impression you're advocating to avoid any bug fix in that area. but if you're ok with fixing the two bugs (better internal consistency in reids), i'm fine with that (since the change will only add some 2 lines to the code, no real complications).",0,0,0,0.9746370911598206,0.931473195552826,0.969859540462494,0.0,accept,unanimous_agreement
971490079,9572,"i have different opinion, i think redis is a database at first, and then because redis is ultra-fast so it can be used as cache, cache or database just depends on users, mysql also can be used as cache, but it's too slow so nobody use it as cache ; )",0,1,1,0.7222899198532104,0.7576448917388916,0.9863405227661132,1.0,accept,majority_agreement
971492177,9572,"but if you're ok with fixing the two bugs (better internal consistency in reids), i'm fine with that (since the change will only add some 2 lines to the code, no real complications). seems you misunderstand me :grinning_face_with_sweat: , i always believe we need fix bug completely or do nothing, and i prefer do nothing in the writable-replica case, but i'm ok with fix it completely. and, i still believe we should remove the writable-replica feature finally, all the use cases are abuse from my pov, i think writable-replica is a bug, at the beginning of redis born, redis doesn't have any protection on slave, i don't know why salvatore didn't fix it, but instead he develop it as a feature.",-1,0,1,0.8196935653686523,0.514648973941803,0.5648894309997559,,review,no_majority_disagreement
971510591,9572,"ok, we'll keep considering to remove it in the future, but for now, let's fix lazy-expire to check slavekeyswithexpire too. please go head. but let's keep this cleanup effort is limited to lazy expire and lookupkey, i do not think we wanna handle other problems with slavekeyswithexpire at the moment (like what happens on replica promotion), where the solution is not straight forward (both clear what should be done, and very easy to implement).",0,0,0,0.9070480465888976,0.7070637345314026,0.8421964645385742,0.0,accept,unanimous_agreement
971563284,9572,"to confirm what to do: in lookup for write, we delete the expired key only if it's in slavekeyswithexpire. otherwise we return the expired key? that means that we don't fix the example in #6842. the test cases in this pr will have to be changed so they instead test the slavekeyswithexpire feature instead of the #6842 issue. is this what you want?",0,0,0,0.987304151058197,0.994286835193634,0.9941391944885254,0.0,accept,unanimous_agreement
971594299,9572,"i meant to check both the `expires` and `slavekeyswithexpire` if the key is expired in any of them, act the same. i.e. obviously on a writable replica if the key you write to is logically expired in `slavekeyswithexpire`, it should be lazy deleted (p.s. this is another bugfix to mention in the top comment). but also if the key is logically expired in the `expires` dict, in lookupkeywrite on a writable replica (if the command doesn't arrive from our master), we wanna delete it (like we do in lookupkeywrite on the master itself)",0,0,0,0.9780330657958984,0.9910154938697816,0.990561544895172,0.0,accept,unanimous_agreement
971690624,9572,this doesn't make sense. `slavekeyswithexpire` doesn't contain any timestamp. [code block],0,0,0,0.7886505126953125,0.9612039923667908,0.9139583706855774,0.0,accept,unanimous_agreement
971707248,9572,"ohh, i'm sorry.. (responding from memory without checking the details). so expire on a writable replica also sets the expiration in the main expires dict. there is no problem then with doing lazy expire like we already do in this pr. the reason the `slavekeyswithexpire` exists is so that the active expire that runs on the replica, will **not** look at keys that where **not** explicitly set expire on the writable replica. but the lazy expire on lookupkeywrite is still ok to delete both types (the ones that are in slavekeyswithexpire and the ones that aren't). maybe we can modify lookupkeyread to also do that (do lazy expiry), but only if the key is in slavekeyswithexpire. i.e. expireifneeded will do the deletion in either of these cases: 1. we're on the master 2. we're writing to a key on a replica 3. we're reading form a key on a replica and the key is in slavekeyswithexpire",-1,-1,-1,0.9907681345939636,0.9815280437469482,0.9854189157485962,-1.0,accept,unanimous_agreement
971751829,9572,"ok, this makes sense, thx. are you ok with this suggestion? btw, i'd like that we define what exactly is supposed to work. the concept of ""more consistent"" or ""less consistent"" is very vague. maybe we can do it (and update docs) based off this pr when it's accepted.",0,0,1,0.913748025894165,0.7395736575126648,0.7638975977897644,0.0,accept,majority_agreement
972461880,9572,i think it's better to open a new pr to discuss the expire mechanism on writable-replica (but need revert the expire change in this pr first).,0,0,0,0.9817803502082824,0.9883402585983276,0.9856554269790648,0.0,accept,unanimous_agreement
972620616,9572,what's wrong with the current [a link] ?,0,0,0,0.6825011968612671,0.9856265187263488,0.9361245036125184,0.0,accept,unanimous_agreement
973843091,9572,"nothing, i just worry this pr may become more and more huge due to the writable-replica (another question is if we need remove the key from `slavekeyswithexpire` when delete it in `expireifneeded`), if you think it's not a problem, just move on.",-1,0,-1,0.8835158348083496,0.9517182111740112,0.9004335403442383,-1.0,accept,majority_agreement
974138588,9572,"i rather move on and do it all in this pr, it's hard for me to think off changing parts of it (mess up the lookupkey without affecting writable replica at all), rather than think of where we wanna get at the end, and look at the code to see if it does it. also, i won't think this pr is gonna be huge. and yes, i think we should delete the key from slavekeyswithexpire when it gets deleted for whatever reason. (didn't check if that currently happens, and i'm not sure i care much, since that feature is already quite broken). please go ahead and implement what was discussed.",-1,0,0,0.9651421308517456,0.7415451407432556,0.7068701386451721,0.0,accept,majority_agreement
974157635,9572,"keys in slavekeyswithexpire are only deleted in expireslavekeys() which is part of active expire. they're not deleted by e.g. the del command. to be able to delete them in e.g. lookupkey() or expireifneeded(), i would need to refactor the expireslavekeys() function a bit to break that part out.",0,0,0,0.987343430519104,0.9935884475708008,0.9880205988883972,0.0,accept,unanimous_agreement
974173196,9572,"seems very much broken to me, or maybe i don't understand the design of that feature (considering i do much of the correspondence on this subject from android, that may very well be). i feel we better not touch it then (don't remove keys from that dict)",-1,0,-1,0.9506165981292723,0.5340456366539001,0.7898437976837158,-1.0,accept,majority_agreement
974768368,9572,"i finally bothered to allocate some time and take a proper look at the code. since each entry in `slavekeyswithexpire` may represent multiple databases, we can't really afford to delete keys from there in lazy expire (easily). to do so, we'll need to update the database id bit mask each time, until we see this was the last bit that's set. instead, the design is that active expire (`expireslavekeys`) will do all that cleanup and it's not necessary for lazy expire or del command to do that, and there's no harm (other than maybe performance) in skipping it when the key is deleted. of course we can improve, but i don't think we should bother. also, since the volatile keys in slavekeyswithexpire are actually also set the ttl in the normal `db.expires` dict, lazy expire is (and was always) actually working on these fine (i.e. as far as the indication to the caller goes), the only other reason to have lazy expire look at slavekeyswithexpire is in order to delete the key (not the entry in slavekeyswithexpire dict), which i think we should do at least in lookupkeywrite, so that the command will be processed the same way it is processed in the master, but for that concern we should delete the key even if it's not in slavekeyswithexpire. i.e. for lookupkeyread, it is normal to return an indication that they key is missing even if it wasn't delete, but for lookupkeywrite, returning an indication that the key is missing without actually deleting it, could cause the command to later fail (when it calls setkey or dbadd), and that's true regardless of whether or not the key was in slavekeyswithexpire. so bottom line, looking at the current code of this pr, we already do all of that, and i don't think anything needs to be changed.",0,0,0,0.8938906788825989,0.966242015361786,0.7836601138114929,0.0,accept,unanimous_agreement
975182961,9572,"one scenario is: 1. insert a temp key with ttl in a writable replica, like `setex foo 10 bar` 2. call del command to delete it, `del foo`, but the key `foo` is still in `slavekeyswithexpire` 3. then insert a same name key with ttl in master, and propagate to the writable replica, like `setex foo 11 bar2` 4. call `persist foo` in master before the key foo become expired 5. before the writable replica receive the `persist foo` command, it check `slavekeyswithexpire` and find key `foo` is expired (replication may delay) and delete it i'm also tired of discussing about writable-replica feature(it's bug, not feature imho), fix a bug completely or do nothing is my working style, if you think it's not a problem or it's a problem but just fix part of it is enough, just go ahead, i can remove my `request changes`, but i feel shame we keep ugly design in redis.",0,0,0,0.9771037697792052,0.994433343410492,0.991232693195343,0.0,accept,unanimous_agreement
975223060,9572,"the scenario you described is a (known) limitation of this `slavekeyswithexpire` mechanism, and it was like that before and we don't change it.. the limitation is that the temporary keys that the writable replica works with, should never be used in the master (these are key names that should be reserved only as temporary keys in the writable replica). this pr doesn't change it, and i agree we shouldn't fix it or worry about it (bad feature and a pile of bugs). i agree with you that this writable replica feature is an issue (causing inconsistencies), and especially the `slavekeyswithexpire` mechanism which is very buggy and incomplete). but i don't feel we can trim this feature yet, and we both agree that we shouldn't attempt to fix bugs in it. the changes in this pr don't aim to fix bugs in this mechanism (`slavekeyswithexpire`), just clean up the code and make it consistent and easy to work with. along they way they do affect this feature slightly (the effect is a small improvement, not a degradation), so i don't see why reject them. if we look at the only arguably valid use case for writable replicas (ones that only write to temporary keys, and never use expire), this pr fixes some bugs, which are: 1. avoid exposing data that is already logically expired. 2. if lookup returns null, delete the key, so the command will be executed similarly as it would on the master, and not crash on some error.",0,0,-1,0.8090847134590149,0.9017696976661682,0.776210606098175,0.0,accept,majority_agreement
975251154,9572,"the reason i believe is, improve the writable-replica can give the users who are abusing it more excuses to continue abuse it. and to make it clear, i mean (in my opinion) all use cases with writable-replica are abuse, there is no reasonable use case on writable-replica. it's ok we have different opinions, so just go head.",0,0,0,0.7148650288581848,0.8225735425949097,0.837726354598999,0.0,accept,unanimous_agreement
978897154,9572,"it occurred to me that maybe we wanna add a test for a case a replica attempts to write to a key that's already logically expired. maybe both incr (uses dbadd), and sunionstore (uses setkey). just to be sure there are no crashes. do you mind adding that?",0,0,0,0.982696771621704,0.9832777380943298,0.9802387952804564,0.0,accept,unanimous_agreement
979033018,9572,"we already have [a link] which does this for incr. setkey is using dbadd or dboverwrite and uses lookupkeywrite to find out if it exists or not, just like most commands do, so i see no reason to add tests specifically for this unless we add this kind of test to all commands. (obviously setkey might crash when you pass the wrong flag `setkey_already_exist` or `setkey_doesnt_exist` mismatching the actual existence, but it might crash regardless of this pr.)",0,0,0,0.9385477304458618,0.991175651550293,0.9860048890113832,0.0,accept,unanimous_agreement
979036562,9572,"there are some unstable test cases here though, because of race conditions with expire i think. this build failed yesterday: [a link] on a master, there is no way to check that a key is expired without deleting it. should i add a few milliseconds extra sleep to extra sure the key is expired?",0,0,0,0.953711211681366,0.8499587774276733,0.9143901467323304,0.0,accept,unanimous_agreement
979057289,9572,"there is a way to check if the key exists without deleting it (debug object). but i don't see why there's a race... the tcl code has `after 100` so we know at least 100ms passed since we got the reply from pexpire. this means that when we execute swapdb, the check in it that tests the expiration time will surely find that it already expired. maybe there's an off by one issue, i.e. `> 100` vs `>= 100`, so when the test is super fast is fails. if that's the case, we can sleep for 101, but i don't see any other explanation. please look into it and then we'll merge.",0,0,0,0.9575838446617126,0.915217101573944,0.9616491794586182,0.0,accept,unanimous_agreement
2587471562,13740,[a link] all committers have signed the cla.,0,0,0,0.9875962734222412,0.9764910340309144,0.995100438594818,0.0,accept,unanimous_agreement
2615150182,13740,"we can decide not to persist, persisting it just help us to converge quicker. wdyt?",0,0,0,0.9740960001945496,0.9502439498901368,0.983113408088684,0.0,accept,unanimous_agreement
2615288618,13740,"i'm not sure i know enough about cluster to decide, i suppose it's safer not to persist it, and also maybe it helps with downgrades? i suppose that if we avoid persisting it, we do that so that if someone gains access to the disk, or a copy of the config, he's getting ""root"" permissions, but on the other hand, if they get permissions to connect to the cluster bus, they can get it from there too. still, i suppose it's safer, and still acceptable behavior after restarts.",0,0,0,0.9187631607055664,0.9563093781471252,0.8897080421447754,0.0,accept,unanimous_agreement
2615291754,13740,"btw, it's a shame we're mixing these topics, we should move this discussion to #13763",-1,-1,-1,0.9899113774299622,0.9907371401786804,0.9926397800445556,-1.0,accept,unanimous_agreement
2615306566,13740,yes i agree gone copy the comment and continue the discussion there.,0,0,0,0.9603960514068604,0.9569525718688964,0.9824305772781372,0.0,accept,unanimous_agreement
2616875588,13740,"as per a discussion with and , we decided to remove the internal commands from the monitor created by a non-internal connection, while for internal connections we will display the internal commands. this was added to the pr. note that we do not display the internal commands in all `command ` commands for non-internal connections as well.",0,0,0,0.988243579864502,0.9940698146820068,0.9934594631195068,0.0,accept,unanimous_agreement
2618042137,13740,"-mon please revert the changes of #13763 from this pr, so it's easier to review and focus on the internal commands mechanism.",0,0,0,0.974425971508026,0.9911621809005736,0.9680312275886536,0.0,accept,unanimous_agreement
2618053047,13740,"my changes rely on these changes, i can use a static password but then i will need to make changes after the review which is not ideal. the changes of [a link] are quite contained in the cluster files, if it possible to ignore them until [a link] is merged that would be best. if you think it will take a long time please let me know and i'll revert the changes for this review, but anyways this pr can be merged only after [a link] is merged and used here.",0,0,0,0.9392695426940918,0.8810867071151733,0.927750527858734,0.0,accept,unanimous_agreement
2618376805,13740,"personally, i think a temporary hard coded password is better for review. and then the other pr is merged first, and this one is updated.",0,0,0,0.97804993391037,0.9797099828720092,0.9844122529029846,0.0,accept,unanimous_agreement
2618635859,13740,"ok , will update the pr soon.",0,0,0,0.9798938632011414,0.983504831790924,0.994005799293518,0.0,accept,unanimous_agreement
2624094336,13740,please ack about the interface changes listed in the top comment.,0,0,0,0.9758976101875304,0.97554349899292,0.9906547665596008,0.0,accept,unanimous_agreement
2624107498,13740,?,0,0,0,0.9320514798164368,0.9557723999023438,0.9296892285346984,0.0,accept,unanimous_agreement
989741920,9872,"it occurred to me that there's probably an opportunity for some semi-related cleanup. when we get sigterm during loading, we immediately `exit`. iirc this code was written before prepareforshutdown had the nosave option (which was added for the shutdown command). now that it has it, i think it's better to go though prepareforshutdown, and pass the appropriate flags (nosave / force). there are no should be no fork children at that time, but it's still a good idea to go though other cleanup steps (e.g. modules). while we are no the subject, i did notice that we're terminating the aof child only if aof is enabled, but currently it is also possible to do bgrewriteaof when it's disabled (see #9794), so i think the child should be stopped. so, if you're already working in that area, we can make additional cleanups.",0,0,0,0.8920177221298218,0.9630772471427916,0.9555604457855223,0.0,accept,unanimous_agreement
994405196,9872,full daily ci: [a link],0,0,0,0.9643882513046264,0.7142459154129028,0.9934364557266236,0.0,accept,unanimous_agreement
996156270,9872,"unstable now contains shutdown.json, which this pr needs to modify. the two things to modify are to provide details on the new arguments (force, and now). and also describe the complexity which was missing till now, but i think now is the chance to describe it. i.e. if we end up doing foreground save, then it's the same complexity of the save command.",0,0,0,0.9801543354988098,0.9891770482063292,0.9905533194541932,0.0,accept,unanimous_agreement
998529519,9872,"approval is for the high level design, i didn't look at the details.",0,0,0,0.9587162733078004,0.9644948244094848,0.9906309247016908,0.0,accept,unanimous_agreement
1003040369,9872,"/core-team may i have your attention again? two more things added in this pr: * client pause per purpose (failover, shutdown, client pause command) * shutdown abort",0,0,0,0.9592812061309814,0.9895898699760436,0.9617744088172911,0.0,accept,unanimous_agreement
1003677040,9872,"thank you. i suppose there are some docs that will benefit from an update about this change. obviously the shutdown command (which must refer to the new config, and new args), but maybe some other places too?",1,1,1,0.8768389821052551,0.8200163841247559,0.9676933884620668,1.0,accept,unanimous_agreement
1003772766,9872,"thank you! yes, i have updated the page about signal handling. it's merged already in redis/redis-doc#1711.",1,1,1,0.983752965927124,0.9858718514442444,0.9919968247413636,1.0,accept,unanimous_agreement
1089528659,10536,"given that we are after the code cut-off for redis 7, we probably won't include this, will probably merge this after 7 goes ga.",0,0,0,0.9861240386962892,0.9918938279151917,0.9891147613525392,0.0,accept,unanimous_agreement
1089539657,10536,"i see strong value in getting this change rolled out together with `cluster shards` as it helps complete the shard scenario end 2 end. additionally, it has externally visible impact in areas like `cluster nodes`, `cluster shards`, and nodes.conf, which are already updated in 7.0 so it will be great if we can piggy back on the same release (as opposed to churning these areas release after release). i will close the remaining two issues today (the missing tests and a way to return own shard_id). i am happy to prioritize this fix for 7.0 ga on my end. fyi",1,1,1,0.9778313636779784,0.9896798729896544,0.9850419163703918,1.0,accept,unanimous_agreement
1089784398,10536,"i see a lot of your points, and i agree with most of them. i have two concerns. let's focus on the implementation, if it's ready we can of course make the decision to merge it earlier. 1. the replica should follow the shard id of the primary, and reconcile itself automatically. 2. you should be able to enforce a shard id, since i don't think shard ids should rotate if all nodes in a shard die.",0,0,0,0.8193536996841431,0.7498922348022461,0.514146089553833,0.0,accept,unanimous_agreement
1089822179,10536,"sounds good . yes, replicas will pick up their primary's shard id upon joining the shard via `cluster replicate` shard ids are persisted in nodes.conf. nodes should be able to retrieve their previous shard ids on restart, assuming their nodes.conf are not corrupted. in the case where replicas lost their nodes.conf, they can still recover their shard ids via the gossip message from their primary, if the primary's nodes.conf is still good. if, for whatever reason, all nodes.conf files in the same shard are lost, we will rotate to a new shard id. i added a new command `cluster myshardid` to expose a node's shard id, similar to `cluster myid`. the reason for a new command is to reduce backcompat risk but let me know if you think otherwise. i can update the documentation after we close this pr.",1,0,0,0.9485582113265992,0.6836907267570496,0.7762845158576965,0.0,accept,majority_agreement
1098286982,10536,"i pushed another commit to make `cluster shards` work better with failed primaries and this should make the shard id support complete. below are the externally visible changes (hence my preference to include them along with `cluster shards` in a major release). * nodes.conf - added a new field in the end point column *before* hostname [code block] 1. this is not the most logical place but it seems to me the only extensible column with minimum backcompat risk 2. with this change, since `shard-id` is always on, i moved it ahead of `hostname`. i am assuming this incompatible change is ok as we are still in the rc phase but let me know if you have concerns * `cluster shards` has a new `shard-id` row [code block] * `cluster shards` now groups failed primaries with their old replicas/new primary in the same shard; previously they are separated after the failover [code block] in case you are interested - this is to improve the native shard support started with `cluster shards`",0,0,0,0.9618594646453856,0.9820169806480408,0.9828532338142396,0.0,accept,unanimous_agreement
1099445527,10536,"great point on future-proofing the format now, especially your 2nd point. it is going to be super hard to introduce new fields in the future if we don't come up with a systematic solution in 7.0, even with just the hostname field. mulling over your suggestion now ...",1,1,1,0.9252752065658568,0.8439596891403198,0.9898082613945008,1.0,accept,unanimous_agreement
1099765509,10536,"i think json would be the ideal long term solution. a potential migration/upgrade strategy would be to attempting to load `nodes.json` first and on `not found` fall back to loading `nodes.conf` and immediately writing out `nodes.json`. this is imo better than hacking `nodes.conf` further. so one option could be to stick with `ip:port,shard[,hostname]` in 7.0 and then make the json switch in 7.2. thoughts? here is an example of nodes.json for illustration: [code block] update - i think an in-place update scheme that reuses the same file name `nodes.conf` for the json content would work fine too.",0,0,1,0.8610248565673828,0.9593669772148132,0.5495767593383789,0.0,accept,majority_agreement
1111778369,10536,"this change is ready. i moved shard-id behind hostname so it should be compatible with 7.0 ga. can you please take a look? pre 7.0: `ip:port[]` 7.0: `ip:port[]` or `ip:port[],hostname` 7.0 + shard-id: `ip:port[],,shard_id` or `ip:port[],hostname,shard_id`",0,0,0,0.9314152598381042,0.9297837018966676,0.9033706784248352,0.0,accept,unanimous_agreement
1114552279,10536,fyi - in case 's bandwidth is limited recently. this pr brings native shard-id to 7.0.0 in a fully compatible way. it is ready for code review. please feel free to let me know if there is anything that i can help to make some progress on this pr.,1,0,1,0.6202821731567383,0.8568236827850342,0.886982262134552,1.0,accept,majority_agreement
1129601213,10536,can you please review this change when you get a chance? thanks!,1,1,1,0.9590908885002136,0.9376090168952942,0.8478959798812866,1.0,accept,unanimous_agreement
1146478507,10536,"ok! i don't know about json, we don't have a json parser today in redis and i don't think we really want to take a dependency on one. i think there are easier and more extensible ways to improve node.conf without moving to json though. in another pr i proposed having key/value fields both the slots data.",0,0,0,0.8351578712463379,0.659221887588501,0.6231070160865784,0.0,accept,unanimous_agreement
1150684893,10536,my primary point is to show that there is a way forward. i don't have a strong opinion that we should use json. can you point me to your other pr where key/value fields were proposed? the idea sounds reasonable to me.,0,0,0,0.9075387120246888,0.8933226466178894,0.9779415130615234,0.0,accept,unanimous_agreement
1160928838,10536,"it was discussed here: [a link] it still uses the older convention though, one of the two prs can implement it and we can cross port it.",0,0,0,0.9877474308013916,0.9914906620979308,0.9947625994682312,0.0,accept,unanimous_agreement
1161235780,10536,"will you take a look at this since we presumably want the same ""shard id"" to be the same between cluster v2 and cluster v1.",0,0,0,0.9879240393638612,0.9898532032966614,0.9920023679733276,0.0,accept,unanimous_agreement
1164977371,10536,"ids in flotilla are monotonically increasing numbers (padded in order to preserve compatibility with cluster v1), but the general concept of this pr does seem to align well with our plans there.",0,0,0,0.9800835847854614,0.9910483956336976,0.9869627356529236,0.0,accept,unanimous_agreement
1168182709,10536,"one of the assumptions being discussed here is that shardids may not necessarily be monotonically increasing, and externally imposed.",0,0,0,0.9313998222351074,0.9835264086723328,0.9875373244285583,0.0,accept,unanimous_agreement
1173167515,10536,"i commented on the ""monotonically increasing"" property on 's cluster v2 spec.",0,0,0,0.987720787525177,0.988522171974182,0.9882617592811584,0.0,accept,unanimous_agreement
1173854154,10536,"/ - externally imposed is fine - the ids allocated by flotilla will still be monotonically increasing (internally, we maintain a counter and verify that the value is indeed unused before returning it)",0,0,0,0.9824228286743164,0.9812259078025818,0.987312078475952,0.0,accept,unanimous_agreement
1223503850,10536,on the other thread we decided to go with the aux format: [code block],0,0,0,0.9870562553405762,0.992517590522766,0.995059609413147,0.0,accept,unanimous_agreement
1223504687,10536,"the only other change is unit tests can now be written for cluster features so they run in the ci, in tests/unit/cluster/*, so all of your changes need to get moves to those files now. other than that i think we should be able to wrap this up quickly and merge.",0,0,0,0.9853408932685852,0.97941392660141,0.950459361076355,0.0,accept,unanimous_agreement
1237608277,10536,sounds good. will move the unit tests to tests/unit/cluster/* next.,1,1,1,0.9465125799179076,0.960973620414734,0.7477734684944153,1.0,accept,unanimous_agreement
1237611817,10536,the shard_id change is ready for your review. this pr includes the auxiliary field support as we agreed upon in pr [a link] (see [a link] i also refactored the logic that builds various ping extensions including both hostname and forgotten_node. please ignore pr #11239 (same payload but wrong branch),0,0,0,0.9873513579368592,0.9769631624221802,0.992734432220459,0.0,accept,unanimous_agreement
1241535699,10536,this test conversion is harder than i thought. i don't want to block your code review on the test code refactoring. i can take your feedback on other parts of the changes while working through the test issues.,0,-1,0,0.8751987218856812,0.6649690270423889,0.6161935925483704,0.0,accept,majority_agreement
1242828284,10536,"sorry :). once we get all the tests converted life will be easier, but for now it's a bit of a pain since the two frameworks don't have all the same functions.",-1,-1,-1,0.9812042117118835,0.9931941032409668,0.9935106635093688,-1.0,accept,unanimous_agreement
1246107780,10536,"thanks ! i have follow-up prs to address both callouts (replicas following primaries and empty primaries) in the pipeline. ps, the feature gap between the old and new test infra is indeed the blocker. still trying to find a way to create a new redis node and control how it is introduced to the cluster. i will continue the refactoring work but i am also open to moving the test refactoring work to another pr and unblocking other prs that depend on the aux field change.",1,1,1,0.9809929728507996,0.9798021912574768,0.9861955642700196,1.0,accept,unanimous_agreement
1246131755,10536,"i'm fine punting it out of this pr to unblock as you mentioned. let me know if i can help, since i am the major one pushing for the refractor.",0,0,0,0.7552048563957214,0.7323704957962036,0.7692326307296753,0.0,accept,unanimous_agreement
1255199670,10536,"core group approved, but waiting on to take a look before merging.",0,0,0,0.982700765132904,0.969244420528412,0.9898954629898072,0.0,accept,unanimous_agreement
1256800735,10536,"i am certainly biased here but here are a few thoughts of mine: 1. i think `cluster shards` is a very useful admin command as well and more so than `cluster nodes` thanks to its native resp format; 2. identity is an integral part of any ""objects"" so having a `cluster shards` command that outputs a list of shards without their identities leaves a cognitive hole imo; 3. just like how we spec'ed out the shard id format and its properties in cluster v2, the v1 shard id along with its format choice (40 bytes) is not an implementation detail imho; 4. in the future, we could introduce commands that take shard ids as parameter, such as [a link]. so being able to retrieve the shard-id easily using a resp parser is valuable; 5. this is just a minor point, we currently print out the node id in `cluster shards` too. curious to hear your thoughts on whether to include shard-id in the `cluster shards` output.",-1,0,0,0.710483729839325,0.8269227743148804,0.7474306225776672,0.0,accept,majority_agreement
1257057383,10536,i agree with ping here. good points.,1,1,1,0.9554218053817748,0.955785036087036,0.9908586144447328,1.0,accept,unanimous_agreement
1257192190,10536,"some counter points: 1. i actually think `cluster shards` is a *bad* admin command. why? it doesn't expose all of the internal state of cluster nodes. we ideally should also include additional metadata such ongoing data migrations. the intention, or at least the one i had, for `cluster shards` was that it would be a better replacement for `clutser slots`. while talking with yossi, we thought that maybe we should introduce a new admin focused command to replace `cluster nodes`. i will advocate for that here. 2. kind of, but going back to the idea that this is primarily for clients. what will ""clients"" do with this mapping? nothing, they don't actually care. 3. sure? i'm fine stamping it as part of the spec here anyways since i'm not arguing against exposing it in `cluster myshardid`. 4. see recommendation on 1. 5. clients should use node id to understand continuity. when they see a node_id in a new shard, they should understand that it was moved to that shard and tear down all relevant connections to the old node_id. most of this presumes that we should build a new command `cluster admininfo` or something, that is an administration command that shows administration info. edit: i also want to provide a pretty crisp boundary between the ""admin"" command and the ""client"" commands, since it's usually two different groups accessing these commands.",-1,0,-1,0.8897992968559265,0.8931952714920044,0.6115451455116272,-1.0,accept,majority_agreement
1259147189,10536,"i am not sure i understand the long term commitment part when shard-id is exposed in `cluster nodes` (and `myshardid`) and, more importantly, when we have gone a long way to establish shards as the first class citizen in redis and given them names. also, there is benefit to the clients in the future if we introduce a slot migration command that takes shard-id like `cluster migrate slot shard ` such that the user doesn't need to figure out the primary node on the target shard. ""client"" vs ""admin"". yes, `cluster shards` is incomplete compared to `cluster nodes`, for now, but it doesn't mean an admin can't/won't use it. there is no enforcement on what admins can or can't call today. so my preference has been to ensure the information returned by any command is complete, conceptually. philosophically speaking, not having the identity included in a command whose purpose is display detailed information about a first-class entity in the system is confusing imo, especially when the entity has a name. imagine that `ifconfig` returns everything about the network interfaces but leaves out the interface names ...",0,0,0,0.8812572956085205,0.9877586960792542,0.8912350535392761,0.0,accept,unanimous_agreement
1260532894,10536,"maybe you misunderstand what i mean by admin clients. there are two distinct sets of apis for cluster, one that application clients execute for topology discovery and routing and the commands that administration runs for managing cluster state (adding nodes, resharding, etc etc). i think shard-id is purely an administration concept. `cluster shards` was intended to be an application client api, not an admin api. i think we want to have two separate commands, one for administration and one for application clients. they can even have the same format if we really want, just expose different sets of information. we aren't launching 7.2 tomorrow, and we can block the release of 7.2 until such a command is ready. i don't want us to make an optimization for ""now"" when the right change isn't a significant change from what we have.",0,0,0,0.7601903676986694,0.8456413149833679,0.898487389087677,0.0,accept,unanimous_agreement
1260555423,10536,"a cluster client lib that utilizes replicas for reads can benefit from having the shard id available. it could use the master's id as the shard id in the internal slot mapping structure, but then when there is a failover, the identity of the shard would change to the new master that the client would need to reorganize its mappings. a failover doesn't affect the ability to read from replicas within in the shard. regarding other potential admin-only information, perhaps it's not useful for clients, but how do we know that? theoretically, why can't a sophisticated client make use of information about ongoing slot migrations?",0,0,0,0.9758471846580504,0.9634904265403748,0.9875861406326294,0.0,accept,unanimous_agreement
1260791570,10536,"not to introduce an awsism, but this is a oneway door. once clients become aware of the concept, they might use it for reasons we don't want them to. clients don't really care about a failover today, since they will just rediscover the topology and should have logic for refreshing the entire state. specifically handling failovers is just extra work i'm not sure clients could do. i'll be the clients slack channel to see if i'm missing something though. i don't want clients to know about ongoing slot migrations :). it should be magic.",1,1,1,0.8667664527893066,0.6292663216590881,0.8600134253501892,1.0,accept,unanimous_agreement
1263606156,10536,please review the comments as well to make sure you agree.,0,0,0,0.9785141348838806,0.9827367067337036,0.9924420118331908,0.0,accept,unanimous_agreement
1264128852,10536,"what prevents an ""abusive"" client from using `cluster nodes` or `cluster myshardid`? i think what is missing here is a comprehensive framework that formally establishes the concept of ""client"" and ""admin"" commands. the proposal of removing ""shard-id"" from `cluster shards` feels ad hoc to me and i am more concerned about the non-intuitive behavior introduced. that is a separate topic and it goes beyond the identity. it would require the atomic migration feature you proposed in the other thread. otherwise, clients will notice the effect on the data path, irrespective of shard-id being returned via `cluster shards` or not.",-1,0,0,0.5885958671569824,0.8008167743682861,0.6914723515510559,0.0,accept,majority_agreement
1267750768,10536,"nothing prevents clients from implementing something we don't want them to do. i understand your meta-point that there isn't a good ""client spec"" that exists that explains how clients should implement cluster mode. that is a gap we should address. we aren't proposing to remove ""shard-id"" from cluster shards, we are proposing not adding it at all. we can always consider adding it later. the main point you identified for adding it is to help with stabilizing the existing slot migration in addition to providing better administration controls.",0,0,0,0.9340502619743348,0.9789398312568665,0.9824934601783752,0.0,accept,unanimous_agreement
1267800525,10536,"sync'd up with offline. in the interest of unblocking other prs, i filed #11354 to continue the discussion on whether we should include shard_id in `cluster shards` or define a new command. it is a trivial change that we can add any time, if needed. there is also no appcompat risk given that the `cluster shards` output is of the proper resp format. i will remove the shard_id field and rebase my changes sometime this week.",0,0,0,0.9582841992378236,0.9779340624809264,0.9717839360237122,0.0,accept,unanimous_agreement
1281917318,10536,sorry for the delay. i have removed the shard-id field per offline discussion.,-1,-1,-1,0.9877724051475524,0.9930927753448486,0.9889675974845886,-1.0,accept,unanimous_agreement
1303750112,10536,"this is still waiting on you, do you have any other concerns with the api besides the point we talked about?",0,0,0,0.9773604869842528,0.9342995882034302,0.9789526462554932,0.0,accept,unanimous_agreement
1306079175,10536,"could you please resolve the code conflict and then yossi maybe do a final code review, thanks",1,0,1,0.9616793990135192,0.7280082702636719,0.7198665142059326,1.0,accept,majority_agreement
1306468091,10536,"i will follow up with yossi tomorrow about it. the merge is trivial, i can do it if ping is busy.",0,0,0,0.9395243525505066,0.9626034498214722,0.9740018844604492,0.0,accept,unanimous_agreement
1308154017,10536,"thanks for your patience, i think we're all good to merge based on the design. only thing left is to create a documentation pr and do the rebase. if you're busy, i'm happy to clean it up and get it over the line. we can start working on the next pr.",1,1,1,0.9825870394706726,0.9929482936859132,0.98631489276886,1.0,accept,unanimous_agreement
1308154073,10536,"thanks for your patience, i think we're all good to merge based on the design. only thing left is to create a documentation pr and do the rebase. if you're busy, i'm happy to clean it up and get it over the line. we can start working on the next pr.",1,1,1,0.9825870394706726,0.9929482936859132,0.98631489276886,1.0,accept,unanimous_agreement
1308157007,10536,thanks for the offer . i will rebase this pr tonight and get on the documentation pr sometime this week.,1,1,1,0.9446548819541932,0.7909393906593323,0.9345496296882628,1.0,accept,unanimous_agreement
1616417805,10536,"please help me figure out something. i see this pr is marked for release-notes, but i don't see it mentioned in the 7.2 release notes. maybe you know / remember why? it bothers me since i need to mention breaking it in 7.2-rc3 (#12166)",0,0,-1,0.5588068962097168,0.8539780974388123,0.9725458025932312,0.0,accept,majority_agreement
1620489933,10536,"there is still the `myshardid` command, which reveals the shardid used by all the nodes in the shard.",0,0,0,0.9841071367263794,0.9934717416763306,0.9949588179588318,0.0,accept,unanimous_agreement
1621095167,10536,"so how come it wasn't mentioned in rc1 release notes? could it be that i just missed it somehow? or did we discuss it and decide to skip it? in any case, we need to decide what to do in rc3 release notes. i can retroactively edit the rc1 release notes and then mention it was dropped in rc3 (breaking change), or just add a new note just about myshardid in rc3. btw, i don't remember being aware of that new command, and i was under the impression that when we removed the metadata in #12166 we reverted all the interface changes of this one. either i missed something or have a memory corruption.",-1,0,0,0.6812702417373657,0.9595985412597656,0.9730020761489868,0.0,accept,majority_agreement
1621098767,10536,"i guess editing the rc1 release notes and mentioning the breaking change is the better alternative. either way, can you please update the top comment in this pr (mention the new command) and mention the other interface changes were reverted and when...",0,0,0,0.9823783040046692,0.9864923357963562,0.980958878993988,0.0,accept,unanimous_agreement
1622032072,10536,"i think i missed adding it, the bigger decision at the time was showing it in `cluster nodes`, so it probably got overshadowed. i think adding that it was dropped in rc3 is the right documentation (we did break it, but in a way that makes it more compatible). this also means that for ga it's more compatible. i think documenting the command in rc3 is fine.",0,0,0,0.8789395689964294,0.9705314636230468,0.5948575139045715,0.0,accept,unanimous_agreement
871363204,9127,"thank you for the cr. i addressed most of the findings, with the exception of the ongoing question about getting the first non-tombstone entry from the stream and the associated work/nesting. we can consider a) uniting tip and edge getters b) keeping as is or c) just maintain 'first_id'. the latter avoids seeking/iterating but will make xtrim/xdel/xadd work a little harder. as for legacy persistence, ""it should work"" and there's a test that uses a v5 stream dump payload to check that migration is successful (ref: [a link]",1,1,1,0.9082255959510804,0.9833030104637146,0.94853413105011,1.0,accept,unanimous_agreement
927350702,9127,i believe we're ready for a cr on this.,0,0,0,0.6510626673698425,0.8865968585014343,0.9422308802604676,0.0,accept,unanimous_agreement
981599725,9127,/core-team please review with the intent to approve - the top comment is up to date.,0,0,0,0.9708012342453004,0.945823848247528,0.988900065422058,0.0,accept,unanimous_agreement
1047553448,9127,approved by the core-team in a meeting.. please mark the resolved comments as resolved. make sure the top comment is up to date and let me know when ready for merge.,0,0,0,0.945801854133606,0.9635702967643738,0.9904264807701112,0.0,accept,unanimous_agreement
1870330385,9127,do we need to propagate group->entries_read in streampropagatexclaim? the following sequence will cause the entries-read of replica to always be nul master: [code block] replica: [code block],0,0,0,0.9896795749664308,0.995629072189331,0.9954299926757812,0.0,accept,unanimous_agreement
1870368217,9127,"i don't recall if we discussed and dismissed it, or simply overlooked it. considering that xreadgroup is a write command and propagates commands to the replica anyway, i suppose it should handle this as well. please ack.",0,0,0,0.964239537715912,0.9651423692703248,0.7171323299407959,0.0,accept,unanimous_agreement
1870418723,9127,"ohh, i forget to mention that we will propagate it entries_read streampropagategroupid, so when we use noack, the entries-read is fine: [code block] replica: [code block] so i think it is an overlook. but fixing it means we have to add a new option to xclaim, and the old server will not recognize it.",0,0,0,0.9173043370246888,0.9581877589225768,0.945185661315918,0.0,accept,unanimous_agreement
1870454962,9127,"i don't like to have xclaim fail on the replica (and have it's other responsibilities skipped too). the two options i see are: 1. send an additional command just for that purpose, and let it fail, 2. start negotiating version / capabilities between the master and replica. i don't like either of them, but i suppose option 1 is the better one of the two. i see that when this pr was merged it was easy because xsetid used to ignore excessive arguments (and it no longer does). any other options you see?",-1,0,-1,0.8236397504806519,0.6095257997512817,0.9294272661209106,-1.0,accept,majority_agreement
1870765197,9127,"i don't like these options either. i didn't think of any other good options. depending on your ideas and how you need to proceed, i can do the coding.",-1,-1,-1,0.9741119742393494,0.975769817829132,0.9757132530212402,-1.0,accept,unanimous_agreement
1870929191,9127,"-binbin yes, it feels like a bug i suggest calling `streampropagategroupid` unconditionally, not only if `noack` was provided. that will normalize entries_read on the replica",0,0,0,0.5647993087768555,0.8859625458717346,0.8467321991920471,0.0,accept,unanimous_agreement
1870937154,9127,"that will normalize entries_read on the replica thanks, i tried it and it is ok now. is there any other impact (according to the comments)? want me to submit the pr? [code block] pr: #12898",1,1,1,0.5301466584205627,0.886474609375,0.9493371844291688,1.0,accept,unanimous_agreement
1870944665,9127,"-binbin yes, thanks just don't forget to update the comment there and add tests",1,0,1,0.828325629234314,0.5205585956573486,0.8952203989028931,1.0,accept,majority_agreement
1028755770,10108,/core-team please approve the new module api,0,0,0,0.984195590019226,0.969358205795288,0.99184387922287,0.0,accept,unanimous_agreement
1028872496,10108,"i've pushed a private commit with an alternative here: [a link] this seems a bit more straight forward to me. callers need to pass an extra `redismodule_commandinfo_version` argument to `rm_setcommandinfo()`, but that is pretty much aligned with how other apis work. the upside is we can avoid the inline function and name mangling.",0,0,0,0.9142420887947084,0.873284101486206,0.9823638200759888,0.0,accept,unanimous_agreement
1028898274,10108,"it is slightly more verbose to the user to use, but i guess that's a drop in the ocean compared to the declarative part of the command metadata, and indeed more inline with other apis (in which we ask the user to explicitly set a version field in a struct), and of course avoids the static function and special naming. so anyway, i'm in favor of what yossi proposed.",0,0,0,0.9171100854873656,0.9514437317848206,0.8242454528808594,0.0,accept,unanimous_agreement
1028920528,10108,"ok. i agree it's more strait forward. in that case, isn't it better to put the version field back inside the info struct, like in the other apis, and make it a struct pointer? the version macro can expand to a pointer to a static constant struct defined in redismodule.h. wdyt? one more reason to get rid of the `rm_setcommandinfo_` trick is that this trick doesn't work: ```c if (redismodule_setcommandinfo != null) { redismodule_setcommandinfo(cmd, info); }",0,0,0,0.9651349186897278,0.6973968148231506,0.9252427816390992,0.0,accept,unanimous_agreement
1028922616,10108,putting this as a first first member of the info struct means we can't extend it with more `size` fields.,0,0,0,0.976748824119568,0.993130385875702,0.9915533661842346,0.0,accept,unanimous_agreement
1028922914,10108,we can if it's a *pointer* to a version struct.,0,0,0,0.9880372285842896,0.9926159381866456,0.9938926696777344,0.0,accept,unanimous_agreement
1028924542,10108,"i thought about that, but wasn't very happy with defining a static variable in `redismodule.h`. we'd need to silence errors about unused definition, and we'd rely on the toolchain to make sure it doesn't *really* end up in every object file. this version makes it completely ephemeral and the api is relatively clean (pass a `version` macro like everywhere else).",0,0,0,0.8344882726669312,0.8398898243904114,0.8978058099746704,0.0,accept,unanimous_agreement
1028929050,10108,we actually already have these. see `redismoduleevent_replicationrolechanged` and friends.,0,0,0,0.9778553247451782,0.9867072105407716,0.9948429465293884,0.0,accept,unanimous_agreement
1028931263,10108,"exactly, the events are all static const. they end up in every object file, in modules which don't use events. also, all the api functions are global pointers. most of them are not used by most of the modules.",0,0,0,0.98686546087265,0.9900089502334596,0.9918436408042908,0.0,accept,unanimous_agreement
1028940850,10108,"good points, so the pointer approach makes sense.",1,1,1,0.8604851961135864,0.5908286571502686,0.8368561863899231,1.0,accept,unanimous_agreement
1334259234,11568,"i didn't look at the code yet, but from the description, the one thing i don't like is the api. the fact we pass the callback in the varargs means we have no type checking, and i'm also not happy with the null return and the dependence of errno. regarding the no_script flag, i don't think that it should be a problem removing the flag from the blocking commands, existing scripts couldn't use these commands anyway, so they won't be affected, but we have to take into consideration few other external aspects: 1. existing modules that exposed blocking commands to redis and may have also set the no_script flag. 2. external clients introspecting commands with command info and how it affects them. one interesting thing is that the blocking flag (which modules can also use when they register commands) is completely new, and likely not in use yet, that probably means we can't use it. however, the no_multi flag isn't exposed to modules to this day, so i guess modules couldn't have relied on setting no_script, and had to react to client_deny_blocking anyway.",-1,-1,-1,0.659217894077301,0.8225001096725464,0.7874466180801392,-1.0,accept,unanimous_agreement
1334302657,11568,"the fact we pass the callback in the varargs means we have no type checking, and i'm also not happy with the null return and the dependence of errno. sure, this is just a suggestion but totally debatable. we can have a new api, `rm_asynccall` that gets the callback as one on the arguments, this will solve the type checking. the new function can also have an out param indicating whether it was blocked or not. for the rest, this new function can behave the same as rm_call. let me know how it sounds and i also like to hear and others about this. the `no_script` flag remains the same, the command will not be able to be call inside a script or in rm_call script mode. the change is on this list of those specific commands where we remove this flag. well, before, when call those list of commands from script, it would have count as an error. now it will not be counted as error. but as you said, there was not point of calling those commands from script so not sure it is a breaking change. however, the no_multi flag isn't exposed to modules to this day, so i guess modules couldn't have relied on setting no_script, and had to react to client_deny_blocking anyway. right, currently modules have to respect the `client_deny_blocking` flag.",0,1,0,0.4999946355819702,0.7451695799827576,0.6326360106468201,0.0,accept,majority_agreement
1436004306,11568,triggered valgrind [a link],0,0,0,0.9891389012336732,0.9894379377365112,0.995079517364502,0.0,accept,unanimous_agreement
1436612140,11568,fixed the comments except for [a link] which i am waiting for input.,0,0,0,0.979903519153595,0.9727162718772888,0.9927759170532228,0.0,accept,unanimous_agreement
1436848227,11568,"basically we could have more commands pending in the input buffer but i am not sure we have a way to turn back and process the input buffer (did i miss something?) i am not sure i am follow, each blocking rm_call is invoke on a separate fake client. maybe i do not understand the problem you refer to?",-1,-1,0,0.5651408433914185,0.5723500847816467,0.9212429523468018,-1.0,accept,majority_agreement
1436905245,11568,"well correct me if i am wrong: for regular client blocking via modules, the executing client is blocked and then later when it is unblocked by the module is queued on 2 lists: - the moduleunblockedclients - which is processed in the before sleep right before processing unblocked clients - the unblockedclients list which is processed after the modulehandleblockedclients() and is also processing what is left to be processed in the inputbuffer. in your case if the module command is currently executed and is spinning a fake client which is then unblocked. the fake client is reprocessed and execute the module continuation callback. but what will cause the processing of what is left in the executing client inputbuffer? (again i might be missing something, but wanted to raise that anyway)",0,0,0,0.8507667779922485,0.857093870639801,0.9521327018737792,0.0,accept,unanimous_agreement
1437028354,11568,is this covers what you refer to? [a link],0,0,0,0.9864365458488464,0.9884628653526306,0.9956271648406982,0.0,accept,unanimous_agreement
1438064567,11568,. o.k now i understand how this is not a problem in the blockedclient implementation. since the test module is also making sure to create a redismoduleblockedclient on do_rm_call_async and is unblocking the original client on the on_unblock callback (rm_call_async_on_unblocked) so the original client will be reprocessed as a regular blocked client. we better make sure to document that using promise on calling blocked commands require blocking the originating client as well (or we change the implementation to handle that),0,0,0,0.9027106761932372,0.9902593493461608,0.8630858063697815,0.0,accept,unanimous_agreement
1440050572,11568,"we better make sure to document that using promise on calling blocked commands require blocking the originating client as well (or we change the implementation to handle that) the async rm_call is not responsible to release the original client that started it all, there might not even be an original client. it just give you a way to call blocking commands and continue your logic when unblocked.",0,0,0,0.9825283288955688,0.9908732175827026,0.99347323179245,0.0,accept,unanimous_agreement
1440335894,11568,i understand. you are right that the context from which the blocking rm_call might not be a real client context. maybe we should just make sure to document this as i suspect it is not trivial (at least it wasn't for me :) ),0,1,1,0.5064109563827515,0.9007828831672668,0.9854809641838074,1.0,accept,majority_agreement
1445319025,11568,"/core-team please approve the new module api, and slight behavior change of blocking commands in scripts. see the top comment for details.",0,0,0,0.984321117401123,0.979382872581482,0.968454122543335,0.0,accept,unanimous_agreement
1448368027,11568,"a few thoughts (haven't gone through all comments yet, so perhaps some of its discussed below initial comment 1) re ""the module should not free the promise call reply."" i'm not sure i personally like that, the general behavior is to free the callreply objects returned from rm_call. this changes those semantics. might it be better for there to be a counter, so that when one registers the handler, it increases it, and a free will behave as one expects? this is especially true in the context of ""watch"", where in reality there is no promise/callback for future, its just persistent state. 2) any promise/asynch type functionality, i think should have the ability to abort it as well. i.e. the ""promise"" should be abortable, which would remove the pending client. the idea here would be a timeout could be implemented external to the rm_call. i'd argue that basically blocking commands should always in this model be implemented with an infinite timeout, and if the module wants to timeout the call, they'll simply set up an rm_timer callback with the abort functionality built in.",0,0,0,0.8612056374549866,0.9697654843330384,0.893031656742096,0.0,accept,unanimous_agreement
1448794619,11568,"another thing re command name i.e. asynccall. why not really think about just creating a new rm_call with a better api, not just take the old api and change it slightly. its had so many things retrofitted onto it over the years. in general, redis avoids adding new apis, but this is now a good reason to create a new one and deprecate rm_call.",0,0,0,0.9427406191825868,0.8715265989303589,0.9820039868354796,0.0,accept,unanimous_agreement
1449426112,11568,"we considered adding another api (which would be confusing since people will need to decide which one to use), and eventually concluded that there's no need for an additional api, and all the new (and future capabilities we're considering) can be handled by the current one (either by passing flags, changing context state before calling, or manipulating the reply object). so despite the current api being awkward, we rather not have two. calling watch (when we'll some day support virtual clients being attached to the context before calling rm_call), would not block and return a promise, it'll return ok. i agree we need a way for the module to abort the call if possible before being executed, not sure about the freeing issue, i'll let meir reply.",0,0,0,0.8734310269355774,0.9041844010353088,0.910815179347992,0.0,accept,unanimous_agreement
1449512146,11568,"re ""which would be confusing since people will need to decide which one to us"", i'd imagine it as not being confusing, the old one would be depcrecated, and the new one would be the ones to be used for new code, it support all use cases of the old one, but be smarter. ex: today we conflate ""flag strings"" and what we can call arg format strings. i'd argue that, a) they should be separated and b) i'm unconvinced that we even need the format strings. rm_call itself having to do arg handling like this (with all the memory allocations (ala modulecreateargvfromuserformat) slows it down for use cases where that's not neccessary. if redis is supposed to be fast, we shouldn't require extra work to be done in cases where its not neccessary. i.e. i'd argue something along the lines of `rm_call2(redismodulecontext ctx, uint64_t flags, callback, uint num_args, redismodulestring **args)` if callback is passed, we return everything to it (blocking or not), its essentially the same rm_call as today (minus formatting of args). one can pass an `rm_call_block` flag, which would be the `k` in the args here. for users today we sho want to be creative with their args there will be an `redismodulestring **rm_parseargs(uint *count, char *format, ...)` the big advantage is that it removes significant overhead (and in my opinion) is a cleaner api. just my 2c.",-1,0,0,0.6081259250640869,0.8493750691413879,0.7758163809776306,0.0,accept,majority_agreement
1449707732,11568,"we considered that option. we concluded that either way, we won't be adding more and more arguments to the api (and create rm_call3 and rm_calluser, and rm_callblocking, and all other variants). and then we concluded that since we go with setting various options to the context, there's no need to change the api. since there are masses of code already using the existing one, deprecating it won't do much, and we'll remain with 2 variants and a lot of confusion. we didn't discuss the overheads at this point, if we wanna resolve just that, we can create an additional interface with a different way to pass the argv, but keep the same approach with all the rest, so both can support the same feature set, and it's just a matter of how the argv is passed.",0,0,0,0.905889928340912,0.9690380692481996,0.9525119662284852,0.0,accept,unanimous_agreement
1457601844,11568,"today, if the module will try to free the promise call reply he will get an assert. we can change it to just ignore. wdyt? when we discussed we said that abort will be part of virtual client api that will follow. if we believe this should be part of this api then we can allow calling abort on the promise object (if it was not yet resolved). let me know what you think.",0,0,0,0.9752305150032043,0.9548112750053406,0.987008273601532,0.0,accept,unanimous_agreement
1457641865,11568,"if the module doesn't need to release it elsewhere, and ignoring it doesn't add any risks, i think it may be ok (then we should document that it's unnecessary to release it as it is ignored). i'm also ok with the assertion, i'm not certain which one of these options is worse. it seems logical to me to handle it now, it makes the api more complete and doesn't stray away from the purpose of this pr.",0,0,1,0.8087906837463379,0.8818953633308411,0.6193578839302063,0.0,accept,majority_agreement
1468065559,11568,"for reviewers who already reviewed the code, here are the summery of the recent changes: ### allow aborting the async rm_call introducing the ability to abort the async rm_call in case it was not yet invoke. to achieve this, a new api was introduce, `rm_callreplypromiseabort`, that gets the promise callreply and abort it. this new api also enforce a changes of the ownership definition of the promise callreply. before it was only borrowed by the module, the module got the promise callreply just so he can set the unblock handler and never touch it again. now the module should be able to keep it for abort purposes. to support that we introduce a new ref counted object (shared ownership) that keep the required information for the unblocking operation (the unblock callback, private data, ...). this object is owned both by the promise callreply and the fake client that was used to invoke the command. in addition it is now also the module responsibility to free the promise callreply, and he must do it when the gil is acquired. ### better atomicity garentee on pure redis command though the documentation says nothing about atomicty guarantees other then the fact that the unblock handler run atomically as an execution unit. we did want the implementation to give some better guarantees. we have the flexibility to decide in the future if we want to strengthen those guarantees or to drop them. the better guarantees are, on pure redis blocking commands, the unblock handler will run atomically with the command that got unblocked. to achieve that, we call the unblock handler right after `processcommandandresetclient` which is called from `unblockclientonkey` (if the client got unblock), and in addition we make sure all this code is running as a single execution unit.",0,0,0,0.7942506074905396,0.9866359829902648,0.9773067831993104,0.0,accept,unanimous_agreement
1468129859,11568,"anyone who's following this pr, please also see the new `additional redismodule api and changes` section in the top comment.",0,0,0,0.9864631295204164,0.9917497038841248,0.9934695959091188,0.0,accept,unanimous_agreement
1468410368,11568,this pr was discussed (again) and conceptually approved in a core-team meeting.,0,0,0,0.9788503050804138,0.984966278076172,0.9944374561309814,0.0,accept,unanimous_agreement
1470640820,11568,full ci: [a link],0,0,0,0.9845400452613832,0.8597871661186218,0.9959890246391296,0.0,accept,unanimous_agreement
1484595612,11963,"is read (in beforesleep) considered a part of cron? and about the 3 instantaneous stats, are they updated every 1.6 seconds, or updated every loop? i think we can leave these to external monitoring tools, so the server can just provide basic stats.",0,0,0,0.9863079190254213,0.9931341409683228,0.9919002056121826,0.0,accept,unanimous_agreement
1484668938,11963,"my idea for a ""cron"" metric was to measure the sum of active-expire, active-defrag and all other tasks done by cron and beforesleep that are not counted by the other set of metrics (so we should exclude read, write and aof from these). regardign the ""instantaneous"" metrics, i agree they're unnecessary for people who have a proper monitoring software connected to redis. they're there for easy consumption from command prompt by using redis-cli. i meant to add them to the stats_metric_count mechanism.",0,0,0,0.9410627484321594,0.9893938302993774,0.976744830608368,0.0,accept,unanimous_agreement
1487863781,11963,"agree, cron sum is useful, especially about the expire cycle and defrag cycle in `servercron`. moreover, we also have two other time events, `moduletimerhandler` and `evictiontimeproc`, do you think we should record these two? i'm not sure if they're useful, and `instantaneous`...`max` these two words are a bit strange together. maybe you wanna record how many eventloop cycles in last xxx seconds, and average/max duration in the last xxx seconds. but i think it should take longer like 1 min, and we should introduce another way to record them instead of using the current instantaneous metric.",0,0,0,0.6115774512290955,0.8170995712280273,0.8383527398109436,0.0,accept,unanimous_agreement
1488063869,11963,"i don't think we wanna break down cron to each and every task it contains. we already have latency monitor metrics for active expiry and defrag, so for the purpose of event loop monitoring i think it can be inclusive without a breakdown (other than io which we exclude from it). then if someone realizes the event loop takes long because of cron, they can dig into it with other ways. regarding the instantaneous metrics, iirc last time we argued about these metrics (in #10062), i argued that we don't need them and you argued that we do... i do think we need to keep them to a minimum, maybe we should only include only `instantaneous_eventloop_cycles`, or none of them. having just `duration_avg` without `duration_max` feels a bit odd to me, and having just cycles per second without any duration metric also feels a bit odd. if the problem is the contradiction of ""instantaneous"" and ""max"" terms, maybe we can add ""recent"". regarding the duration being 1 second or 1 minute, i don't wanna introduce another ""infrastructure"" for this, so i'd rather keep the 1.6 seconds thing we have now. under constant load, it would be sufficient to have them represent 1 second (will show same values as 1 minute), so let's keep that. in that case maybe having an average duration, and no max is actually ok. bottom line, if you are ok with it, let's drop the max and add just two.",0,0,0,0.9124085903167723,0.8991016745567322,0.9344983100891112,0.0,accept,unanimous_agreement
1488251253,11963,"so do we have a final design for this? info or histogram? and what metrics to expose? if we just show the metrics in info, we may have a new section `info eventloop`, and it contains the list below (may be drop the instantaneous_eventloop_duration_max). [code block] if we want a histogram, we may have a new command (or perhaps a new subcommand under `latency`?), and it returns the information below. [code block] or we show sum in info, and details in histogram?",0,0,0,0.985057294368744,0.992646098136902,0.9914537668228148,0.0,accept,unanimous_agreement
1488285062,11963,"i'm leaning towards keeping the simple metrics in info stats (with the instantaneous but without the `max`), and exposing the histogram in latency eventloop. the histogram data itself in the event loop should probably be plain (similar to the latency histogram output), but maybe put under a ""histogram"" field in a map, so we can extend that map in the future.",0,0,0,0.970772385597229,0.9907268285751344,0.9839052557945251,0.0,accept,unanimous_agreement
1488317820,11963,"i need clear it, i mean instantaneous max metric is strange. imho instantaneous is always used on xxx per second metric, like qps for query per second and bandwidth for net bytes per second, i don't know how to understand ""duration time per second"".",-1,-1,-1,0.8312159180641174,0.9119916558265686,0.9589841961860656,-1.0,accept,unanimous_agreement
1488382440,11963,"i view the ""instantaneous"" as a metric representing the very recent time (last 1.6 seconds), it doesn't have to be a per-second metric. `instantaneous_ops_per_sec` is a per-sec, but it doesn't mean there couldn't be an `instantaneous_cpu_usage`. i.e. unlike `used_memory_rss` which represents the current state, cpu usage, and ops/sec represent delta of some metric over time, and we choose to show the delta over the last 1.6 seconds, i don't see a problem showing `instantaneous_eventloop_duration_avg`",0,0,0,0.9725821018218994,0.98722106218338,0.9816061854362488,0.0,accept,unanimous_agreement
1488412307,11963,"seems you misunderstand the current instantaneous mechanism or i missed something? it is the average of the past 16 per-second metric samples, in another word it is indeed the per-second metric that just calculated as an average of 16 samples, nothing todo with 1.6 seconds. [code block]",0,0,0,0.9400967359542848,0.971173107624054,0.978791356086731,0.0,accept,unanimous_agreement
1488506328,11963,"i think the confusion is that you're referring to the way they're implemented and i'm referring to the way they're presented to the user. if we go my way, we can rename `trackinstantaneousmetric` to `trackinstantaneousratemetric`, and add a similar `trackinstantaneousavgmetric`. i.e. the current mechanism takes a rate sample (ops / sec) once in 100ms, and then computes an average of these over 1.6 seconds. but we can add another similar function that takes a sum and count samples every 100ms to compute an average (average duration in this case), and then compute an average for these over 1.6 seconds. i'm saying 1.6 seconds because there are 16 samples and we take one every 100ms.",0,0,0,0.973200500011444,0.9469240307807922,0.960404932498932,0.0,accept,unanimous_agreement
1496036321,11963,"since this was stale for a while, i'll try to promote a minimal version, so the status from my perspective (if we put aside the [a link] feature) what's missing is: * add `eventloop_duration_cron_sum` - total time spent in servercron, and beforesleep (excluding writes and aof) agree, cron sum is useful, especially about the expire cycle and defrag cycle in servercron. * remove `eventloop_duration_max` for now. imo it could be nice to have two metrics for easy consumption from redis-cli (but not a must): * `instantaneous_eventloop_cycles` - cycles per second in the last 1.6 seconds * `instantaneous_eventloop_duration_avg` - average duration in the last 1.6 seconds to clarify (since i didn't see a response) we can rename `trackinstantaneousmetric` to `trackinstantaneousratemetric`, and add a similar `trackinstantaneousavgmetric`. `getinstantaneousmetric` can remain as is, and i don't think any of that contradicts the ""instantaneous"" concept from the user's perspective. can you handle these changes or maybe want me to try to step in and help?",0,0,0,0.8535407781600952,0.9836810827255248,0.6816320419311523,0.0,accept,unanimous_agreement
1496323262,11963,"sorry about the delay, i went on vacation last weekend and i will get back to this tomorrow.",-1,-1,-1,0.9879335761070251,0.9919577836990356,0.9925050735473632,-1.0,accept,unanimous_agreement
1496982060,11963,no worries.. was just trying to see if i can help,1,1,1,0.8370429277420044,0.9381585121154784,0.955636203289032,1.0,accept,unanimous_agreement
1500870806,11963,"i'd prefer you avoid force-pushes. it makes it harder to do incremental reviews. since we're gonna squash-merge it (using the pr top comment as a commit comment, these incremental commits are ok",0,0,0,0.9608797430992126,0.9696657061576844,0.9569666385650636,0.0,accept,unanimous_agreement
1514907515,11963,"we discussed this in a core-team meeting, specifically the [a link] about whether or not we should expose a breakdown of the composition of the event loop or not. there were two main concerns: 1. the current code for measuring io adds per-read overheads (not per event loop), and could slow down the throughput. 2. some of the metrics in the breakdown are hard to document and we may want to change them in the future which could be considered a breaking change. the decisions we came up with: 1. drop the io measurements. in theory, it could be possible for someone to compute the delta between the total and the sum of breakdowns and conclude that this might be io. 2. keep the event loop cycle counter, duration_sum, and cmd_sum in the `stats` section, but chuck the other ones in some `dev`/`debug`/`experimental` section that's not displayed by default (not in even in `all`) we didn't get to discuss the instantaneous ones. from the threads above, it seems we agree on keeping `instantaneous_eventloop_cycles_per_sec`. we can name the other one `instantaneous_eventloop_duration_usec` or drop it for now. other than that, i do think we can follow up on many of jim's cleanup suggestions, and then i'd hope we can merge this.",0,0,0,0.8606746792793274,0.984628677368164,0.931434690952301,0.0,accept,unanimous_agreement
1518044309,11963,"another interesting metric could be the number of commands per event loop cycle, specifically the max. considering that we now intend to introduce an experimental section, maybe we can add `eventloop_cmd_per_cycle_max` and maybe re-add `eventloop_duration_max` to have them temporarily until histograms are present? wdyt? is the non-`default` (and non-`all`) ""experimental"" section hidden enough to add these temporarily, or you still rather avoid them for now?",0,0,0,0.9707221388816832,0.9917483329772948,0.987909495830536,0.0,accept,unanimous_agreement
1537658518,11963,"seems we forgot it? i think it's useful, since the metric in info may be lost, due to the issue of sampling frequency. it's bettor to record the historical data.",0,0,1,0.9836673736572266,0.974867582321167,0.7268441915512085,0.0,accept,majority_agreement
1537743849,11963,"forgot what? our plans to add a histogram? we didn't forget it, just assumed that we won't have time to deal with it for 7.2. if or someone else has time we can try pushing it forward, but i'd rather have this one merged as a backup plan, even with some [a link] metrics in the debug section that can later be removed.",0,0,0,0.9618542790412904,0.981856644153595,0.5684863924980164,0.0,accept,unanimous_agreement
1541432320,11963,"so we now have these in `info stats` [code block] and these in the hidden `info debug` [code block] do we really need `eventloop_cmd_per_cycle_max`? i don't see this metric very helpful. if nothing else is to be modified in `info`, maybe we can merge this as a temporary conclusion and move to a new pr to handle histograms.",0,0,0,0.9801523089408876,0.9840400815010072,0.930067777633667,0.0,accept,unanimous_agreement
1542674774,11963,"we discussed this in a core-team meeting, and since we're leaving the histograms for a future pr, which might not make it to the next release, we'd like to add the two metrics (eventloop_cmd_per_cycle_max and eventloop_duration_max) to the debug section. besides that, i'd love to have some (even basic) test coverage. can you handle these?",1,1,1,0.5656964778900146,0.7928807139396667,0.7200396656990051,1.0,accept,unanimous_agreement
1545671854,11963,full ci: [a link],0,0,0,0.9845400452613832,0.8597871661186218,0.9959890246391296,0.0,accept,unanimous_agreement
1545718741,11963,| before and after sleep 1 | idle | memtier_benchmark | --pipeline 100 | | |--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------|--------------------------------------------------------------------------|----------------------------------------------------------------------------|---| | eventloop_cycles - delta 1s apart eventloop_duration_sum - delta 1s apart eventloop_duration_cmd_sum - delta 1s apart instantaneous_eventloop_cycles_per_sec instantaneous_eventloop_duration_usec eventloop_duration_aof_sum eventloop_duration_cron_sum eventloop_duration_max eventloop_cmd_per_cycle_max | 13 1618 69 10 108 24 10043 247 1 | 1052 979175 237679 1058 932 478 209836 23137 199 | 133 1017352 294548 127 7841 646 292085 26621 18400 | | | | | | | | | | | | | |,0,0,0,0.8913344740867615,0.9810600876808168,0.862115740776062,0.0,accept,unanimous_agreement
1545738161,11963,"please take a look at the test failures, some are thresholds that should be adjusted (take a safe distance, maybe twice of what's printed), and there's also some odd fortify compilation error that i didn't look into.. thanks.",1,0,1,0.8542550802230835,0.6919031739234924,0.605755627155304,1.0,accept,majority_agreement
1545945292,11963,"tests are now passing, and i also verified that we don't see that fortification error in unstable. trying to figure out how it is related to this pr..",0,0,0,0.919965386390686,0.9034370183944702,0.9674009084701538,0.0,accept,unanimous_agreement
975183372,9822,"thanks. i like the idea of saving memory when there are many clients that don't actually need it. but i don't like to add an extra config. i would like to look into a possibility of making a better redis by default, without users needing to spot the problem and tune it. in theory, we can realloc the client from time to time. i.e. create a small one initially, and make it grow when we see it normally consumes more than that first buffer. but then i'm worried about a usage pattern that creates disposable clients. i.e. client connects (created small), then runs some traffic, get resized, and then disconnect. if we're flooded by such usage pattern, we could cause a significant regression. -steinberg i'd like to hear your thoughts.",1,1,1,0.9318982362747192,0.9871720671653748,0.9044768810272216,1.0,accept,unanimous_agreement
975276283,9822,"thank you for you quick reply! i agree that adding a new config is restricting and an automatic adjustment mechanism would better serve the general case. i do think there is a variety of use cases and sometimes users would mostly like to use as much memory is the cost of performance and in some cases the other way around while identify the use case might sometimes be problematic. we can consider introducing an occasional memory relocation for clients, but usually once the relevant memory is already allocated for user data, there can be little spare for clients resizing (unless evictions are made) we can also come up with a formula to choose the optimal client size based on maxclients and maxmemory configurations, but again this might be restrictive and sub-optimal in some user setups. given said all that i would be happy to followup and consider a better alternative :)",1,1,1,0.9319704174995422,0.9796792268753052,0.9921225905418396,1.0,accept,unanimous_agreement
975617335,9822,"since the client output buffer is dynamically allocated with an initial static buffer of 16k and then a linked list of dynamic buffers, i'm guessing that any usage pattern that needs large buffers will cause dynamic allocations anyway so doing this dynamically doesn't make sense to me. this should be either a compile time constant (like it is today) or a config (like proposed here). both are good solutions. i looked a bit at the history of this 16k size, i see it started out as 4k, then grew to ~7.5k and then grew to 16k. from the logs i couldn't really understand why. i think we should be willing to consider making this smaller (lets say 8k) to save memory.",0,0,0,0.7461072206497192,0.7858883738517761,0.9501447081565856,0.0,accept,unanimous_agreement
975632865,9822,"thank you -steinberg ! i also agree regarding the dynamic reply allocations once that fixed size buffer is filled (which is why reducing the size will result in performance degradation) i believe this change was first introduced by in this commit: [a link] i can start an effort to test performance of different reply sizes and check the performance degradation, but as i said before - i suspect this is mainly intended for users to opt-in their redis to specific use cases (which are not that common imo)",1,1,1,0.9753785729408264,0.9658334255218506,0.9900402426719666,1.0,accept,unanimous_agreement
976339763,9822,"as you said, this buffer is need for performance (avoid allocations and also the indirection and possibly cache locality we'd have if it was a pointer). as was pointed out, it was smaller in the past then then increased, so obviously decreasing it is not a wise idea. i generally want to avoid adding configs for such tuning. too low level and i don't think users need to worry about it. also i'd like to improve memory usage even for ones who didn't bother to dig in and realize what's eating their ram. you can proceed to run some performance tests, maybe we'll realize something new. but i'd also like to try to think of some automatic mechanism that can be put here instead of that config.",0,0,0,0.8649567365646362,0.788946807384491,0.8729333281517029,0.0,accept,unanimous_agreement
977220521,9822,"how about this: - clients are allocated with some minimum fixed `buf` size (lets say 4k). - if this buffer is ever maxed out (meaning we added a dynamic allocation to the `reply` list) then we turn on some `client_buf_was_full` in `flags` in the client struct. - in `clientscron()` periodically we check if this flag is on. if it is we reallocate the client struct so its `buf` is doubled and replace it in the clients list (this means we can't have this client object referenced anywhere else). after reallocation we zero the flag. - we need a maximum too (lets say 16k) and stop growing the client struct after we reach this max. so we need to keep track of the `buf` size in the client struct. - things to consider: - we need a shrinking mechanism to reallocate to a smaller size if between crons we don't use more than half the buffer. so we need another flag or perhaps the flag should always just mention half the buffer size? - this design might not be possible because of client struct references. an alternative can be to chuck the preallocated `buf` at the end of the client struct and just make sure we always have an initial entry in the `reply` list which we can manage with similar cron based growing and shrinking logic. - the cron intervals might not be good enough? do we need to add some timestamps to the client struct signifying ""last grow/shrink time""?",0,0,0,0.9777662754058838,0.9931556582450868,0.9906122088432312,0.0,accept,unanimous_agreement
977577900,9822,thank you -steinberg! i was thinking the same direction. we can keep track of cases were the buffer was full. as you said we can check this counter on the clientscron and enlarge the buffer when the counter crossed some boundary we can also establish some decay function on the counter of the buffer full occasions and on the same cronjob reduce the buffer size when this counter is below some boundary. what i am more concerned about is the effect of making this a dynamic buffer in terms of cache. we can relocate the entire client struct but that would introduce much complexity. i will now work to produce benchmark results for this buffer size with different payload sizes so that we can understand better and take correct actions before continue.,1,1,1,0.953876256942749,0.9834924936294556,0.9878308176994324,1.0,accept,unanimous_agreement
977585312,9822,"buffer when the counter crossed some boundary... i'm not sure a counter is the right direction here. when are we going to increase this counter? different traffic patterns will produce different results but not necessarily meaning a higher value requires a larger buffer. my thought was that if the buffer was maxed out it means we will benefit from a larger buffer. this means a simple flag might be enough. i'm not worried about cache because my assumption is that we do the reallocation only periodically and this interval needs to be large enough to minimize performance implications which include both the cache invalidation and then `memcpy` itself that will happen during the reallocation. i think we just need large enough intervals, and in the case of redis between 0.1-1 is probably more than enough (these are the _in practice_ intervals you'll get with the `clientscron` implementation). excellent! waiting for the results..",0,0,0,0.7400402426719666,0.6100007891654968,0.9356258511543274,0.0,accept,unanimous_agreement
978911204,9822,i have made several performance tests. all tests where done on an m5.2xlarge instance on aws. i only wanted to measure get performance so i initially fill up the data with **512 byte**s size values over **3750000 keys**. each test run for **60** seconds using **5 clients** and different pipeline and reply buffer sizes. | tps | p50 | p95 -- | -- | -- | -- p=1 | | | 1k | 111097 | 0.039 | 0.047 4k | 109655 | 0.039 | 0.047 8k | 113081 | 0.039 | 0.047 16k | 111977 | 0.039 | 0.047 32k | 111933 | 0.039 | 0.047 p=5 | | | 1k | 229835 | 0.103 | 0.119 4k | 311151 | 0.071 | 0.087 8k | 314887 | 0.071 | 0.079 16k | 315922 | 0.071 | 0.079 32k | 313676 | 0.071 | 0.079 p=10 | | | 1k | 359264 | 0.119 | 0.143 4k | 357449 | 0.119 | 0.143 8k | 418340 | 0.103 | 0.119 16k | 420073 | 0.103 | 0.119 32k | 417734 | 0.103 | 0.119 p=15 | | | 1k | 422487 | 0.151 | 0.175 4k | 427107 | 0.151 | 0.175 8k | 482625 | 0.135 | 0.159 16k | 486129 | 0.135 | 0.151 32k | 488663 | 0.135 | 0.151,0,0,0,0.952581822872162,0.976414918899536,0.8975070714950562,0.0,accept,unanimous_agreement
979197938,9822,"a few random notes: 1. regarding the benchmark, i think the objects are too small, 512 byte and pipeline of 15 only 7kb, which is why you can only see a difference of performance between buffer size of 4k and 8k. (btw, which benchmark tool did you use, and did you make sure to cpu is not saturated on either host in this test?) 2. we can't just watch the fact the reply list was populated since there are cases of deferred reply, which will use the list even if the reply is small. 3. static buffer is faster than a pointer because of indirection indirection, we must keep a static buffer as part of the client struct and relocate the whole struct when needed, but we can maybe skip that if the client is listed in some list (like blocked, etc). 4. when resizing, we wanna avoid a spike of many clients resized at the same time, i.e. many clients connected at the same time and will reach the threshold on the same second. 5. keep it simple!, i think a boolean may be enough, or anyway, let's not make it too complicated.",0,0,0,0.9390749335289,0.9849682450294496,0.9253760576248168,0.0,accept,unanimous_agreement
980547357,9822,"i think the benchmark does show us how significant splitting the reply between the static buffer and the dynamic list is. once the reply is split into two buffers we see a major performance impact. this is enough to convince us the original idea of having a big pre-allocated buffer is a good idea. it might be a good idea to try the tests again, but change the code so the pre-allocated buffer is a dynamic buffer allocated in `createclient()` and not part of the `client` struct (in other words make `buf` a `char*`). this will help us asses how important it is to keep the static buffer implementation or which complicates things if there clients is referenced elsewhere.",0,0,1,0.5576573014259338,0.6130248308181763,0.5808625221252441,0.0,accept,majority_agreement
981107074,9822,"thank you 1/ i agree the value size has major effect on the results. i based the 512 since in our internal statistics this reflects the majority of the value size used. also note that in my current implementation the reply list is still growing in at least 16k interval (which is also something to consider) so i did expect to only get a single performance drop point for each test case. i can repeat the test with much bigger value sizes, but i think the general concept should be clear by these results. per your question - i used redis-benchamrk -p -c 5 -t get -r 3750000 -n 3750000 and an additional parameter test-duration which was introduced in the benchmark code to limit the test time. the cpu was operating at high rate, but i still think we can have a clear grasp of the performance degradation from these results. however i can perform more tests in order to satisfy our view of the potential degradation. 2/ also agree here the main trigger should consider the buffer state and not the existence of the reply list. when i get to detailed design i will address all the issues. 3 / that is also my callout previously regarding the fact that we should reallocate the client struct. however this might greatly complicate the implementation which might raise the question of profitability. keeping logic for client is pointed by others will be very hard to maintain and will introduce risk of bugs in some scenarios. i really support avoiding the indirection, but lets first establish the cost of it. 4+5/ 100% agree",1,1,1,0.92726331949234,0.9429566264152528,0.982955515384674,1.0,accept,unanimous_agreement
981107949,9822,"i agree with you -steinberg . i did perform a comparative test of dynamic vs static buffer: again with preallocated 512 values in the db. redis-benchmark -p 30 -c 5 -t get -r 3750000 -n 3750000 | tps | p50 | p95 -- | -- | -- | -- static | | | 4k | 542377 | 0.239 | 0.271 16k | 586762 | 0.223 | 0.255 dynamic | | | 4k | 540891 | 0.239 | 0.271 16k | 585845 | 0.223 | 0.255 as you can see the results currently does not reflect any issue with the dynamic buffer (less than 1% tps degradation) but i might want to run a more ""realistic"" test with many client machines and 3k clients in order to better support this assumption",0,0,0,0.9095348119735718,0.7192206382751465,0.9648709297180176,0.0,accept,unanimous_agreement
981135467,9822,"but i might want to run a more ""realistic"" test with many client machines and 3k clients in order to better support this assumption this is good news. this means we might be able to have a single preallocated `buf` which we can grow/shrink periodically without worrying about the client struct being referenced in other places. regarding reallocating many `buf` at the same time, this will be mitigated by the fact that `clientscron()` doesn't handle all clients at the same time, to avoid such potential spikes for any per-client cron work. and if this won't be enough we can limit the number of clients processed per-cron cycle even more. this should probably be enough.",1,1,1,0.7731540203094482,0.5847574472427368,0.5047431588172913,1.0,accept,unanimous_agreement
981344467,9822,"i want to stress two points form my past discussions with salvatore. 1. when i suggested to get rid of the static buffer and replace it with a pointer, iirc he argued that this will degrade performance (i think he mentioned cache efficiency, which i'm not sure is right, but it might be the indirection). so my point is that if we go this way we need to really test this carefully, maybe even on a smaller replies, like a pipeline of pings. 2. iirc (you can validate that in git blame), clientscronresizequerybuffer has some limit not to try to resize buffers that are smaller than 4kb. this condition was added after complaints that this function caused latency issues (despite the fact we only process certain amount of clients per cron).",0,0,0,0.9134863018989564,0.8677646517753601,0.9604339599609376,0.0,accept,unanimous_agreement
981600569,9822,thank you ! 1/ i was originally also concerned regarding the tlb/cache effect of the indirection. i totally agree that in order to validate the cache/indirection effect of moving to dynamic pointer buffer will not introduce degradation. in aws we have specific benchmark setup which i already started to adjust for this testing purpose. i will circle back with the results asap 2/ ack - maybe we will keep a lower limit of 4kb. i will make sure to test small buffer sizes in the tests,1,1,1,0.9805504083633424,0.9853228330612184,0.9940231442451476,1.0,accept,unanimous_agreement
993788434,9822,"hi and -steinberg i am sorry for the long delay - i was held back due to some personal + work obligations. in order to support the dynamic reply buffer i have implemented a very simple poc which does the folowing: 1. allocate an initial 1024 bytes buffer. 2. when the reply buffer is filled we set a flag to increase the buffer size x2 (done in cron job) 3. when the reply buffer is fully written in case the bytes written is up to 1/2 the buffer size we set a flag to shrink it by 1/2 (done in cron job) 4. when the client is idle for idle-time secs we set a flag to shrink it by 1/2 (done in cron job) i have performed some benchmark tests to validate the dynamic buffer overhead: all tests performed on: - m5.2xlarge instance - 3 io-threads (main+2 threads) - 468750 keys with 4k values - 15 c5n.2xlarge client machines running single threaded redis-benchmark. - each redis benchmark running 50 clients with only get requests and run for 900 seconds in the first test i compared 3 variations: 1. static - the current static buffer 2. non-static - same as current implementation only buffer is pre allocated during client creation 3. dynamic - poc version described above. | non-static | dynamic | static -- | -- | -- | -- tps | 163985 | 163945 | 162103 p90 latency(ms) | 6.62 | 6.09 | 6.3 in the second test i run the static vs dynamic variation **and used pipeline of 5 commands** and set idle-time to be one of {2,5,7,10} the reason is to locate the optimal idle-time to shrink the buffer the results are: | tps | p90 latency(ms) -- | -- | -- static | 253523 | 13.103 dynamic/10 secs | 263910 | 13.96 dynamic/7 secs | 247233 | 16.56 dynamic/5 secs | 237149 | 17.103 dynamic/2 secs | 231911 | 17.5 imo the pipeline scenario illustrates a very intensive workload, however i think there is no special reason not to define a high idle time in order to shrink the buffer. i think the results indicate the dynamic buffer will not introduce a performance degradation, however the dynamic shrinking might introduce some in specific workloads (large values + many clients). i do not see any reason why the idle time should not be high, since the main purpose is to support long time idle connections. please share your thoughts, and if we agree to continue i can introduce the more constructed version of my poc",-1,-1,-1,0.9892452955245972,0.9837544560432434,0.9496282935142516,-1.0,accept,unanimous_agreement
993834452,9822,"thank you , this looks promising. a few questions: - i'm not sure i understand the logic behind the buffer shrinkage. between cron intervals there might be many cases where we've written 1/2 the buffer but also many cases where we filled it completely. so you'll have both flags set. what do you do? i think what should be done is that if at the beginning of the cron interval the buffer isn't half full we can turn on a flag saying ""potential_to_shrink"", if during the interval we pass half the buffer then we turn off the flag. if by the next interval the flag is still on we can shrink. - i didn't understand what you mean by ""idle_time""? is this time of no activity on that client? if so how does this relate to your benchmarks, because surely there's some activity for all clients within the given intervals you checked (2-10sec). why do we need an idle time shrinking mechanism if we can shrink based on half buffer usage anyway?",1,1,1,0.9637688994407654,0.9871084094047546,0.9880762100219728,1.0,accept,unanimous_agreement
994700518,9822,"also i have checked the statistics i added for the poc to monitor the number of expansions and shrinkage during test of idle-time=2 and idle-time=7: for the idle-time=7: while for the idle-time=2: i think though the numbers are not high, given the tests are identical in terms of setup and workload, we should expect no expansions. **btw: i was wrong before claiming the initial buffer size is 1kib. in my poc it is 16kib...**",0,0,0,0.8935684561729431,0.950236201286316,0.9814122915267944,0.0,accept,unanimous_agreement
995522720,9822,"according to this design proposal: ""potential_to_shrink"", if during the interval we pass half the buffer then we turn off the flag. if by the next interval the flag is still on we can shrink. you won't need the idle mechanism because shrinking will be handled in the cron even when there's no traffic. in any case i still don't understand why you had **any** idle time in your benchmarks. i also think that your benchmark might be shrinking the buffer too often in case both flags contradict. i might be wrong because i have a feeling i don't totally understand your logic in the poc. but since the results seem promising i suggest you create a (possibly draft)pr of your code so we can review the algorithm. does this make sense?",-1,-1,0,0.6117332577705383,0.6070149540901184,0.5690460801124573,-1.0,accept,majority_agreement
998056588,9822,-steinberg - i pushed the latest poc implementation of the dynamic reply buf size. the main changes are: 1. maintain per client buf_peak - i think that a better option than observe the last write position is to maintain a peak observed between cron runs. i reset the peak after each cron iteration on the client thus i am able to ignore the idle check. 2. the shrink/expend logic is maintained only in cron job according to the following logic: - in case the last observed peak was equal to the buffer size - double the buffer size - in case the last observed peak was less than half the buffer size - shrink the buffer in half. - otherwise do nothing 3. i added counters for the number of expends and shrinks (global counters) and added them in the info stats. 4. added some reply buffer info in the client list. i have retested benchmark results in compare to the static buffer case: | | -- | -- | -- | dynamic | static tps | 164353 | 164667 p90 latency(ms) | 6.1 | 6.1,0,0,0,0.9608526825904846,0.9849225282669068,0.7910417914390564,0.0,accept,unanimous_agreement
999394391,9822,can you update the pr with your new design (instead of config option for buffer size) and update the title accordingly.,0,0,0,0.987541139125824,0.9934011101722716,0.9954590797424316,0.0,accept,unanimous_agreement
999539705,9822,-steinberg done. also made some cache misses analysis and added it to the pr. thank you for reviewing this!,1,1,1,0.9864341020584106,0.9857982993125916,0.983867883682251,1.0,accept,unanimous_agreement
1027265682,9822,"hi, i'm finally free to take another look at this. i understand that the benchmark mentioned at the top done with pipeline of 1, right? are you sure that in this case the bottleneck wasn't the network? i.e. in order to measure the impact of the extra indirection, we need the cpu to be the bottleneck (loaded to 100%). maybe you have additional feedback or a quick way to test this.",0,0,1,0.8855814933776855,0.8686193227767944,0.6945221424102783,0.0,accept,majority_agreement
1027267996,9822,"the other concern in terms of benchmark is the effect of resizing. i.e. an app that uses a lot of dormant connections that suddenly start loading the output buffer with a lot of data, and then goes silent (all connections are growing or shrinking at the same time). this pr will probably introduce a regression for this case, which is expected since that's also the case where we save a lot of memory, but maybe we can limit the impact by throttling the growing and shrinking.",0,0,0,0.9401825666427612,0.9433133006095886,0.9812442660331726,0.0,accept,unanimous_agreement
1027925450,9822,thank you for taking the time to review this! indeed in this case i used a pipeline of 1. i checked and during the test all engine and io-threads were running in 100% cpu and the network limit was verified by our tools to not reach the limits.,1,1,1,0.9877147674560548,0.9814261794090272,0.9903871417045592,1.0,accept,unanimous_agreement
1027931897,9822,i understand the concern here. i wonder if the result of throttling the rate of change (like the number of resizes per second) will not cause other type of degradation due to dynamic list allocations/deallocations. i can look into this or simply try to make cron job operate the resizing every n runs instead of each run. i actually tested some variation of it but i will have to repeat performance tests in order to be sure about the results.,0,0,0,0.6905307769775391,0.9495893716812134,0.9358702898025512,0.0,accept,unanimous_agreement
1028913477,9822,"fyi, i'd rather avoid force-pushes (amending commits and rebase-merges). these are a bit harder to review and realize what's changed, and since we use squash-merge for most prs, it doesn't really matter for the end result.",-1,0,0,0.6663921475410461,0.9003268480300903,0.9684873223304749,0.0,accept,majority_agreement
1028944047,9822,no problem - point taken.,0,0,0,0.9743822813034058,0.929407000541687,0.6747143864631653,0.0,accept,unanimous_agreement
1029272969,9822,i'm having a hard time reviewing the recent changes to to the force push that mixed a rebase and other changes [a link] do you remember what you changed other than the cron timing?,-1,-1,-1,0.9562022089958192,0.5796899199485779,0.7384019494056702,-1.0,accept,unanimous_agreement
1031172064,9822,i am very sorry for that! my changes where: 1. fix the introspection test 2. fix the cron function to reset the peak every 5 seconds 3. align the cron boundary check on the minimum/maximum resize threshold 4. fix the styling issues you raised,-1,-1,-1,0.9896021485328674,0.9928050637245178,0.9940866231918336,-1.0,accept,unanimous_agreement
1031791720,9822,/ -steinberg thank you for this educating review (one of my first prs so it might be a bit glitchy :) ) i might have located some race condition in case the cron will resize the buffer while the bufpos is not 0 (i suspected a potential issue there) i will start debugging it and circle back here. sorry if this is taking longer.,1,-1,1,0.8450242280960083,0.701162576675415,0.9913365840911864,1.0,accept,majority_agreement
1039918374,9822,full ci: [a link],0,0,0,0.9845400452613832,0.8597871661186218,0.9959890246391296,0.0,accept,unanimous_agreement
1040453120,9822,re-run the ci [a link],0,0,0,0.9884950518608092,0.98980975151062,0.9956825971603394,0.0,accept,unanimous_agreement
1040675854,9822,summary of failures: failed 6 times: [code block] failed 2 times: [code block] failed once (might be unrelated): [code block],0,0,0,0.9229084253311156,0.990406334400177,0.9834307432174684,0.0,accept,unanimous_agreement
1046110181,9822,using the 14 benchmarks we have on the oss benchmark spec we can: - detect a total of 2 improvements above the improvement water line. - 13.6% on the hgetall test with hashes with 5 fields with 100b each field - 6.5% on lpop rpop benchmark with lists with 100b values on each element - detect a total of 9 stable tests between versions. - note that 2 benchmarks are unstable (high variance) on the unstable branch so we discard their numbers of the analysis. [code block],0,0,0,0.9764623045921326,0.9900085926055908,0.9905519485473632,0.0,accept,unanimous_agreement
1046174356,9822,"13.6% on the hgetall test with hashes with 5 fields with 100b each field 6.5% on lpop rpop benchmark with lists with 100b values on each element interesting. i was not expecting any improvement in performance, only hoping for no regression. -steinberg do you have any idea what could cause this? looking at the changes this pr [a link] it looks like it attempts to revert many of the recent changes. please fix.",0,0,1,0.9164603352546692,0.626788318157196,0.5023853778839111,0.0,accept,majority_agreement
1046183086,9822,i fixed the pr commits. regarding the performance tests: this is hard to tell as i was also hoping only for no performance degradation. do we have any system metrics to match with the benchmark (maybe some perf recordings which are operated in the background during the tests? syscalls/cache misses/tlb misses etc...),0,0,0,0.9714098572731018,0.9416343569755554,0.9714314937591552,0.0,accept,unanimous_agreement
1046663335,9822,"/core-team please approve. there are some interface (introspection) changes, in addition of course to the new mechanism that has no interfaces, but is a major change with some potential for issues.",0,0,0,0.9829968214035034,0.9862384796142578,0.9820343255996704,0.0,accept,unanimous_agreement
1842129847,12822,can you please share your thoughts?,0,0,0,0.9699105620384216,0.968350350856781,0.9788411259651184,0.0,accept,unanimous_agreement
1845957570,12822,"i guess i'm still not convinced that we need a separate entity that differentiates the db from the dict array, to me they still seem one and the same. there is a lot of logic being silo'd out into the dictarray that is still pretty heavily coupled to redis, cursors and per-slot information for sampling.",0,0,0,0.8636593818664551,0.6437644958496094,0.916232407093048,0.0,accept,unanimous_agreement
1862439640,12822,note that the merge commit [a link] inserts the logical change of moving the `resharding` list from the server struct into `dictarray` i.e. in some way handling portion of #12848 in the reverse way.,0,0,0,0.9886026382446288,0.9947112798690796,0.9934499859809875,0.0,accept,unanimous_agreement
1867954869,12822,"thanks for working on this. not right away evident to me, when does the code developer life becomes easier or avoids them making mistake with this refactoring? could you highlight some api access or usage pattern?",1,1,1,0.8503004312515259,0.9191501140594482,0.956774115562439,1.0,accept,unanimous_agreement
1868751199,12822,hi the main motivation is code cleanliness. imho the dict-array code is self-contained enough to deserve its own file and api.,0,1,0,0.8514537811279297,0.4955349564552307,0.863906741142273,0.0,accept,majority_agreement
1873698807,12822,"to be honest, based on the literal meaning, i prefer db over da.",0,0,0,0.5857474207878113,0.944844365119934,0.9047978520393372,0.0,accept,unanimous_agreement
1874246842,12822,"this pr was discussed in a core-team meeting and was conceptually approved. some things remain to be argued about like: 1. the abbreviation of ""da"" which we don't like. 2. the fact we would be maintaining the bit data structure for shard_channels (may rarely be needed, e.g. for iteration, but on the other hand should have a low overhead). 3. i have a feeling i forgot one. please comment.",0,0,0,0.5777968764305115,0.8679654002189636,0.9054783582687378,0.0,accept,unanimous_agreement
1891285223,12822,"i think i've resolved most of the issues raised, can you please take a look?",0,0,0,0.945432722568512,0.9057068824768066,0.9275557398796082,0.0,accept,unanimous_agreement
1920536317,12822,"worth mentioning that due to the recent change the reply of debug htstats changed, in case of no keys were ever added to the db. before: [code block] after: [code block]",0,0,0,0.9783552885055542,0.9910949468612672,0.9912593960762024,0.0,accept,unanimous_agreement
1920572493,12822,"ok, i don't mind. but please mention it in the top comment.",0,0,0,0.9266974329948424,0.9093759059906006,0.8530434370040894,0.0,accept,unanimous_agreement
1926409258,12822,"i'd like to merge the pr as is, and maybe address the last remaining comment later. any objections?",0,0,0,0.9777069091796876,0.9898311495780944,0.9872865080833436,0.0,accept,unanimous_agreement
930424227,9564,"could you please update [a link] on part: human readable names for nodes to add this pr link, thanks",1,1,1,0.968446969985962,0.9049853682518004,0.6275767087936401,1.0,accept,unanimous_agreement
932866870,9564,sure. should we be propagating the cluster node name to other nodes?,0,0,0,0.9883760213851928,0.992137372493744,0.9868109822273254,0.0,accept,unanimous_agreement
934543383,9564,do you mean propogating nodename or hostname? i have added the humanreadable nodename to the cluster nodes commands shown below: ![a link],-1,0,0,0.8665324449539185,0.9828160405158995,0.9942799806594848,0.0,accept,majority_agreement
935265940,9564,"yes, sorry, i meant node name.",-1,-1,-1,0.981809377670288,0.9898530840873718,0.936522901058197,-1.0,accept,unanimous_agreement
941528820,9564,"hey , is there anything that needs to added for this pr or is it fine?",0,0,0,0.97690612077713,0.983059823513031,0.9909849166870116,0.0,accept,unanimous_agreement
956614386,9564,"hey , i have updated the code and added the gossip functionality for the human readable name. and the node's name can be set manually by using the command: cluster setname. please take a look. thank you.",1,1,1,0.9523230195045472,0.9861273765563964,0.984933078289032,1.0,accept,unanimous_agreement
1009378790,9564,"hey, now that the hostname extension framework is merged, do you want to follow up on this? i think it should be easy to add a new extension that is the nodename.",0,0,0,0.9849322438240052,0.97919362783432,0.9914259910583496,0.0,accept,unanimous_agreement
1010386774,9564,"thanks madolson, i will start working on this pr. thanks",1,1,1,0.9860370755195618,0.9885363578796388,0.9936386942863464,1.0,accept,unanimous_agreement
1031585342,9564,"hi , i agree that this can be used as debugging/human readable name that would be exposed in places like info and logging. this would be better than having a node id when trying to debug any issue with a particular node.",0,0,0,0.9676353335380554,0.9525742530822754,0.8196955919265747,0.0,accept,unanimous_agreement
1079956732,9564,"i think we can change the ip to hostname, and we can use hostname to identify the node. no need add nodename ![a link]",0,0,1,0.9001381397247314,0.939663827419281,0.5440453290939331,0.0,accept,majority_agreement
1080130046,9564,"we cannot easily modify the output of the command (any commands), it will be a breaking change and also node ip is useful information in any case i think, no reason to ""remove"" it",0,0,0,0.9787624478340148,0.9710018038749696,0.988298237323761,0.0,accept,unanimous_agreement
1094064504,9564,"ok, another question, can you confirm the impact on large clusters.for example: the cluster has 300 masters and 300 slaves",0,0,0,0.9796748757362366,0.9869158267974854,0.9874743223190308,0.0,accept,unanimous_agreement
1099198048,9564,", i have added some tests, let me know if you think they are fine. thanks!",1,1,1,0.9748865365982056,0.9942080974578856,0.9899407029151917,1.0,accept,unanimous_agreement
1504064988,9564,"hey , i have updated the code as per the comments. please take a look. thank you.",1,1,1,0.9282807111740112,0.9863708019256592,0.984976887702942,1.0,accept,unanimous_agreement
1509288586,9564,"/core-team another small change that is probably suitable for 7.2. it adds a human readable node-name that is available in logging for redis clusters that is gossiped, so that node failures/failover authorizations can also include the logical name. the only ""major change"" part is the config and the fact it's exposed in `cluster nodes`.",0,0,0,0.9872713685035706,0.993376076221466,0.962058126926422,0.0,accept,unanimous_agreement
1513364864,9564,"not really, although it is a label of free text, but it will display in the result of cluster nodes command, not only error happens.",0,0,0,0.9740755558013916,0.980380117893219,0.9694187045097352,0.0,accept,unanimous_agreement
1550450247,9564,"i thought i commented on this, we decided to merge this change, [a link] first and then we will merge this.",0,0,0,0.9710801839828492,0.9785856008529664,0.9936561584472656,0.0,accept,unanimous_agreement
1572272931,9564,"hello , #12166 is merged and i have also resolved all the conflicts. kindly review.",1,1,0,0.539458155632019,0.8780708909034729,0.5341977477073669,1.0,accept,majority_agreement
1577882835,9564,"we discussed this previously, but i'm going to double check no one else wants to review it, and then will merge it tomorrow.",0,0,0,0.9769656658172609,0.9613634943962096,0.9892128705978394,0.0,accept,unanimous_agreement
1586036280,9564,"the top comment needs an update, right? i.e. that's a new config that affects the generated config files, and some logging, but doesn't affect any commands at the moment, right? please make sure the top comment / commit comment mentions that, as well as reasoning and future plans. thanks.",1,1,1,0.9181596040725708,0.976947784423828,0.952039897441864,1.0,accept,unanimous_agreement
1592304253,9564,"sorry for the delay, just got behind. everything should be updated and consistent now.",-1,-1,-1,0.9881640672683716,0.9936437606811525,0.9932770133018494,-1.0,accept,unanimous_agreement
1592364341,9564,"will you double check the top comment and look at my last commit. i tried to write a slightly better test, simplify the code, and updated the redis.conf text.",0,0,0,0.9817805886268616,0.9829592704772948,0.9930012226104736,0.0,accept,unanimous_agreement
1593288039,9564,"thanks for updating them. it looks good , sorry i missed the comments.",-1,1,1,0.8676726222038269,0.8195695281028748,0.983712673187256,1.0,accept,majority_agreement
1597330863,9564,sporadic test failure: [code block] [a link],0,0,0,0.8397983312606812,0.989215850830078,0.9810325503349304,0.0,accept,unanimous_agreement
1598093285,9564,i think this should fix it: [a link],0,0,0,0.9808472990989684,0.9790619611740112,0.9595544338226318,0.0,accept,unanimous_agreement
1125460355,10515,"thanks for your comments. sorry it's taken so long for me to get back to you. responses inline: regarding the general approach, i'm open to suggestions. i haven't been able to come up with anything dramatically better. we have to match up arguments with input words in order to know which words and which arguments have been consumed and shouldn't be hinted. and we have to generate a hint describing any arguments which might still be matched. some of the edge cases are tricky, though, and you've found a few of them. (nitpick: the matching isn't boolean, it's numeric, since one argument entry can match more than one input word, and an argument entry can be partially matched, such as a token without its parameter value.) actually, i thought i was handling this scenario correctly. you seem to have found a bug. i'll investigate. your first and third examples are cases i believe i can handle. i'll try to fix them. the second example is trickier, but it might also be doable. it's a bit problematic, though. consider the following partial input: `zadd k in` this is presumably the start of the token `incr`, but it hasn't been matched yet. do we want to guess this and suggest the completion of the word `incr`? we currently only match whole words, not prefixes. but `zadd k 3` is presumably the start of a score, which we could know only by trying to validate the numeric data type - so far the code doesn't do any type checking. and, of course, if the next mandatory argument was string-valued, we'd have no way to know whether `in` was the start of `incr` or the next mandatory string argument. the code currently falls back to the previous hinting algorithm if no arg specs are available. interesting. is it that common that an upgraded cli is run with an old server? if it's worth the effort, that can be left for a future task.",-1,-1,-1,0.9899427890777588,0.987194836139679,0.9483307003974916,-1.0,accept,unanimous_agreement
1126154958,10515,"regarding the mandatory arguments for tokens, i don't see how you could handle these, but please try to fix the bug and i'll review. maybe that would be acceptable. regarding optional arguments coming before mandatory ones, maybe we could have some code to realize we've hit a mandatory argument, and go mark all the optional ones before it as irrelevant (or keep a pointer to the first argument that's relevant going forward).",0,0,0,0.9791801571846008,0.9640954732894896,0.936408519744873,0.0,accept,unanimous_agreement
1130728071,10515,"okay, i think i've fixed the cases we discussed earlier. let me know what you think of them, and the behavior in other situations. i've also added a test suite for the hint mechanism. (i also seem to have messed up my branch of the repo, but i'll fix that later... sorry...)",-1,-1,-1,0.9905449151992798,0.9888667464256288,0.990404486656189,-1.0,accept,unanimous_agreement
1133923080,10515,"haven't reviewed the code yet, but i did give it a quick test. so first off, it does work **a lot** better! thank you! here are a few places were i see room for improvement: * ![a link] this works good (typing `p` doesn't yet do anything) * ![a link] but then, i add `x` and it hides everything (since it's matching `px`, but it could also be `pxat`, so i'd rather do the matching only after hitting space (i.e. exclude the last non-terminated word from the matching and add it only when the user presses space). * ![a link] for some reason, when i type an unrecognized keyword, it messes up the rest of the matching, maybe that's avoidable. * ![a link] if i don't ""consume"" the `any` arg, it keeps hunting me like a ghost, even if i already moved on and it's no longer valid. * ![a link] if i type an incomplete keyword here (i.e. the `n`), it assume it's a score and hides all the optional args. * ![a link] and then when i add the `x` of `nx` it recognizes it's not a score, and re-adds all the optional args. * ![a link] when i type the initial `score`, it suggests `member` next, but when when i type another score, it still suggests `[score member]` ideally, it would suggest `member [score member]`. but that's petty, so just a small room for improvement, certainly not a must.",1,1,1,0.9871721863746644,0.9918593764305116,0.9951388835906982,1.0,accept,unanimous_agreement
1134651797,10515,thanks - those are some great examples. i'll go through them and see what can be improved. will update you when there's news.,1,1,1,0.9821694493293762,0.9842292070388794,0.9923098087310792,1.0,accept,unanimous_agreement
1138568102,10515,"do you mean in general? only match the last word after the user hits space? that's not the behavior of the old hints. they appear as soon as the command name is complete, and then they match each word as soon as its first character is typed. we can change that behavior in general, and maybe we should. but it would be a change from the current behavior.",0,0,0,0.9804449081420898,0.9874432682991028,0.9871342778205872,0.0,accept,unanimous_agreement
1138651560,10515,"for positional arguments, maybe we can mark them as matched as soon as one character is typed. but for optional args (specifically when there's more than one option), it could be a problem. what bothered me specifically was when you type `px`, it immediately hides all the options and shows ""milliseconds"", but it could still be pxat. i'm not certain, but i feel it might be better.",0,0,0,0.8565961718559265,0.9583574533462524,0.8045347332954407,0.0,accept,unanimous_agreement
1140521054,10515,"most of the glitches you point out in this comment can be solved by deciding that we only match words after the space is typed. i've made that change and i prefer the behavior that way. i'm inclined to adopt that rule in general. i'm not sure that's avoidable. the unrecognized keyword is not valid. i think the only solution would be to stop hinting if an unmatchable word is entered. this one is tricky. i need to think a bit more about how to fix it. i agree that it's not ideal, but i don't think it's easy to solve and i'm not inclined to spend time on it for now.",-1,-1,0,0.9126891493797302,0.7957616448402405,0.7186703681945801,-1.0,accept,majority_agreement
1148674803,10515,"i took anther quick look (mostly at the tests, not the code). afaict the main remaining issue that's hard to overlook is the problem with the any after count in georadius. regarding the other issues. * i think that after an unrecognized input, we can simply stop hinting (might be better than showing irrelevant options). * the problem i mentioned about zadd is arguably even more annoying in bitfield after repeating the first action",-1,0,0,0.5091615915298462,0.7630496025085449,0.5945965051651001,0.0,accept,majority_agreement
1151708076,10515,"okay, i believe i've fixed all the issues we've discussed. the code now handles: * repeated arguments * unmatched optional arguments ""orphaned"" by a subsequent matched argument * disabling hinting when argument matching fails * type validation of integers and doubles i can't rule out the possibility that i've overlooked some edge cases, but i think it's pretty solid in most circumstances. let me know what you think.",0,1,1,0.6273703575134277,0.604771077632904,0.9529261589050292,1.0,accept,majority_agreement
1151709169,10515,"(i should note that changes to the command arg specs will break the unit tests for the hints, since the tests use the current command docs. this just happened with `bitfield_ro`.)",0,0,0,0.9864208698272704,0.9947064518928528,0.9925585985183716,0.0,accept,unanimous_agreement
1169668366,10515,"i'm not sure how soon i'll have the time to do more work on this. when i do, i guess i'll start with separating the commands structs.",-1,0,0,0.6845260262489319,0.7272235155105591,0.6893775463104248,0.0,accept,majority_agreement
1200428623,10515,"i don't know when/if i'll have the time to do this refactoring properly. i'm not sure how best to proceed. we ended up in this situation because of the desire to support pre-7.0 servers with the best possible hinting. that required static linking of the command argument specs in the cli, which then either needs to duplicate the command tables for the client and server, or to refactor the command struct to separate the server-specific runtime parts from the shared syntax specs. that refactoring is nontrivial, since the command struct is used widely in the server code. i started working on the refactoring but am now struggling with some memory bugs, and in any case it's going to be a big change. i can't promise to spend much more time on it. so that's where this currently stands. i think the improved hinting would be a very nice feature, but the question is how much effort is it worth to support legacy servers, and how long it will take to get the rest of the work done.",0,-1,-1,0.8519214987754822,0.8014779686927795,0.8075644373893738,-1.0,accept,majority_agreement
1211821672,10515,i think i've made all the changes you asked for except for sharing the arg structs. let me know what you think when you have the chance.,0,0,0,0.9715814590454102,0.8737412691116333,0.7840326428413391,0.0,accept,unanimous_agreement
1211827067,10515,heads up: [a link],0,0,0,0.9866352677345276,0.9829875230789183,0.9918950200080872,0.0,accept,unanimous_agreement
1211839096,10515,so i assume i should wait until #11051 is merged before moving forward with this?,0,0,0,0.9832234978675842,0.989834725856781,0.9893898367881776,0.0,accept,unanimous_agreement
1211844704,10515,i guess so i'll try to get it merged tomorrow,0,0,0,0.9738737940788268,0.9751495718955994,0.9820594191551208,0.0,accept,unanimous_agreement
1212323313,10515,i removed `key_specs_static` etc. i hope i did it right. the tests pass.,0,0,0,0.7543893456459045,0.8734310269355774,0.5801780819892883,0.0,accept,unanimous_agreement
1212391480,10515,"okay, something is leaking from the changes i made to module.c... not sure what i did wrong.",-1,-1,0,0.9447643756866456,0.7421756982803345,0.6288701891899109,-1.0,accept,majority_agreement
1214646058,10515,ok. so what's next? are we conceptually done?,0,0,0,0.9790211915969848,0.9784598350524902,0.982934296131134,0.0,accept,unanimous_agreement
1214859569,10515,"as far as i'm concerned, it's done. the code is feature complete, and i think includes all the appropriate refactorings. i assume it could use to be reviewed more thoroughly, and people might want to interact with it and try to find more buggy edge cases.",0,0,1,0.947809636592865,0.855667769908905,0.6758648753166199,0.0,accept,majority_agreement
1219415415,10515,[a link] is merged,0,0,0,0.9881815314292908,0.98906147480011,0.994045615196228,0.0,accept,unanimous_agreement
1221484614,10515,thanks. i updated this pr.,1,1,1,0.8994771242141724,0.9207412004470824,0.899221658706665,1.0,accept,unanimous_agreement
1252286023,10515,is there any plan for moving this change forward?,0,0,0,0.9822192192077636,0.9889451861381532,0.9914830327033995,0.0,accept,unanimous_agreement
1253362025,10515,"my apologies, i do wanna merge it but currently don't have the time to review it properly. i did review bits of it, and provided guidelines for the design decisions we made, so i'm quite confident these are ok. maybe if we can find a way to split it, find another person with more free time to review the redis-cli.c part, and i will make sure to allocate time to review the other files?",-1,-1,-1,0.7105274796485901,0.5425164699554443,0.728249728679657,-1.0,accept,unanimous_agreement
1463745467,10515,"thanks for taking a look. at the time i did review all the parts outside redis-cli.c and did skim through redis-cli.c to provide guidance and advice. but i felt the code need an extra pair of eyes to review it aside from the author and was waiting for someone to step up. i agree the risk for issues is small but still don't like to merge a mass of unreviewed code. maybe the level of review you did is enough? i suppose there's no need to explicitly look for bugs, but i think we do need to make sure it is constructed in a readable manner and includes enough comments to be maintainable.",1,1,1,0.7938814759254456,0.9215293526649476,0.9618823528289796,1.0,accept,unanimous_agreement
1463748310,10515,"if we can pick this up and close it within a couple of weeks, i think it can fit 7.2",0,0,0,0.8923267722129822,0.9738730192184448,0.977009117603302,0.0,accept,unanimous_agreement
1463873689,10515,"yeah, i didn't execute the code in my head, but the code looks nice, seems to have enough comments, fairly short functions, comprehensive variable names, etc. i would say it's probably maintainable.",0,1,1,0.5983975529670715,0.8093279600143433,0.9057907462120056,1.0,accept,majority_agreement
1463879943,10515,"yes, thanks for taking a look at this. i'm unlikely to have much time to devote to this at the moment, however. regarding maintenance, i would just note that the test suite has a minor maintenance burden in that it is dependent on the current command specs. if some of the command argument specs change, the test cases for those commands will have to be updated accordingly (since the hint strings will change). it might be ""better"" in some ideal world for the tests to be based on some static set of command specs, but that's not realistic to implement imho. in any case, it would be nice to see this code merged. i think it's a handy feature.",1,1,1,0.7385848164558411,0.7958322763442993,0.9643521308898926,1.0,accept,unanimous_agreement
1466206195,10515,"i gave it some thought and also consulted yossi and itamar, i decided to take viktor's advise and merge it without another reviewer, considering this is an isolated area, and we can fix things or improve readability later. so now it'll just take someone to defrost this pr and bring it back to mergable state. maybe you wanna have a go at that? (since jason said he doesn't currently have the time)",0,0,0,0.8834257125854492,0.9133775234222412,0.8807369470596313,0.0,accept,unanimous_agreement
1466213727,10515,"defrost? :grin: only fix merge conflicts and possibly address my own review comments, right?",-1,0,1,0.3693382143974304,0.8342167735099792,0.9865857362747192,,review,no_majority_disagreement
1466222596,10515,"yes, plus any additional cleanup you can do which results of a another pair of eyes reading this (like clarifying come comments)",0,0,0,0.9863161444664,0.9760515689849854,0.9930480122566224,0.0,accept,unanimous_agreement
1466373984,10515,"ok, i'll do it. :+1:",1,1,1,0.7143898010253906,0.9488970041275024,0.9952840209007264,1.0,accept,unanimous_agreement
1466447981,10515,"btw, for the technical part, maybe can grant you access to his branch, or you can make prs to it.",0,0,0,0.9829410910606384,0.980587363243103,0.9909688830375672,0.0,accept,unanimous_agreement
1490558468,10515,:tada: :tada: :tada: merged thank you,1,1,1,0.9786747694015504,0.9848830103874208,0.9935275912284852,1.0,accept,unanimous_agreement
1181929176,10966,"should note, that i have a weirdness in my test module code for handling the error case, if i don't do that and just do a rm_replywithcallreply for everything (i.e. errors or not) it dies with an address sanitizer error sanitizer report [code block]",-1,-1,0,0.8094197511672974,0.5626469850540161,0.8428481221199036,-1.0,accept,majority_agreement
1182316936,10966,"the above test / module implementation is problematic, as the docs for setting acl in redis module code is not quite accurate (as i learned when i tried to do more). it says it uses the same syntax as acl setuser, but it only works on a single op at a time. need to investigate how to use aclmergeselectorarguments and then loop over the args it builds.",0,0,-1,0.6601952314376831,0.6411247253417969,0.6350839734077454,0.0,accept,majority_agreement
1202184775,10966,"open q: currently rm_callwithuser will accept a null redismoduleuser (and hence be equivalent to a plain rm_call(). should this be an error or is that behavior the desired behavior. i.e. should we require users to use rm_call() for running command as the ""root"" user or is it ok to enable that with rm_callwithuser",0,0,0,0.9874825477600098,0.9937286376953124,0.9941896200180054,0.0,accept,unanimous_agreement
1214397312,10966,"if something can already be done without relying on `null` semantics, i lean towards doing that and require user to be non-null.",0,0,0,0.9439088106155396,0.987922728061676,0.9880889058113098,0.0,accept,unanimous_agreement
1214826911,10966,can you review this?,0,0,0,0.9849414229393004,0.9909096360206604,0.9939131140708924,0.0,accept,unanimous_agreement
1223819716,10966,"so in summary of above, i reverted all the refactor changed to rm_call. no more rm_callwithuser. now we have a rm_setcontextmoduleuser. if set (open q above if it should be unset afte rm_call), it will behave like the rm_callwithuser (i.e. assign the user to the 'tempclient' allocated). otherwise, its basically normal rm_call, with a slight logic change for 'c' acl checking to use either the ctx->user if defined or ctx->client->user if not. to enable it to distinguish ctx->user == null from the ""null"" user, created a nulluser const object that can be referenced if want to actually set it to the ""null/root"" user. now, unsure its totally necessary (just don't do an acl check if if want the null user), but perhaps it makes code cleaner to have single paths with explicit mentioning of null user then to have 2 different paths if null or not.",0,0,0,0.9792470335960388,0.986867904663086,0.9866929650306702,0.0,accept,unanimous_agreement
1229433143,10966,"sorry for the rebase, i was trying to squash to additional commits together and that rebased my whole branch by accident.",-1,-1,-1,0.989333152770996,0.9924787282943726,0.9912598729133606,-1.0,accept,unanimous_agreement
1229461011,10966,"so please let me know from which commit i should start reviewing the new content? i understand the force push contains a fresh copy of unstable, and that the squashes you made are to meld new commits that i didn't see together right? so there should still be a commit that represents what i saw and after it ones that i didn't see.. or should i better just start over?",0,0,0,0.9567291736602784,0.9763941168785096,0.9670011401176452,0.0,accept,unanimous_agreement
1229562185,10966,"yeah, i meant to just squash some local commits together before pushing, but muscle memory was rebase -i upstream/unstable (need to break that muscle memory). and then after realizing push failed, should have reset head, stashed the changes, reset commit to origin/branch and then stash popped. sorry about that. i believe its only the last one or two commits that you haven't seen (probably just the last one)",-1,-1,-1,0.9893606901168824,0.9887372255325316,0.9848595261573792,-1.0,accept,unanimous_agreement
1250618202,10966,"/core-team please approve or comment. please notice the behavior change for rm_call(""eval"") with the ""c"" flag, will now mean the script gets checked with acl too (behavior change).",0,0,0,0.9844974279403688,0.9887729287147522,0.9646692872047424,0.0,accept,unanimous_agreement
1250751822,10966,"in private conversation wondered why we need the new `rm_setcontextmoduleuser` api when we have `rm_authenticateclientwithuser`. it works by overwriting the user of `ctx->client->user` with whatever the caller passed in. so, i'm confused by how this works and what the side effects of it are. 1) `rm_call` seems to be designed to work even with `ctx->client` is null, i.e. when `rm_authenticateclientwithuser` would fail. i.e. today if one calls `rm_call` with the 'c' flag (i.e. for acl checking), it explicitly fails, which makes me presume that this is an expected use case. 2) the `ctx->client` object is used at a number of other places within the code. examining the code, it seems that the ctx->client is allocated in normal use cases when the `redismodulectx` is created. i'm unsure if it can ever not be allocated? what is the code path where it would be null? and that leads to a follow up q, if (true) every redismodulectx has its own allocated `ctx->client`, why in `rm_call` do we need to ""allocate"" a temp client to issue the `rm_call` with? why can't we use the client that is part of the context itself (no allocation, no resetting after execution....)",0,0,0,0.7680325508117676,0.767326831817627,0.5021878480911255,0.0,accept,unanimous_agreement
1250825050,10966,"i think we still need to do some cleanup and call reset family functions, because we are making some changes to that client in `rm_call()`, no? ---- just thinking out loud, can reentrant/concurrent calls of api functions cause problems? `rm_call()` calls a module function, module function calls another api function which also uses `ctx->client`. (or makes another rm_call()?, i don't know if this is allowed). so, if we are reusing same `ctx->client`, it might be in a partially modified state. i don't know if this is a valid/possible scenario. (my brain melts while trying to think these scenarios).",0,0,0,0.9689528942108154,0.6963068842887878,0.9667454957962036,0.0,accept,unanimous_agreement
1250831884,10966,"rm_authenticateclientwithuser and rm_setcontextmoduleuser are meant for completely different purposes. the first one is meant to allow implementing commands such as auth and change the currently active user, and the second one is meant only to affect rm_call. i understand that maybe we can achieve what you want to do using the existing api and then undoing the change, but it feels wrong, and would have side effects. i'm not sure about the null edge case you mentioned. regarding the fake client, it is needed for many reasons, one of them is so that the caller of rm_call would get the reply and not the real (socket) client.",0,-1,-1,0.7195519804954529,0.5849255323410034,0.788495659828186,-1.0,accept,majority_agreement
1252346876,10966,pr approved in a core-team meeting. requires one final review and it can be merged.,0,0,0,0.9701481461524964,0.973015069961548,0.9935196042060852,0.0,accept,unanimous_agreement
590485393,6929,"i started implementing this almost 5 years ago, so i decided to create a new pr. the old one is here: [a link]",0,0,0,0.9367128014564514,0.9665408730506896,0.9829263687133788,0.0,accept,unanimous_agreement
590489569,6929,"i was in doubt about mainly 3 things: - tests: i just replicated all the rpoplpush and brpoplpush tests. they add quite some testing time (only the blocking ones), but i think they add very little value, since the code is practically the same. but anyway, i felt that i should replicate then all, it's easier to just erase some now if that's the case. - comments: i didn't add any new comments, just modified the existing ones. didn't really feel the need to, since the lists code is quite clean and easy to understand, and my changes don't make it more complicated. - coding style: i did my best to comply to overall coding style, although i haven't seen any guideline, just tried to look at the surrounding code to grasp it. but i'm a strong advocate of strict and consistent coding style, so i was confused because there are some bits of code that don't seem to comply with the overall style, like for example max line length. i tried to keep my code under 80 characters, but there were some long lines already, so when i modified these i just kept it as one-line. if anyone wants to point something that is not conformant with the project coding style, i'd be very grateful and eager to fix it.",0,0,-1,0.5730974078178406,0.6901437044143677,0.4888923168182373,0.0,accept,majority_agreement
590837475,6929,+1 for being persistent :),1,1,1,0.928856372833252,0.9942434430122375,0.9865332841873168,1.0,accept,unanimous_agreement
648364189,6929,"did a rebase with unstable solving a minor conflict (wasn't an actual conflict, just a change very close to my changes flagged as a possible conflict). as a plus it solved the ci checks that were failing before (and wasn't my fault as expected).",0,0,0,0.9283538460731506,0.9514036774635316,0.7850587368011475,0.0,accept,unanimous_agreement
648700956,6929,"looks like a good coincise implementation , thanks. and since this is a plain extension of the original implementation, i guess the replication part should work as expected (didn't check the details). so indeed it is worth a careful review and a potential merge.",1,1,1,0.9795457124710084,0.9727247357368468,0.9913949370384216,1.0,accept,unanimous_agreement
649771096,6929,"great, let me know if there is anything else i can do to help or improve the code!",1,1,1,0.9745415449142456,0.978249192237854,0.989264190196991,1.0,accept,unanimous_agreement
657140034,6929,please review and consider for 6.0.6 would it be too much to ask you to help with the respective docs once approved? ;),1,1,1,0.4964251518249511,0.9445403218269348,0.9854700565338136,1.0,accept,unanimous_agreement
657238006,6929,"not at all, please count me in for anything you need, it's a pleasure contributing to redis!",1,1,1,0.986150085926056,0.9924901127815248,0.9956455826759338,1.0,accept,unanimous_agreement
657239081,6929,"thanks for your review, i'll go over everything right now. i may struggle a bit with the tests, since i've never worked with tcl, i just copied the existing tests for rpoplpush, but i think i'll manage. just a quick question: what is the usual process here, do i overwrite my branch (`push --force`), or should i push multiple commits and afterwards we squash them (or maybe don't even squash, just merge as it is)? for now i'll just push new commits, but i'm asking because i like to keep my git history very clean, so i usually do a lot of overwrites on feature branches to keep them as tidy as possible before merging.",1,1,1,0.7292554974555969,0.9744716882705688,0.9743198156356812,1.0,accept,unanimous_agreement
657239806,6929,"i like my git history clean too, we'll either squash it at the end, or if for some reason we want to keep some changes in separate commit, we'll do some cleanup. for now you can either add more commits, or amend and push-f. if you need help with the tests let me know, i think you'll just be able to use your clipboard and common sense..",1,1,1,0.9322391748428344,0.6952239274978638,0.7549352049827576,1.0,accept,unanimous_agreement
657253761,6929,"i just pushed some new commits addressing everything you commented. i just replied every thread instead of resolving the conversations, if you want i can resolve all the trivial ones (removing duplicate tests for example).",0,0,0,0.984353482723236,0.9653359055519104,0.9893223643302916,0.0,accept,unanimous_agreement
657377376,6929,thanks you. everything looks great. hope to merge it soon (although not sure it can go into 6.0). p.s. out of curiosity i run the list test. original version took me 11 seconds the one before my review comments took 26 seconds and the last revision took just 7 seconds. so adding tests and still runs faster.,1,1,1,0.9886963367462158,0.994462788105011,0.9957262277603148,1.0,accept,unanimous_agreement
657618917,6929,"great, thanks! makes total sense that it runs faster too, the idea you gave of replacing the `after` calls with `wait_for_condition` is a very good optimization, no sense in waiting a full 1s for something that usually happens a lot faster.",1,1,1,0.9936461448669434,0.9948558807373048,0.9965161085128784,1.0,accept,unanimous_agreement
658664956,6929,"/core-team since this pr adds commands (which we can't later remove or rename), it requires more than lazy consensus. there's no need to review the code again, but please approve the addition of these 6 new commands.",0,0,0,0.9772818088531494,0.986335039138794,0.9806191921234132,0.0,accept,unanimous_agreement
661624702,6929,"i think the new commands are sensible. i want to bring up a separate point that i think we should rewrite the blocking command infrastructure. today it works by marking the client as blocked on a key with a bunch of special variables, when that key is written to we used those stored special variables to complete the request. this has several issues: 1. we maintain a lot of duplicate code to process the requests. 2. the code as written is sort of spaghetti code. 3. we may still have to re-block after the command is executed, which requires special handling in that case which is unnecessary. 4. explicit rewriting for replication. instead, i think we should merge the blocking framework with the mechanism that was outlined for the background threads. if a client is block from a non-existent key, it should just remove itself from the event handler and register that it is blocked on a key. when that key is touched, the blocked client will be unblocked and will attempt to re-execute the command that blocked it in the first place. this shouldn't introduce any major compatibility issues and should throw the same exceptions we do today. it can then inline the command it has within the replication system. it should hopefully remove a lot of the weird checks that we have to do that are included in the cr. i think this refactor is worth it either for this change or so that it is easier to make these sort of changes in the future. (if we agree it's worth it for this change, i'm okay with this specific pr getting merged and a separate one for the refactor)",0,0,0,0.7603965997695923,0.9495702385902404,0.5096784234046936,0.0,accept,unanimous_agreement
661687919,6929,great idea! we need to remember to unblock clients according to the order they were originally blocked (if two or more clients are blocked on the same key),1,1,1,0.9917725920677184,0.9872295260429382,0.9961082339286804,1.0,accept,unanimous_agreement
661689784,6929,"i agree that some major refactoring may be useful there, and it should certainly be in a separate pr. had this pr include such refactoring it would have been very hard to review.",0,0,0,0.9422481060028076,0.9547100067138672,0.9641170501708984,0.0,accept,unanimous_agreement
661705107,6929,"such a refactory (suggested by above) adds considerable risk, and should anyway be done after we merge this pr or any other pr that adds small features or fixes to the blocking code, otherwise it'll be one huge merge conflict. however, since we can't push this pr into 6.0.x (due to adding new write commands and causing compatibility issues with aof/replicas), we need to carefully chose when to merge it to unstable (although the changes are not that heavy to create major cherry pick conflicts). then we need to decide when to start that major refactory (possibly for redis 7.0, but before adding any other features that will build on top of the refactored code). any thoughts?",0,0,0,0.9382429718971252,0.976276159286499,0.9383944272994996,0.0,accept,unanimous_agreement
661794403,6929,"i agree we should merge this closer to the time of next major release. i think this is a great idea, do you have any suggestions how to push this forward? maybe a dedicated issue and initiate a discussion around how the api for this can look like in more details?",1,1,1,0.9569119811058044,0.7873080372810364,0.985140323638916,1.0,accept,unanimous_agreement
662160309,6929,"added a new issue: [a link] for immediately tracking the idea. i don't have the time right now, but i'll spend some time this week outlining all the touch points and how they'll get updated. i think 7.0 is a good target, so i would propose sketching out the design and then implementing it when we find appropriate.",0,0,0,0.849835216999054,0.9005267024040222,0.6182160973548889,0.0,accept,unanimous_agreement
663266186,6929,"i updated all references to brpoplpush in a new commit here: [a link] did it in a new commit pushed to a different branch for now, if it's ok i can just fast-forward this branch.",0,0,0,0.9858078956604004,0.991032898426056,0.9927035570144652,0.0,accept,unanimous_agreement
663332651,6929,please go ahead and merge that to this branch.,0,0,0,0.9850597381591796,0.9809539318084716,0.995464026927948,0.0,accept,unanimous_agreement
663334992,6929,done!,0,0,1,0.514024555683136,0.6890168786048889,0.6799882054328918,0.0,accept,majority_agreement
667365345,6929,anything else needed before merging ?,0,0,0,0.9808947443962096,0.992837369441986,0.9942892789840698,0.0,accept,unanimous_agreement
667465170,6929,"no, this pr is good to be merged, it's just that e can't make it part of 6.0.x (would break compatibility with old replicas), so we're not sure it should be put into `unstable` since it may cause issues with cherry-picks. so i'm postponing the merging for now.",0,0,0,0.933825433254242,0.9877723455429076,0.9827508330345154,0.0,accept,unanimous_agreement
667738472,6929,"ok, great! :thumbs_up:",1,1,1,0.9911336302757264,0.9947689771652222,0.9971354007720948,1.0,accept,unanimous_agreement
673598145,6929,"just saw that there was a merge that generated some conflicts here. i'll push a new commit solving these conflicts soon, looks very simple to fix.",0,0,0,0.8050991892814636,0.8803964853286743,0.7408292293548584,0.0,accept,unanimous_agreement
675182159,6929,done. just two small simple conflicts.,0,0,0,0.97735196352005,0.7543344497680664,0.9399290084838868,0.0,accept,unanimous_agreement
675182973,6929,"i noticed some tests were failing on the unstable branch on macos (the os i'm using), so i just checked if the exact same tests are failing on my branch. the tests added/modified by this pr are ok.",0,0,0,0.9805493950843812,0.9798582792282104,0.9455268383026124,0.0,accept,unanimous_agreement
675277707,6929,"the tests are very repetitive - maybe they run in a loop-like manner? i.e. for cmd in {rpoprpush, rpoplpush lpoprpush, lpoplpush} ...",0,0,0,0.8950322270393372,0.5828178524971008,0.9834149479866028,0.0,accept,unanimous_agreement
675542399,6929,"i agree, but i never worked with tcl before, so i'm not sure what is really possible in this case. i'll review the tests more carefully, because in some of them i believe it's just a matter of adding the loop and replacing the main command. but it seems that in most cases, it's not only the main command that changes, but also the order of the values in the output variable for example, like in the ""*pop*push base case"" tests. i'll try to think of a generic way to do those tests (for all poppush commands in a loop), but i'm afraid they could get too complicated. perhaps the simplicity and clarity is enough to justify repeating instead of a loop? another possibility for these tests would be using some tcl features that i'm not aware of, or used to. if that's the case, could you provide some pointers, examples, documentation, or anything that could help me in doing that?",0,0,0,0.8933602571487427,0.9489395022392272,0.9527857899665833,0.0,accept,unanimous_agreement
680133252,6929,"replaced some repetitive tests with some foreach loops as suggested by . for the other tests, i came up with some options, but as i said, i'm not sure they are worth the sacrifice in readability, so i didn't commit them yet. instead i created this gist: [a link] these tests in the gist are still fairly simple, there are others that would require more logic/values in the loops. but if you think they are worth it, i'm more than willing to replace all of them!",-1,1,1,0.4296202063560486,0.9321810603141784,0.9488468766212464,1.0,accept,majority_agreement
684416598,6929,"sorry to bother you again with more requests. i was about to merge this, but it then occurred to me that on another recent discussion (around adding `zrangestore [byscore|bylex] [rev]` rather than `z[rev]rangebyscorestore`) we concluded that we want to to reduce the number of commands making them more flexible and later deprecate the old ones. so i would like to suggest that instead of the 6 new commands you added, we'll introduce just two: `lmove` and `blmove`, both take 4 arguments like: `lmove [left|right] [left|right]` this would make today's brpoplpush an alias for `blmove right left `",-1,-1,-1,0.9879640340805054,0.9880929589271544,0.986480176448822,-1.0,accept,unanimous_agreement
684924379,6929,"very interesting idea, i'll work on it! i'll try to have it finished by the weekend. as for the command syntax, what about making the ""left|right"" parameters be required instead of optional? and then we could move then to be like: [code block] the idea is to make it more ""readable"" like ""move from src left to dst right"". makes sense?",1,1,1,0.965811848640442,0.988781750202179,0.9938183426856996,1.0,accept,unanimous_agreement
684926362,6929,"yes, certainly mandatory.. it was a typing error.",0,0,0,0.5901200175285339,0.8221448063850403,0.8941755890846252,0.0,accept,unanimous_agreement
684927377,6929,i personally think that key names should come first (similarly to other commands like smove).,0,0,0,0.9863603115081788,0.9763034582138062,0.9778874516487122,0.0,accept,unanimous_agreement
685872597,6929,"makes sense, and consistency is certainly more important.",0,0,0,0.9394851326942444,0.9735156297683716,0.980042040348053,0.0,accept,unanimous_agreement
685927540,6929,"as a side bonus, this will make the docs simpler and less repetitive :)",1,1,1,0.8045217394828796,0.9600149989128112,0.8766781687736511,1.0,accept,unanimous_agreement
697175024,6929,"any news? if you don't think you can get to it, maybe one of us can step in..",0,0,0,0.947911262512207,0.9325308203697203,0.9753701090812684,0.0,accept,unanimous_agreement
697709763,6929,"sorry for taking so long, i'll try to get something done by friday. but if anyone wishes to step in at anytime, i've no problem with that!",-1,-1,-1,0.98891419172287,0.9930395483970642,0.9724871516227722,-1.0,accept,unanimous_agreement
703178596,6929,"i did some work on a new branch today, it's basically done, but the replication test is failing and i didn't find out why yet. [a link] i'll try to fix that by tomorrow. but the if anyone can take a look, i had to change some things, mainly add more information to the ""blockingstate"" since we lost the information about head/tail now that we have a single command.",0,0,0,0.8695572018623352,0.9484896063804626,0.8382134437561035,0.0,accept,unanimous_agreement
703191253,6929,"done! everything works, and all tests are passing. i now have just a few doubts: - is it ok to add more fields to the blockingstate struct? - i didn't add blmovecommand to the redisserver struct because brpoplpushcommand wasn't present there, but i'm not sure why. - i'm creating some strings for ""right"" and ""left"", but i'm not sure how the reference counts works in redis. are they always required? should i have made the string objs as shared objects? should i have incremented their ref-counts after creating them? also, should i even be creating these strings all the time? could i have created just two global ""constant"" strings and reused them whenever need? - was using rewriteclientcommandvector a good solution? i wasn't sure if this is how it is supposed to be used. - i kept the rpoplpush tests and created lmove tests where we had the other pop-push tests, but there were many tests just for rpoplpush. now that we have a new command with a quite different name and new parameters, i thought that maybe we would like to change all rpoplpush tests to use lmove instead. not sure if that is worth it (or even if there is any gain at all in doing that), so i just kept the rpoplpush tests as they were.",0,0,1,0.5075130462646484,0.7955658435821533,0.9888578653335572,0.0,accept,majority_agreement
703239839,6929,"i'll try to sum up what i think is left (at least the important ones): 1. create lmovegenericcommand and use it in all the command entry points instead of `rewriteclientcommandvector` or parsing arguments twice. 2. add some code to serveclientblockedonlist to be able to generate rpoplpush when the original command was brpoplpush, but keep generating lmove for the rest of the cases. 3. create static strings for `left` and `right` (just a minor performance optimization). 4. update the redis-doc pr.",0,0,0,0.9440099000930786,0.9923717975616456,0.9866721630096436,0.0,accept,unanimous_agreement
703330240,6929,"ok, just a quick question: should i create the `left` and `right` static string as attributes of `shared` (in the `createsharedobjects` function)? if not, any suggestion to where should i put those?",0,0,0,0.9819631576538086,0.9808322191238404,0.9910898208618164,0.0,accept,unanimous_agreement
703332213,6929,"on another topic, earlier today i started thinking more about the suggestions made by that we should try and use loops to avoid having repetitive tests for the poppush commands. now that we have a single command, i thought it made even more sense, so i came up with this commit: [a link] but the same concern still plagues me, that maybe these aren't worth the extra complexity, so i didn't add them to this pr yet. what do you think?",-1,0,0,0.6883165240287781,0.8113025426864624,0.6662821173667908,0.0,accept,majority_agreement
703427092,6929,"regarding the `left` and `right` strings, yes put them in `createsharedobjects` same place you see these: [code block] i skimmed though the test changes in [a link] they seem fine to me. - although the test itself is more complicated to read now, it does reduce the overall line count in the tests by sharing code between the test and adding `if`s just where they differ (earlier it would be hard to see what exactly are the difference between these consecutive tests). - it may be true that they also test shared code so they maybe it is enough to have just one test (e.g. just left to right), and no need to repeat it 4 times, but if they're quick, we don't have anything to lose, and it does add some additional coverage (may come in handy some day). - it doesn't refactor existing tests, or remove rpoplpush tests, just changes the new tests.",0,0,0,0.9181461334228516,0.9746478796005248,0.9847491383552552,0.0,accept,unanimous_agreement
703999392,6929,"just pushed 3 commits with: - the joined tests (i changed them a bit from the commit i had sent before, just to make it a little simpler by avoiding nested ifs) - the left and right shared strings - replacement of rewriteclientcommandvector with more generic function that both commands can use - compatibility for rpoplpush/brpoplpush when propagating or rewriting i'll now experiment with the wherefrom/whereto parameters of the **blockforkeys** function, and update the redis-doc pr.",0,0,0,0.9847571849822998,0.9810934662818908,0.9873033165931702,0.0,accept,unanimous_agreement
704017847,6929,"option 1 - using list_none: [a link] option 2 - using a struct: [a link] not sure if it was just me choosing terrible names, but option 2 doesn't look very good.",-1,-1,-1,0.8976860046386719,0.8936070203781128,0.9813780784606934,-1.0,accept,unanimous_agreement
704568959,6929,"just pushed the commit to create the internal listpos struct as you suggested , looks much better now, thanks!",1,1,1,0.9835562705993652,0.9940974712371826,0.986817181110382,1.0,accept,unanimous_agreement
704569061,6929,i'll take a look at the tests you mentioned.,0,0,0,0.9796159267425536,0.9677542448043824,0.9873836636543274,0.0,accept,unanimous_agreement
704586142,6929,"ok, i think i managed. tcl is really new to me, so i didn't know much about what was possible. i created a new proc to avoid changing the file `tests/unit/introspection-2.tcl` too much.",0,0,0,0.9037174582481384,0.9233298897743224,0.7895547151565552,0.0,accept,unanimous_agreement
705035238,6929,"ok, great, thanks for the orientation! this actually helped me understand something i had suspected but wasn't really sure: the propagated command is actually the non-blocking version (i had to fix that for the tests to pass). that's how it worked in the rpoplpush command before my changes, so i just replicated the same idea (blmove propagates lmove). should we change that?",1,1,1,0.9894892573356628,0.9824190139770508,0.9926906228065492,1.0,accept,unanimous_agreement
705055626,6929,"change what? you're right, the propagated command is the non-blocking one. and with the changes we did, when the original command was [b]rpoplpush we propagate an rpoplpush, and when it's [b]lmove we propagate an lmove. is that not what we did? i think the tests prove it. so what did you suggest to change?",0,0,0,0.9710174202919006,0.9792207479476928,0.9776341915130616,0.0,accept,unanimous_agreement
705074569,6929,"i thought maybe this wasn't the intended behaviour, and that perhaps we should propagate the same command (the blocking one) instead.",0,0,0,0.961600124835968,0.9833642244338988,0.9595188498497008,0.0,accept,unanimous_agreement
705084053,6929,/core-team please approve the addition of new lmove and blmove commands (deprecating [b]rpoplpush). note that when receiving a brpoplpush we'll still propagate an rpoplpush (but on blmove right left we'll propagate an lmove),0,0,0,0.9864262342453004,0.9929647445678712,0.9849604964256288,0.0,accept,unanimous_agreement
705340486,6929,merged.. thanks a lot for sticking that long and implementing all the feedback.,1,1,1,0.981531023979187,0.9906213283538818,0.9827055931091307,1.0,accept,unanimous_agreement
705636591,6929,"hey, thank you all very much for all the feedback and for taking this in, especially you ! it was my first significant contribution to an important open-source project, and i have to say, the experience was great. looking forward to contribute more!",1,1,1,0.9926209449768066,0.9955431818962096,0.9966961145401,1.0,accept,unanimous_agreement
832021954,8687,/core-team please take a look at this new feature for redis 7.0 (details at the top),0,0,0,0.9820815920829772,0.979885756969452,0.9793060421943665,0.0,accept,unanimous_agreement
856848263,8687,"there's the issue of tracking memory usage of watched keys: - a `watch` command adds the key name to a **global** dict of watched keys. each entry in the dict contains a list of clients watching that key. this means that this isn't a per-client memory consumption. so we need to think of a mechanism of limiting how much memory watched keys can consume. another config? - we also don't have any reporting of these global dicts. so mem overhead reporting should be updated accordingly. - in addition, each client contains a list of pointers to all the keys it's watching. this can be accounted for per-client, reported in `client list` and used for client eviction. this is already implemented in my last commits. any thoughts?",0,0,0,0.9298898577690125,0.9900110363960266,0.9820225238800048,0.0,accept,unanimous_agreement
856896972,8687,after talk with about how to handle `io-threads-do-reads` we came up with following concept (to be tested): to handle eviction buckets being global and update them when filling data per client in the read threads we can simply make sure all updates are either atomic decrement or increment (we need decrements when moving a client from one bucket to another). we can also check (and update) the total memory usage sum. if we pass `maxmemory-clients` we can stop processing the client or even abort the thread. when we're back in the main thread we can safely assume all sums in the buckets are valid because of eventual consistency. and at this point handle any client evictions if needed.,0,0,0,0.9676576852798462,0.9873549342155457,0.977744162082672,0.0,accept,unanimous_agreement
858448024,8687,"regarding the watched keys: i don't think the client eviction mechanism needs to be perfect and count all per-client overheads, it's ok that we solve the output buffer problem and other painful problems, and some edge cases remain unsolved (it's not a security feature). so the things that are truly per client, and are easy to count, we'll count (no reason no to), but things that are shared between clients, we can skip. we can however improve the total overhead reported in info memory, and the detailed report in memory stats to include these watch, and maybe csc (client side caching / tracking) overheads (for manual troubleshooting).",0,0,0,0.8338568806648254,0.9622061252593994,0.954533874988556,0.0,accept,unanimous_agreement
859245515,8687,"regarding the memory usage, i agree with oran that right now a best effort will catch most of the issues for now. if we get t the point in the future we see issues, we can iterate on this solution.",0,0,0,0.9422165751457214,0.9750277996063232,0.9600870609283448,0.0,accept,unanimous_agreement
860199635,8687,i didn't understand your comment about the `valid_fn` (for some reason i can't respond to that comment),-1,0,0,0.7892574667930603,0.7007514834403992,0.9319084286689758,0.0,accept,majority_agreement
860339661,8687,"i'm not entirely sure how that comment ended up there, it was a response to a sundb comment but somehow got duplicated as its own comment. i can't respond to it either, so i deleted it.",-1,-1,0,0.547038197517395,0.5789309144020081,0.6347067356109619,-1.0,accept,majority_agreement
1219066276,8687,-steinberg i got some [a link] with valgrind. maybe you have time to look into it [code block],0,0,0,0.9582033157348632,0.966213881969452,0.8755936026573181,0.0,accept,unanimous_agreement
1219096789,8687,"not sure, the test seems fine. if it recreates you can check the server logs to see why the two client's aren't being disconnected for reaching their output buffer.",0,0,0,0.9821597933769226,0.9649394154548644,0.9187992811203004,0.0,accept,unanimous_agreement
586593229,6891,"hi itamar, thanks for your comment, the `info default all` return ""info all"" i was considering this since we need to return upperbound information that user needed. if user specifies ""all"" then we can simplely return every category. thanks",1,1,1,0.9775264263153076,0.9809720516204834,0.990192174911499,1.0,accept,unanimous_agreement
942654996,6891,", can you please take a look at this pr. this is a old pr but i think it is still useful and valid. i have rebased the code for this pr to remove any conflicts. thanks",1,1,1,0.980355978012085,0.9260177612304688,0.9922029972076416,1.0,accept,unanimous_agreement
958110259,6891,"hey , i have updated the code for this pr and editted the top comment for description. please take a look. thank you.",1,1,1,0.9476047158241272,0.9843383431434632,0.987307071685791,1.0,accept,unanimous_agreement
969289871,6891,"hey , i agree with your suggesions. i have some questions about the implementation. 1) the dictionary which is passed to the genredisinfostring function, should we add `default` to it or all the subsections of default like server, memory, cpu ,etc. 2) should we unify the two loops for argc? the first one is used to check if default is present and sets the has_def_sections flag to 1. this is used to avoid duplication in the case `info cpu default` is called. the first loop won't we needed if instead of `default`, we add the individual components (server, cpu, memory, ...) of default to the dict.",0,0,0,0.9293006062507628,0.6316742897033691,0.6311842799186707,0.0,accept,unanimous_agreement
969933139,6891,"yes, the loop will add the individual default sections (`cpu`, `memory`, etc) to the dict when it sees the argument `default` mentioned, or when no arguments are passed. the arguments we pass to genredisinfostring will be: the dict, and the `all` and `everything` flags.",0,0,0,0.9890596866607666,0.9935696125030518,0.9936780333518982,0.0,accept,unanimous_agreement
992537579,6891,"hi oran, i am still working on this. i will let you know when my codes update. thanks a lot.",1,1,1,0.9803629517555236,0.9940087795257568,0.9938282370567322,1.0,accept,unanimous_agreement
998225091,6891,"hi oran, i just finish the code refactor according to your suggestion. you could find the following changes: 1. create a new function named: gensectiondict, it is used to create a dictionary 2. in the modulescollectinfo, i pass the whole dict as parameter. please take a look and let me know any concern. thanks a lot for your advises.",1,1,1,0.9771292805671692,0.9909600019454956,0.9866849780082704,1.0,accept,unanimous_agreement
1010227443,6891,"hi oran, i update the codes following your comments, and add some test cases. i think the codes are very close to our final goal. thanks and take a look.",1,1,1,0.9417701959609984,0.9864184260368348,0.9914404153823853,1.0,accept,unanimous_agreement
1026680016,6891,"it seemed that i'm unable to communicate my design suggestion, so i went ahead and just implemented it. also resolved some bugs, and the rest of my unresolved comments. merged recent unstable and made sure the tests are passing. please review my changes and let me know if you see any problem. p.s. i'm not sure the sentinel tests are good enough to detect any issues, so i suppose they better be improved. also, i think we should not rely on the module tests alone, and add some tests to the normal redis test suite.",0,0,0,0.8939687013626099,0.7156274318695068,0.5648773908615112,0.0,accept,unanimous_agreement
1027308723,6891,"as we discussed, i update the c->argv+2 and c->argc-2 to c->argv+1 and c->argc-1 to fix the minor bug, now in sentinel mode, ""info"" command run well, and i add more test cases for ""info"", ""info all"", ""info default"" and ""info everything"" commands, and i also add some test cases for ""info + one subsection or multiple sections"" for redis server mode and sentinel mode. please take a look and thanks for your great help.",1,1,1,0.9764823317527772,0.9708211421966552,0.9904766082763672,1.0,accept,unanimous_agreement
1028764265,6891,"/core-team please approve, changing info to take multiple sections.",0,0,0,0.9807974696159364,0.982364296913147,0.9869601726531982,0.0,accept,unanimous_agreement
1030873444,6891,"i've benchmarked this pr against unstable, with the plain info command (no args). even with the recent optimization of not creating the default dict every time, it's still about 10 times slower. i've tried adding an `out_defaults` argument, and adding an `defaults ||` to all the sections `if`s (so we don't resort to `dictfind` on each one), but it didn't help. it seems that the remaining calls to `dictfind` (e.g. for `commandstats` and `latencystats`) are causing the slowdown. and it also seems that the latency is the same if we have just one of call to `dictfind` or several of them. calling `dictexpand` didn't help.",0,0,0,0.8944576978683472,0.6741634607315063,0.8698116540908813,0.0,accept,unanimous_agreement
1030896673,6891,"scrap all that, the regression was due to the accidental inclusion of the `latencystats` section by default (not the use of dict). branch|throughput|latency ---|----|--- unstable|52994|0.873 w/o dict caching|47961|0.971 w/ dict caching|54347|0.851 avoid most dictfind|54854|0.842 with latencystats|6664|7.432",0,0,0,0.9809685349464417,0.992661952972412,0.9713963866233826,0.0,accept,unanimous_agreement
1030900356,6891,"my final version: args|throughput|latency ---|---|--- no args|53390|0.867 ""default""|49701|0.936 ""server""|137931|0.229 explicitly list all default sections|47596|0.979 ""all""|6565|7.539 unstable: args|throughput|latency ---|---|--- no args|49652|0.936 ""default""|49212|0.942 ""server""|140646|0.200 ""all""|6876|7.192",0,0,0,0.9740065336227416,0.9890857338905334,0.9924724102020264,0.0,accept,unanimous_agreement
750859179,8242,"can you please improve the description of the pr. please explain what exactly are you improving, what are the benefits, or what are the problems in the old code that you solve.",0,0,0,0.979677140712738,0.9843564629554749,0.9911207556724548,0.0,accept,unanimous_agreement
750868703,8242,"first of all, the `epoll_create` call is an old version of `epoll_create1`, `epoll_create` was added to the kernel in version 2.6 and its `size` argument is ignored since 2.6.8 while the `epoll_create1` call is a new version to create an epoll instance, it's added to the kernel in version 2.6.27 which accepts flags like `epoll_cloexec` to be essential in some multithreaded programs, see [a link] informed the kernel of the number of file descriptors that the caller expected to add to the epoll instance. the kernel used this information as a hint for the amount of space to initially allocate in internal data structures describing events. (if necessary, the kernel would allocate more space if the caller's usage exceeded the hint given in size.) nowadays, this hint is no longer required (the kernel dynamically sizes the required data structures without needing the hint), but size must still be greater than zero, in order to ensure backward compatibility when new epoll applications are run on older kernels. in fact, calling `epoll_create1` with `epoll_cloexec` flag first and falling back to `epoll_create` when the former doesn't work on that kernel version is a more canonical way to create an epoll instance, which is also widely used among several world-renowned open-source projects: - [a link] - [a link] - [a link] - [a link]",0,0,0,0.982763409614563,0.9944097399711608,0.9941554069519044,0.0,accept,unanimous_agreement
750871446,8242,"i still fail to understand what does it do for redis. other than being ""cleaner"" (code looks dirtier due to the fallback). is redis performing better? has less bugs? does redis suffer any bug due to not using epoll_cloexec? in which case we can maybe enable cloexec by other means with cleaner code. p.s. if that's the case, we need to handle cloexec anyway (even on old kernels where epoll_create1 is missing)",-1,0,0,0.726888120174408,0.9150207042694092,0.6259708404541016,0.0,accept,majority_agreement
750871809,8242,"p.s. the oss libraries you mentioned are ""libraries"" (unlike redis). they may need to do things in a safe way since they don't know how they're gonna be used.",0,0,0,0.9852367043495178,0.9852262139320374,0.9829818606376648,0.0,accept,unanimous_agreement
751220881,8242,"yes, but even if redis is not used as a library, this epoll fd may be misused in the future development, let's say that the epoll instance may get involved in a forked process, in which case some issues will occur, i think it's no harm to set the `close-on-exec` flag for epoll instance to prevent some potential issues and since you are uncomfortable with the current solution of `epoll_create1` + `epoll_create` fallback, how about removing the `epoll_create1` and just add a `cloexecfcntl` call?",0,0,0,0.9624857306480408,0.5142930746078491,0.9837663173675536,0.0,accept,unanimous_agreement
751446637,8242,"fd_cloexec is for `exec` not `fork`, am i missing something? redis is a heavy user for `fork` and we have a few fds that we close when doing that (see `closeclildunusedresourceafterfork`) redis does use `execve` in one place (`sentinelrunpendingscripts`), but i wonder why this specific epoll fd deserves a different treating than all the other fds we have open? if there's any specific concern for the epoll fd, maybe we should indeed merge this, but if this is a global issue, maybe we better handle it in sentinel, or in some other global way. maybe you can look into that?",0,0,0,0.8384202718734741,0.8809336423873901,0.9627865552902222,0.0,accept,unanimous_agreement
751594088,8242,"yes, it's indeed for `exec`, i was talking about the case that a parent process first `fork` a child process and then the child process `exec`. ok, i will take a look at that in the near few days.",0,0,0,0.9752047657966614,0.97674822807312,0.9886194467544556,0.0,accept,unanimous_agreement
753802964,8242,"in fact, `restartserver` also calls `execve` to replace and start over the process: [a link] , which leaks the epoll/kqueue fd opened in `aecreateeventloop` --> `aeapicreate`: [a link] thus, the `fd_cloexec` is necessary here. as for the `closeclildunusedresourceafterfork`, we can enable the `o_cloexec` when opening `cluster_config_file_lock_fd` to eliminate the explicit close to it in `closeclildunusedresourceafterfork`, making code neat: [a link]",0,0,0,0.9866287708282472,0.9940035939216614,0.9946863651275636,0.0,accept,unanimous_agreement
753805280,8242,"misunderstood the `closeclildunusedresourceafterfork` part, it didn't call `execve` after `fork`, will add the explicit close back.",0,0,0,0.977258801460266,0.9882767796516418,0.9489281177520752,0.0,accept,unanimous_agreement
753898838,8242,"`restartserver` is only used by debug restart, so only used by the test suite (and iirc even that was recently dropped), so i don't care too much about it. the only real usage of `exec` is in `sentinelrunpendingscripts` but again, do we have any reason to believe that that the event loop fd deserves a different treatment from other fds? does it create a more severe problem than others? if not, then i still think we need to find a way to solve it systematically. just taking care of one fd isn't gonna matter much.",-1,0,0,0.8826744556427002,0.5811999440193176,0.816562294960022,0.0,accept,majority_agreement
753916311,8242,"i still don't quite get it about handling it systematically, could you point it out?",0,0,0,0.6390276551246643,0.6048011779785156,0.5968512296676636,0.0,accept,unanimous_agreement
753980968,8242,"there are many other file descriptors being held open (pipes, sockets, files), e.g: `aof_fd`, and many many others. is there any reason why we must fix the `exec` problem specifically for the epoll fd, and not for others? would that file descriptor cause more damage than others if not closed properly? if not, then we either need to fix the problem for all descriptors in some way, find another solution to the problem, or conclude there's no problem for some reason. that's what i mean..",0,0,0,0.9502359628677368,0.9120059609413148,0.950740396976471,0.0,accept,unanimous_agreement
754471741,8242,"is there any reason why we must fix the exec problem specifically for the epoll fd, and not for others? now that the `execve` call is trivial in `restartserver` cuz the `restartserver` is only used for test and not an actual production feature, then the poll fds are not urgent to be set the flag `fd_cloexec` (although i think it would be better to do that). on the other hand, the only `execve` call that matters is located at `sentinelrunpendingscripts` in `sentinel.c`, i think it's reasonable to set the `fd_cloexec` on those opened socket fds to redis master servers. please share your opinion about this, thanks!",0,0,0,0.9765201807022096,0.9922277331352234,0.9886574149131776,0.0,accept,unanimous_agreement
754535807,8242,"yes, i suppose we need to set it to all our fds. (or if we can somehow conclude that this doesn't cause any problem in the case of `sentinelrunpendingscripts` we can skip it altogether. either way, i don't see any reason to specifically handle just the epoll fd and not others.",-1,0,0,0.5314886569976807,0.9827238917350768,0.9853159785270692,0.0,accept,majority_agreement
754544046,8242,"we don't have to enable `fd_cloexec` for those opened fds whose processes don't call `execve`, but we should set this flag for those fds in processes with `execve`, `sentinelrunpendingscripts` calling `execve` may not cause some severe problems but it still results in fd leaks if we don't enable `fd_cloexec`.",0,0,0,0.9858077764511108,0.988484799861908,0.9894969463348388,0.0,accept,unanimous_agreement
754567132,8242,"ohh, right, i forgot that sentinel doesn't mess with an aof. but still there are more fds there than just the epoll one. can you try to find all and handle them?",0,0,0,0.7584791779518127,0.8204830288887024,0.9294580221176147,0.0,accept,unanimous_agreement
754570917,8242,"sorry but what did you mean about more than epoll fd? i've remove the `fd_cloexec` flag from the epoll/kqueue logic and now enabling it on the sentinel socket fds, i was talking about those socket fds (see the latest commit in this pr), were you saying that there are more fds we need to handle in sentinel.c?",-1,-1,-1,0.9827916026115416,0.9652075171470642,0.9559519290924072,-1.0,accept,unanimous_agreement
754578262,8242,"oh, i've seen your point, a sentinel also starts with an epoll/kqueue fd and other fds like regular redis servers, so these fds still need to be handled with `fd_cloexec`, right?",0,0,0,0.970690369606018,0.9860878586769104,0.9904029369354248,0.0,accept,unanimous_agreement
754602777,8242,please take a look at the latest commit.,0,0,0,0.981089413166046,0.9800649285316468,0.9945173859596252,0.0,accept,unanimous_agreement
754614517,8242,"besides, should we return to the original fallback of epoll_create1? leverage epoll_create1 to create an epoll instance with epoll_cloexec in an atomic way: [code block]",0,0,0,0.9877285957336426,0.9953973889350892,0.9950568675994872,0.0,accept,unanimous_agreement
754857959,8242,"i don't see a need for atomicity. if we could have just use epoll_create1 that would have been great, but since we have to use the fallback, i think a separate code to set fd_cloexec would be cleaner. another petty note about clean code, i don't think this short logic-less function deserves a file of its own (fileopt_unix) where the the copyright notice is longer than the actual code. i think we can find a place for it in another file. maybe anet.c? anyway, let's get to the important part, and i'm sorry i can't me more useful here. we need to find a way to locate all the file descriptors sentinel uses and make sure they're all set. maybe an empiric approach is the right one? maybe run sentinel, and then using `strace` or dig into `/proc` will help us be sure we found them all? or even better, maybe we can extend the sentinel test to validate that when sentinel is stopped, all its fds are set with the flag? this way we'll even be able to spot future regressions. i'm not too familiar with sentinel mysel. maybe can chip in?",-1,0,0,0.6348176598548889,0.554132878780365,0.8626139163970947,0.0,accept,majority_agreement
754996150,8242,"okay. my initial idea is to put the `cloexecfcntl` into anet.c but it just doesn't seem to be the right place for this function to me, considering anet.c is a place for network-related functions, so i created a new file for it and thought this new file could be a separate place for all file-option-related functions in the future? but it's up to you guys, if you think anet.c is a better place for `cloexecfcntl`, then we can move it to anet.c. i caught these fds in sentinel based on the `closeclildunusedresourceafterfork` which is invoked in `redisfork()`, right after a `fork()` system call, since the sentinel shares the underlying data structure `redisserver` with regular redis servers. further more, including the particular socket fds of master redis servers in sentinel, i reckon that's all?",0,0,0,0.9250231981277466,0.8388437032699585,0.8997138738632202,0.0,accept,unanimous_agreement
755056053,8242,"compared to redis server mode, sentinel maintains extra two connections(cmd and pubsub) for each monitored master/replica node and one connection(cmd) with other sentinels monitoring same master. i think already locate the places where the socket fds was initialized.. ps maybe we can use `lsof ` to verify these opened fds if we want",0,0,0,0.9440894722938538,0.9889445304870604,0.99065500497818,0.0,accept,unanimous_agreement
755938426,8242,any new comments on this?,0,0,0,0.9770001173019408,0.9911751747131348,0.9891953468322754,0.0,accept,unanimous_agreement
755941111,8242,"i'm sorry, i'm a bit busy these days, i'll probably only be able to look into it properly next week. my only blind tip is that i think we should somehow empirically check that all sentinel fds are handled, and that it would be nice if the test suite did that so that we make sure any future fds we create will be taken care of too.",-1,-1,-1,0.9890506267547609,0.9901384115219116,0.9882871508598328,-1.0,accept,unanimous_agreement
755950824,8242,"i believe that the current opened fds in sentinel are handled properly by this pr based on an empirical approach, see [a link] which has been confirmed by . as for the verification in the redis test suite, since i'm not so familiar with it, i may need some hints about the test suite, is there any doc i can refer to? or we just submit this commit first to fix this fd leak and add tests in the next pr latter?",0,0,0,0.935685098171234,0.9737944602966307,0.9508781433105468,0.0,accept,unanimous_agreement
758373527,8242,"please take a review at this pr when you are available this week, thanks~",1,1,1,0.9256333708763124,0.9749019145965576,0.8484793901443481,1.0,accept,unanimous_agreement
758432776,8242,"i haven't forgotten you, it's just that i'm still busy (trying to close some gaps to release new version), and this pr requires some digging on my part, and isn't urgent. i'll get to it later this week. sorry.",-1,-1,-1,0.9896822571754456,0.9916406869888306,0.9939993619918824,-1.0,accept,unanimous_agreement
758438050,8242,"ok, take your time.",0,0,0,0.929775059223175,0.9085839986801147,0.9661815166473388,0.0,accept,unanimous_agreement
760258263,8242,"i finally got to look into this pr, sorry for the delay. i've added a few minor comments, but what's more important is to know that these changes cover all the fds that sentinel uses. as i said, there is really no reason to handle some if we don't handle all of them (doing so won't resolve the problem). i don't know the sentinel well enough to judge, so i made some modifications to the test suite to see if this pr closes all of them, and i see that it doesn't (i.e. some sockets and pipes are still open). as you can imagine by now, i'm a bit overloaded (need to respond to many other contributors and bug reports), and since this issue isn't an urgent one, i rather just provide guidance and hope that you can take it to completion. so i'll push a commit now into your pr with my initial test suite changes. i think it should be fairly easy to continue in that path and: 1. find the code responsible for the fds that aren't closed yet, and add the flag to these too. 2. change the test suite to error when a script is executed and it detects any open fds other than 0, 1, 2 (std streams). i.e. you can change my script to append the output of `/proc/self/fd/ into some text file, and then when the tests exit, check that there are no surprises there. (tcl is not so fun, but it's not that complicated to guess your way though it and write these few lines).",-1,-1,-1,0.9874594211578368,0.973952054977417,0.9668077230453492,-1.0,accept,unanimous_agreement
760261204,8242,"just to be clear, please run `./runtest-sentinel` on the branch with my modifications, and you'll see it prints a few open file descriptors other than 0,1,2.",0,0,0,0.986316978931427,0.9906663298606871,0.9928883910179138,0.0,accept,unanimous_agreement
760299608,8242,is it supposed to print all leaked fds after the cleanup? i didn't spot any fds in the end of the output: ![a link] did i miss anything?,-1,0,-1,0.9813354015350342,0.9742063283920288,0.9912051558494568,-1.0,accept,majority_agreement
760304938,8242,"never mind, i added some commands before listing leaked fds and i can see it now, working on those leaked fds.",0,0,0,0.9734954237937928,0.9260966777801514,0.9754042029380798,0.0,accept,unanimous_agreement
760319000,8242,"yeah.. each time this script runs, it should not find any fds other than 0,1,2.",0,0,0,0.8489329814910889,0.9397589564323424,0.966263711452484,0.0,accept,unanimous_agreement
760432033,8242,"i've eliminated the opened pipes in sentinel successfully, but there are still opened sockets residing, even though i've add some code attempting to close those sockets, it doesn't seem to be working, i am at my wit's end now. since is an expert on redis sentinel, could you come rescue me here? thanks! before: ![a link] after: ![a link]",1,1,-1,0.9231091141700744,0.9801183938980104,0.9759304523468018,1.0,accept,majority_agreement
761522846,8242,"after a deep dive into the test scripts, it turns out that those remaining tcp sockets might be created and held by `tclsh`: [code block] [code block] these sockets are connected between `tclsh` and `redis-sentinel`: ![a link] besides, i've tracked down the sentinel process searching every `socket` system call with tcp before `execve`, all tcp sockets are set with `fd_cloexec` flag after being created: [code block] therefore i reckon that the result from the current test scripts could be false positive? how do you guys think of this?",0,0,0,0.779018759727478,0.9465512633323668,0.965495467185974,0.0,accept,unanimous_agreement
761530502,8242,"unless i'm missing something, the tclsh can't open sockets to sentinel without sentinel cooperation. aren't these simply the client connections, accepted by `anetgenericaccept`?",0,0,0,0.9805595874786376,0.9814401865005492,0.9930965304374696,0.0,accept,unanimous_agreement
761532282,8242,"you are right, these are indeed the client connections which should also be set with `fd_cloexec` flag. i had mistakenly thought they were internal fds created by the sentinel, then i guess all leaked fds of sentinel are handled properly now.",0,0,0,0.9772266149520874,0.9768726825714112,0.9767063856124878,0.0,accept,unanimous_agreement
761546941,8242,"all fd leaks in sentinel are now fixed and the test scripts for detecting sentinel fd leaks have been added into the redis test suite, please take a look.",0,0,0,0.9850672483444214,0.9871330261230468,0.8906749486923218,0.0,accept,unanimous_agreement
761828249,8242,ok. all seems good to me. let's wait for review and then we can merge it. thank you for bearing with me..,1,1,1,0.9887675642967224,0.9927048087120056,0.9866601228713988,1.0,accept,unanimous_agreement
761828911,8242,"don't say that, it's all for keeping the redis code refined.\(^o^)/",-1,0,-1,0.598239004611969,0.5960264205932617,0.9461467266082764,-1.0,accept,majority_agreement
762792456,8242,any progress on this pr?,0,0,0,0.9793014526367188,0.988725244998932,0.9906713962554932,0.0,accept,unanimous_agreement
762863697,8242,"i'll ping again, and set a reminder to merge this tomorrow either way.",0,0,0,0.9853600859642028,0.9797057509422302,0.9921403527259828,0.0,accept,unanimous_agreement
762874513,8242,"okay, thanks~",1,1,1,0.8420248627662659,0.9680668711662292,0.7924838662147522,1.0,accept,unanimous_agreement
762877566,8242,"sorry , got overwhelmed by the github notification emails and did not see your guys ping, i will review asap :(",-1,-1,-1,0.9920554757118224,0.9938593506813048,0.9963405728340148,-1.0,accept,unanimous_agreement
762928883,8242,"hi , the code looks good to me, thank you very much for your time and good work... one comments regarding the tests, i saw the way you are using for check fd leaks is for linux only, if you running in sentinel tests on mac or other os you may see a lot of warnings `ls: /proc/self/fd: no such file or directory` maybe you can consider to add a if else check in the notify.sh using uname command to avoid running the fds leak check in other os environment. thanks. also pinging in case if i miss anything here . thanks",1,1,1,0.9900304079055786,0.9958024621009828,0.9955301880836488,1.0,accept,unanimous_agreement
762949630,8242,"ok, working on it, will update the shell a few minutes later.",0,0,0,0.9830664396286012,0.977952003479004,0.9840011596679688,0.0,accept,unanimous_agreement
762962877,8242,"now the shell will play to the score on different os's, please take a look, thank~",1,1,1,0.8799480199813843,0.9691058397293092,0.8963549733161926,1.0,accept,unanimous_agreement
762984592,8242,"![a link] no idea why this is happening, the installation for tcl8.5 worked well before, any clues?",0,0,0,0.5201337933540344,0.9008519053459167,0.6994137763977051,0.0,accept,unanimous_agreement
762990521,8242,"hi , i don't think this is related to your changes, maybe the ci environment is broken for some reason, another pr also fails in the same case [a link]",0,0,0,0.9541837573051452,0.9245920181274414,0.9742810130119324,0.0,accept,unanimous_agreement
762992548,8242,"okay, then could you please comment on the latest commit?:)",1,0,0,0.9861628413200378,0.8746011257171631,0.9812115430831908,0.0,accept,majority_agreement
763136727,8242,"thanks , finally merged.",1,1,1,0.7522681355476379,0.8898242115974426,0.9396255612373352,1.0,accept,unanimous_agreement
764494216,8242,"this test caused many failures in the deily ci [a link] maybe you have time to look into it? i tried running it locally and it seems to normally pass, so i'm not sure what's the difference. when i run it locally with tls, i got these (which are probably easy to solve): [code block]",0,0,0,0.9216132760047911,0.9466959238052368,0.9255849719047546,0.0,accept,unanimous_agreement
764503423,8242,so there are still fd leaks under the tls?,0,0,0,0.9838513731956482,0.9900304079055786,0.9899303913116456,0.0,accept,unanimous_agreement
764517125,8242,"yes, there's a problem under tls (easily reproducible). but according to the daily ci ([a link] there are also problems with non-tls runs.",0,0,0,0.9703142046928406,0.9700627326965332,0.9888004064559937,0.0,accept,unanimous_agreement
764522505,8242,is there any difference between the environment of common ci and daily ci? i've seen all common ci's after this pr pass while daily cis fail.,0,0,0,0.9298927187919616,0.9737836718559264,0.9808276295661926,0.0,accept,unanimous_agreement
764527682,8242,"looks like the difference is that the normal ci doesn't run the sentinel tests at all (i suppose it was meant to be quick and shallow, and sentinel isn't changed frequently) [a link] [a link]",0,0,0,0.981333553791046,0.98920738697052,0.9879090189933776,0.0,accept,unanimous_agreement
764533653,8242,"well, everything is fine on my x86_64 linux server, odd... also, it seems that some unexpected fds pop up in the child process, like `lr-x------ 1 runner docker 64 jan 21 01:28 45 -> /dev/urandom` , which doesn't look like the inheritance from the parent process.",-1,0,-1,0.7172617316246033,0.5899358987808228,0.8021774888038635,-1.0,accept,majority_agreement
764536672,8242,any chance that the root cause is because the test suite runs in docker?,0,0,0,0.9861998558044434,0.9834113717079164,0.988091230392456,0.0,accept,unanimous_agreement
764537471,8242,"i doubt it, but can you look into it? the tls issue i posted above was without docker.",0,0,0,0.9773353338241576,0.7694070339202881,0.9548935294151306,0.0,accept,unanimous_agreement
764538579,8242,"ok, i will try to figure out why this is happening in the next few days.",0,0,0,0.9601977467536926,0.9783084988594056,0.9701541662216188,0.0,accept,unanimous_agreement
764548840,8242,"[code block] seems like there are one or more sockets that are not listening nor connecting in the child process under tls, any clue what is it for? where is it created?",0,0,0,0.8969613909721375,0.978861689567566,0.9886705875396729,0.0,accept,unanimous_agreement
764551742,8242,can you offer some advise?,0,0,0,0.9818800687789916,0.9878973960876464,0.9894299507141112,0.0,accept,unanimous_agreement
764562665,8242,do you have any idea what's `/usr/lib64/libonion_security.so.1.0.19`? if it gets preloaded or otherwise injected into the process it could also be responsible for those additional fds.,0,0,0,0.987250566482544,0.9949021339416504,0.994263470172882,0.0,accept,unanimous_agreement
764591298,8242,this file will be opened even the process is not under tls: ![a link] see [a link] so it may not be the root cause.,-1,0,0,0.9332408308982848,0.9786818623542786,0.9783345460891724,0.0,accept,majority_agreement
764668170,8242,"looking into the tls part, i think some cleanups are missing.",0,0,0,0.9701901078224182,0.9722360372543336,0.9843977093696594,0.0,accept,unanimous_agreement
764724910,8242,"pushed a fix, seems like a small race condition. this solves the leaked socket, but it seems proper openssl cleanup is also needed to prevent other potential leaks (e.g. `/dev/urandom`). need to take a closer look there because (as always) every version of openssl is a bit different.",0,0,0,0.9474400877952576,0.980628490447998,0.9779850244522096,0.0,accept,unanimous_agreement
764729194,8242,well done!,1,1,1,0.9546852111816406,0.9444600343704224,0.9604062438011168,1.0,accept,unanimous_agreement
764731072,8242,could you also take a look at #8376 ? it will help with debugging this issue.,0,0,0,0.9845669865608216,0.9795274138450624,0.9932807087898254,0.0,accept,unanimous_agreement
764868536,8242,"you can experiment and investigate this without merging that pr. you can copy parts of daily.yml into ci.yml or modify the trigger (`on`) and `if` (repository) of the daily.yml. then if you push it to a branch in your github repo, it'll run there (assuming actions are enabled in your repo). or if you push a pr into our repo, you'll see these tests in the pr (and we'll revert the yaml changes before merging)",0,0,0,0.98223876953125,0.9915340542793274,0.9912023544311525,0.0,accept,unanimous_agreement
765157265,8242,"that pr is not only for this debugging, also for the future test, the current output file only shows the number and type of an fd, but with `lsof`, we can get more useful details, for instance, we can learn that the leaked sockets are not established, see [a link] if a failure of this test occurs in the future, we can get details directly from the output of the github actions instead of doing some extra effort in the test suite and run the test suite again for more details.",0,0,0,0.9773480892181396,0.9904735684394836,0.991813063621521,0.0,accept,unanimous_agreement
765870805,8242,"i've opened a new pr for the leftover of fd leaks, #8383, the `/dev/urandom` and pipe leaks are resolved now.",0,0,0,0.9864576458930968,0.9931269884109496,0.9890211224555968,0.0,accept,unanimous_agreement
936083406,9601,"wow, i'm always impressed by what compilers do (in this case both collapsing lots of reads and shifts into one, and also deleting a libc function call and replacing it with assembly). also, that's a great demonstration of how much the ub is in many cases bs, in all 3 cases the compiler still generates code that's still doing unaligned access. :smile: of course, had this code is used on an architecture that can't handle unaligned access, the compiler generate different results: [a link]",1,1,1,0.988881766796112,0.9944251775741576,0.9954028129577636,1.0,accept,unanimous_agreement
937659048,9601,"just here my understanding overall i'm not a compiler dev by any means but i think undefined behavior something like ""almost"" impossible to reason about. anything can happen. at some point, questions like if integer shift really causes an issue or if overflow would be fine in a specific case loses its meaning because it may not cause a problem with the current compiler version but next version. (or on different architecture). it may work for funca() calls generally but for one of funca() calls, compiler decides to inline the function and then decides to apply some optimization which takes advantage of undefined behavior. so, not easy to answer if any of these issues are actual problems :) more reasons to hate ubs: a jaw dropping [a link] good news, i believe almost all of the reasonable size c projects (if not all) have undefined behaviors (what a good news :) ). redis has more than these sanitizer findings probably (a lot of things are ub if you follow the standard, makes you depressed each time you learn another thing is also ub). so, no need to be a paranoid and fix each issue. question is whether we want help from sanitizers or not. if yes, we can follow a pragmatic approach, we can fix some of the issues, suppress ones we don't like.",0,0,-1,0.9224337339401244,0.5153868794441223,0.5658671259880066,0.0,accept,majority_agreement
937705880,9601,"wow, i'm not a compiler dev either, but imho that optimization shouldn't exist. (instead if c++ devs want speed, they should switch to c!). anyway, i do agree we wanna avoid using undefined behavior, and that we want to use the sanitizer to help catch them. i just think that if we can make sure to define the behavior, or only use it when we know it's defined (and fall back to less efficient implementation otherwise), that's better in some of these cases (the ones which [a link] the code or make it less [a link]. **but i don't want to rely on specific flags that exist only in some compilers, and now i'm also paranoid about the fact that a compiler will decide to erase all the files on my disk [see below]. so at this point i'm not sure what to do, it seems my two desires collide.** [below] i.e. because if we do something that's undefined, then erasing all the files on the disk is a possible outcome. (if on some theoretical platform that would be the behavior, why not apply it for all). i know that's not the reason in the above link, but who knows what other interpretations of undefined behaviors exist...",0,1,-1,0.6243544220924377,0.7024919390678406,0.962754249572754,,review,no_majority_disagreement
948077442,9601,"i tried to minimize the changes, please take another look at the pr. also, realized that latest clang gives more warnings, so i fixed them as well and added clang build to the daily. let me know if you're okay with that. (we can use gcc and/or clang with ubsan and asan). i suppressed unaligned load issues. to be honest, i don't think these would cause any performance issues in any environment but it's impossible to benchmark it anyway. so, suppressed them in a few place.",0,0,0,0.8547112345695496,0.5950025916099548,0.9281497597694396,0.0,accept,unanimous_agreement
964176873,9601,i see all comments are addressed.. so are we good to merge this? triggered a full ci to see there are no complaints by some esoteric platform: [a link],0,0,0,0.9382495880126952,0.8992084860801697,0.9822922945022584,0.0,accept,unanimous_agreement
964193623,9601,"i was waiting daily to be green to rebase and run full ci for this branch. you triggered that action for ""unstable"" right? anyway, i also triggered ci for these pr + rebase here : [a link] edit: ok i see, you used github action from unstable. sorry, i was confused for a moment. it's better to use github action from this branch so it runs sanitizer builds as well. ci build i triggered does that. let's wait and see if any new issue is introduced after rebase. i'll ping you when it ends.",-1,-1,-1,0.8054453134536743,0.9798683524131776,0.9757223725318908,-1.0,accept,unanimous_agreement
964204721,9601,"ohh, right. the action i triggered is silly: 1. it uses the old yaml files. 2. the branch wasn't rebased lately and unstable was a mess so it may contain bugs that are now fixed.",-1,-1,-1,0.9458807706832886,0.9785454273223876,0.9652058482170104,-1.0,accept,unanimous_agreement
966239901,9601,"rebased the pr and added two new commits, please take a look",0,0,0,0.9739187955856324,0.965570032596588,0.9919204711914062,0.0,accept,unanimous_agreement
1202464358,9601,"gents, i don't remember if we discussed this, but how do we feel about changing the makefile to default to `-o3` after this set of changes?",0,0,0,0.9707076549530028,0.913120687007904,0.9909127950668336,0.0,accept,unanimous_agreement
1203762184,9601,"sure, we can do it. in my experience, pgo is a bit more risky for ub. so, i think we can just switch to `-o3` now. also, what about `-flto`? usually, it’s more important. `-o3` might cause some more inlining but i guess `-flto` will inline functions a lot. (between compilation units). so, one downside is we might see less accurate stacktraces on a crash. are we okay with that? also, `-flto` will make build a bit slower. another concern, these flags might make some scenarios slower. we need to run some benchmarks for it, hopefully we won’t see regressions.",0,0,0,0.898125410079956,0.7815536260604858,0.8352342247962952,0.0,accept,unanimous_agreement
1203781693,9601,"already has some benchmarks indicating 5% improvement (with both `-o3` and `-flto` are used). but i wonder they cover a wide range of cases, and if there are any regressions too. i'm still paranoid about bugs, but considering we're far away from the next release, i guess we can try it. i also wonder if there were previous discussions on that subject and why we stayed with o2. are you aware of anything from the past?",-1,-1,-1,0.628127932548523,0.5829232931137085,0.7291180491447449,-1.0,accept,unanimous_agreement
1204066019,9601,"i'm not aware of anything, this is probably a good timing to try this.",-1,-1,0,0.4649960696697235,0.5100687146186829,0.8362691402435303,-1.0,accept,majority_agreement
1207873724,9601,"i was off last week, catching up. we won't know without trying and i guess sanitizer will help us with a small part of the possible bugs. still, we might do something extra for it if you want. i think our best bet would be running tests with sanitizers on different architectures manually just to check if we see any failures/warnings. i'd pick these three builds: 32 bit (we can run on our local), armv8 (maybe we can ask someone with new apple laptop or we run on the cloud) and s390x (we can run on qemu and try to ignore timing related failures. if there are many timing issues, maybe we just check if there is a sanitizer warning and ignore the rest of the failures). edit: i'll try these, let's see if we get any warnings.",0,0,0,0.7631000280380249,0.8787002563476562,0.844477653503418,0.0,accept,unanimous_agreement
1345511038,9601,"in the process of making a new 6.2.x release, the ci on alpine fails due to bad bitfield overflow detection. [a link] looks like in this case, the fix here is not just to silence a warning, it actually fixes a bug. trying to decide between cherry picking this whole pr over there, ignoring the error and letting 6.2 remain as is, or just fix this specific issue with bitfield.",0,0,0,0.8748648762702942,0.8061299920082092,0.8293684124946594,0.0,accept,unanimous_agreement
1345520019,9601,i took parts of this pr to 6.2.8 see [a link] and mentioned the bitfield issue in the release notes. i'll also update the top comment here with a note about that issue. let me know if you see something wrong or have more info.,0,0,0,0.983810305595398,0.977767527103424,0.9212809801101683,0.0,accept,unanimous_agreement
2080252812,13209,"i'm stuck adding hsetf to new setex api. it becomes too complex. i just realized i got setex api impl wrong for listpack even without hsetf (lost track of return values etc.). i started to feel like we would do better without an unified api (maybe it'd better to have some other smaller abstractions). i pushed hgetf, i'll prepare and add hsetf without using setex api. there will be duplicated code for these but otherwise i cannot make progress. after that, maybe we'll find a way to reuse some code or you may have a better idea how to combine it to setex api. meanwhile, please feel free to push changes to integration branch. i won't be able to do my work quickly anyway.",-1,-1,-1,0.9725961685180664,0.813957154750824,0.8754778504371643,-1.0,accept,unanimous_agreement
2099562622,13209,"1. forget to handle the listpack_ex encoding in riowritehashiteratorcursor(), dismisshashobject(). 2. should we add listapck_ex support to `debug listpack`?",0,0,0,0.9653146862983704,0.9953712821006776,0.9911202788352966,0.0,accept,unanimous_agreement
2099591511,13209,"as we discussed, i think we can address riowritehashiteratorcursor() and dismisshashobject() when we are dealing with aof and rdb save. added support for `debug listpack`.",0,0,0,0.9842569231987,0.9909249544143676,0.9820875525474548,0.0,accept,unanimous_agreement
2100938912,13209,-kalish please approve if you don't have any additional comments. i'll merge it if daily run passes fine.,0,0,0,0.910593807697296,0.8867920637130737,0.9553371071815492,0.0,accept,unanimous_agreement
1557027287,12209,"ci failed again and seem that nothing to with this pr... but i didn't find out why, -binbin could you help me to have a look? thanks! refer [a link]",1,1,1,0.7923715114593506,0.9610462784767152,0.978162169456482,1.0,accept,unanimous_agreement
1557051233,12209,"don't worry, it looks like the failure has nothing to do with this pr. the test framework occasionally does this.",0,0,1,0.7691141366958618,0.892973005771637,0.8306928277015686,0.0,accept,majority_agreement
1560136776,12209,do we have any confirmation from benchmarks or profiling which show that this provides any performance benefits? also it looks like module code is already using something similar [a link] should we try to unify our approach?,0,0,0,0.98658549785614,0.9925599098205566,0.9938668608665466,0.0,accept,unanimous_agreement
1566665964,12209,"also it looks like module code is already using something similar [a link] should we try to unify our approach? thank you and sorry for the late reply, we indeed need a benchmark and i will test it when all advises are fixed. i'm not good at aboult module, imo the code is reusable enough, both scan command and module use the `dictscan` function, but have a different callback function.",-1,-1,-1,0.9611898064613342,0.9698096513748168,0.9844489693641664,-1.0,accept,unanimous_agreement
1569800114,12209,"hi, here is the perfomance result: 1. scan keys which have expirtion time scan | match all | scan pattern and 100% unmatched | scan pattern and 50% unmatched | scan type and 100% matched | scan type and 100% unmatched | scan type and 50% matched -- | -- | -- | -- | -- | -- | -- unstable | 24785 | 60916 | 33435 | 16693 | 26946 | 21249 this pr | 24925 | 169997 | 40288 | 22913 | 130979 | 42820 2. hscan key which use dict encoding hscan | 100% matched | hscan pattern and 100% unmatched -- | -- | -- unstable | 17949 | 31693 this pr | 18088 | 143322 3. sscan key wich use intset encoding sscan intset | 100% matched | sscan pattern and 20% matched -- | -- | -- unstable | 7529 | 10255 this pr | 7819 | 22122 4. zscan key which use listpack encoding zscan listpack | 100% matched | zscan pattern and 10% matched -- | -- | -- unstable | 18794 | 28605 this pr | 20004 | 81147",0,0,0,0.9561914205551147,0.9821960926055908,0.9823018312454224,0.0,accept,unanimous_agreement
1594691416,12209,"i just finished it according to your last review and compare the benchmark performance bettwen before and after the lastest commit. benchmark command: `scan 0 type string count 1000` scan | 100% type matched |100% type unmatched -- | -- | -- before | 2432| 7811 after| 2491| 8537 benchmark command:`sscan set1 0 count 1000`, set1 is a key with intset encoding. benchmark command:`sscan hash1 0 count 1000`, hash1 is a key with hash encoding, and every field is bigger the 44bytes(avoid use emb str object). command | sscan |hscan -- | -- | -- before | 18959| 114058 after| 19099| 121752 result analysis: 1. when type string convert to interger, if the key 100% match the type, the performance bottleneck seems to be on the reply, and the performance increase is negligible. but when all key can't match the type, that is, reply is always empty, we gain a 9% increase. 2. using the sds instread of robj, when filed is small that always used the emb string object, there is nothing different in performance. but when the robj is raw string at before, we gain a 7% increase.",0,0,0,0.9752590656280518,0.9896719455718994,0.9871093034744264,0.0,accept,unanimous_agreement
1594859895,12209,did you use a pipeline in your benchmark?,0,0,0,0.9871281385421752,0.9927157759666444,0.9935377240180968,0.0,accept,unanimous_agreement
1598058084,12209,thanks,1,0,1,0.6094269156455994,0.5400217771530151,0.8643599152565002,1.0,accept,majority_agreement
1598215073,12209,wow. scan is now so much better (or so much less inefficient). fixed all the things that have been bothering me for years... /core-team please have a look at the behavior changes mentioned in the top comment and approve if they're ok for a minor version release.,1,1,1,0.7191921472549438,0.9905983805656432,0.909191071987152,1.0,accept,unanimous_agreement
1598216730,12209,maybe you can re-do some simple benchmark and mention some quick numbers in the top comment..,0,0,0,0.9775444269180298,0.9891995191574096,0.9846156239509584,0.0,accept,unanimous_agreement
1598217679,12209,full ci: [a link],0,0,0,0.9845400452613832,0.8597871661186218,0.9959890246391296,0.0,accept,unanimous_agreement
1599032384,12209,"the top comment says which is my edit, but looking at the code again, i see that it might not be correct. before we could expire keys by either the `expireifneeded` call or the `lookupkeyreadwithflags` call. so i'm i'm not missing anything, it means that the behavior change in this pr is only for the type matching. the pattern match feature already did skip the expiration attempt before this pr. please ack that i'm right.",0,0,0,0.9301698803901672,0.9208282232284546,0.5712876915931702,0.0,accept,unanimous_agreement
1599233953,12209,"while on that subject of breaking changes, i discussed this pr with the core-team earlier today and we concluded that we'd like to avoid the other behavior changes in 7.2 as well (due to be released in a month or so), since we're required by semver to avoid breaking changes in minor versions. this leaves us with two options: 1. leave this pr out of 7.2 and merge it afterwards (so it lands in 8.0). 2. modify this pr (leaving some of the optimizations for a later one), so that it doesn't introduce behavior changes, and merge it now. i prefer number 2, even though it's more work, rather than have roughly a year before people can enjoy the optimizations we made. afaik this means we can keep the memcpy and malloc optimizations and also the early pattern match changes, but that we have to give up the type name filter improvements. i.e. we need to do the type matching by string and at a late stage, or we can keep doing it by integer comparison, but avoid the side effects (keep doing lookupkey, and make sure that unrecognized types don't match any key). then soon after 7.2 is released we can merge another pr to apply the changes again (bring it to the current state of the pr). wdyt?",0,0,0,0.9467790126800536,0.9879862666130066,0.964735209941864,0.0,accept,unanimous_agreement
1600024053,12209,"yes, you are right, i confused them",0,0,0,0.94038724899292,0.7711549997329712,0.9303948879241944,0.0,accept,unanimous_agreement
1600113605,12209,"i also prefer number 2, i'll roll back the code asap.",0,0,0,0.9805507659912108,0.9756703972816468,0.9924129843711852,0.0,accept,unanimous_agreement
1600123145,12209,"thanks. ideally, try to still avoid string compare, and just put the startup type check in a comment so we can revive it later. i.e. try to keep the optimizations, but add some code or temporarily disable some code, so that we keep the old behavior temporarily.",1,1,1,0.8142909407615662,0.6509339809417725,0.897404134273529,1.0,accept,unanimous_agreement
1607438212,12209,"thanks for following all my requests (both the tips to incrementally further improve scan compared to what this pr initially meant to do), and also the rollback process, tests and comments. i'm quite happy with the final result. one last comment to handle imho is [a link] i.e. next to the currently disabled `scan unknown type` test, add a temporary one that does the opposite (prove that unknown types don't filter anything). after that, please refresh the top comment, which i'll use as a squash-merge commit comment. if you can also include some raw benchmark numbers to give an idea of the impact of this pr on performance. and then i'll merge it.",1,1,1,0.982207179069519,0.9919913411140442,0.9841527342796326,1.0,accept,unanimous_agreement
1607600335,12209,"looks all set, i'm going to retest the performance impact tomorrow.",0,0,0,0.9647406339645386,0.9617326259613036,0.9854347705841064,0.0,accept,unanimous_agreement
1607802104,12209,i updated the top comment. let me know if you see any problem. i.e. the current version doesn't have any behavior changes (we moved them all to 8.0),0,0,0,0.9851722717285156,0.9884576201438904,0.988734245300293,0.0,accept,unanimous_agreement
1607889267,12209,"i checked out this pr, to do two things: 1. run coverage report, in which i found that the listdelnode after expireifneeded was uncovered (apparently all the tests for expiring keys used type filter). 2. run all the new tests against the old version, to make sure we didn't have any unnoticed behavior change.",0,0,0,0.9724971652030944,0.9877423048019408,0.9884750843048096,0.0,accept,unanimous_agreement
1607891894,12209,full ci: [a link],0,0,0,0.9845400452613832,0.8597871661186218,0.9959890246391296,0.0,accept,unanimous_agreement
1608002479,12209,"i now realize that this was already covered by expire.tcl (i only tested coverage by running scan.tcl). but that test seems to be focused on multi-exec and replication, so i suppose we better keep the one i added.",0,0,0,0.9825507998466492,0.9825830459594728,0.988375186920166,0.0,accept,unanimous_agreement
1609222698,12209,"i have updated the performance test results in the top comment, it seems that except for the type filter, the other results have not changed much",0,0,0,0.9826488494873048,0.9760332107543944,0.9885336756706238,0.0,accept,unanimous_agreement
1609305404,12209,"thanks. is the first column, the one who says ""match all"" completely unfiltered? i understand that none of these tests use volatile keys, right?",1,1,1,0.6484252214431763,0.9065968990325928,0.9484030604362488,1.0,accept,unanimous_agreement
1609398856,12209,"yes, i used the `match *`, actually it is eqaul with no any filters. yes, i used the no-volatile keys to better display the perfermance improvement :-)",1,1,1,0.9325902462005616,0.7375794649124146,0.987387716770172,1.0,accept,unanimous_agreement
1609539339,12209,"merged. thank you for your dedicated work. feel free to issue another pr to delete / uncomment lines, which i'll merge to unstable once 7.2 is out. (just so that we don't forget). feel free to re-do some benchmark to show what additional improvements it brigs.",1,1,1,0.9751288294792176,0.944948673248291,0.9886257648468018,1.0,accept,unanimous_agreement
1616746616,12209,can you make a pr? i'm afraid to forget...,-1,-1,-1,0.9377729892730712,0.9605146646499634,0.9906591176986694,-1.0,accept,unanimous_agreement
1617390410,12209,"sorry, i didn't forget about it, just had a couple of nasty accidents last week so i didn't have time, i'll to work on it this week.",-1,-1,-1,0.9856457114219666,0.9929572343826294,0.9938714504241944,-1.0,accept,unanimous_agreement
986062379,9890,"""sort out mess"" :+1: :grin:",1,-1,1,0.9367748498916626,0.9903665781021118,0.994263470172882,1.0,accept,majority_agreement
986252280,9890,"thanks.. few requests while i review: 1. please start off the top comment to describe what was that mess and what complications it caused. i.e. some parts using ""also"" late propagation, others using an immediate one, causing edge cases, ugly / tacky code, and tendency for bugs?. 2. please look into the sanitizer timeout failure in `latency of expire events are correctly collected`, i don't recall seeing it before.",1,1,1,0.8918575644493103,0.7478398084640503,0.8512703776359558,1.0,accept,unanimous_agreement
986271621,9890,"i can't figure it out, i ran it locally (build with sanitizer=address and ran the tests) and it passed",0,0,0,0.9807220697402954,0.9906663298606871,0.9714801907539368,0.0,accept,unanimous_agreement
986279798,9890,"i've re triggered the tests and got the same outcome. maybe it has something to do with a timing issue issue causing a hung, or maybe related to the gcc version being used?",0,0,0,0.9829697608947754,0.973459005355835,0.9846262335777284,0.0,accept,unanimous_agreement
989667097,9890,i'll be happy if you can have a look and see if you can spot any issues with the new approach.,1,1,1,0.8962761759757996,0.5067092180252075,0.6350398063659668,1.0,accept,unanimous_agreement
989682422,9890,"ok, i'll check in a day or two, too busy this week...",0,-1,0,0.8443387150764465,0.5287265181541443,0.8003649115562439,0.0,accept,majority_agreement
997348251,9890,please review: [a link],0,0,0,0.9703348875045776,0.9662150144577026,0.995048463344574,0.0,accept,unanimous_agreement
997802768,9890,"/core-team this is not technically a major decision in the sense that it doesn't change any interfaces, but still it's a sensitive subject that you might wanna review, or at least read the top comment.",0,0,0,0.9337161779403688,0.937188982963562,0.7981894016265869,0.0,accept,unanimous_agreement
999594189,9890,test failures are unrelated to this pr (a recent regression in unstable). merging..,0,0,0,0.9060249924659728,0.7941656112670898,0.9847643971443176,0.0,accept,unanimous_agreement
999603032,9890,full ci: [a link],0,0,0,0.9845400452613832,0.8597871661186218,0.9959890246391296,0.0,accept,unanimous_agreement
1000690581,9890,"i don't like the way you call `preventcommandpropagation()` in `updatemaxmemory()`, the code is not easy to understand. and you didn't fix the bug thoroughly. actually it's not only the eviction's bug, it's very complex involve module system and redis-core propagation mechanism and notification system. you give an example that the `propagate-test` module subscribes all keyspace events, and the `notify_callback` does a sync `rm_call` with `!` that lead to `server.dirty` increasement. yes, it may let `config set maxmemory` be propagated, but to make matters worse, the wrong usage of module notification callback may lead to more bugs, for instance: a read command like `get` touch a stale key and triggers the lazy expire deletion, and a module's `notify_callback` call a write command and let `server.dirty` grows. after all in `call()` the read command `get` will be propagated. i believe you would not call `preventcommandpropagation()` in all read commands to fix it, i think we need refactor the propagation mechanism, the first idea comes to my mind is give the native redis commands a context to record what happened during the execution and recored what is cased by itself and what is cased by others(like modules). it's a big work we need design well at first.",-1,-1,-1,0.9493680000305176,0.903795599937439,0.8632845282554626,-1.0,accept,unanimous_agreement
1000915537,9890,"i'm sorry, i now realize you said you wanna review this (it was quite a while ago).. i'm not sure i understand all that you wrote (maybe it's the wine), so first let me ask two general questions: do you agree with the general approach (of always queuing propagation ""later"", and not mixing it with immediate propagation), and just have concerns about some specific details? (which we can improve in a followup pr). did you spot any regression or bug introduced here? or just that it didn't completely solve the problems? i.e. i'm trying to understand if your suggestion at the end is one that builds on top of what's done here, or completely replace it, and if you think it can me planned for some future day (we're running out of time for 7)? or if you think we must change / improve things asap.",-1,-1,-1,0.9878256916999816,0.9855985045433044,0.9780874252319336,-1.0,accept,unanimous_agreement
1000921154,9890,"yes, i don't like it either. afaic we can revert the preventpropagation change until we find a better solution for the damage potential caused by notifications+module. but, in general, are you ok with the approach of this pr?",-1,-1,-1,0.8939558267593384,0.9532269835472108,0.8635630011558533,-1.0,accept,unanimous_agreement
1001553732,9890,"i agree we should sort out the mess around propagation, but i have to say the propagation is so complex... and i think we should clarify the current mechanism (before and after this pr) first, here i try to arrange the background and the relation between propagation and other systems: # introduction redis propagation mechanism has two categories in general, one is about single command and another is about some special features like transaction and modules etc. ## single command redis propagation is so complex, the single command is the simplest part, but it still has three different ways: 1. the general way in `call()` function, check if `server.dirty` increases after command's `proc()` executed. if so redis will propagate the command (to aof or replicas). 2. some commands like `expire` `incrbyfloat` rewrite themselves by using `rewriteclientcommandvector()` `replaceclientcommandvector()` and `rewriteclientcommandargument()` to reach deterministic propagate. 3. some commands like `spop key count` use `alsopropagate()` and `preventcommandpropagation()` to reach deterministic propagate (and wrap with `multi/exec` if the array has more than one command, see #6615). p.s. i think we can eliminate way 2, just keep way 3 is enough. ## special features i have to say because of these features, redis propagation became very complex. 1. transaction * before 6.2, transaction emits `multi` by itself before the first write command in `execcommand()`, and does `server.dirty++` to let function `call()` propagate `exec` (which i think it's not easy to understand). * after 6.2 with #8097 and #8216 , function `propagate()` handles the `multi` emitting (by checking `server.in_exec` and `server.propagate_in_transaction`), but `exec` propagation still depends on `server.dirty++` in `execcommand()` (it's still hard to understand imho). * now this pr introduce a new approach simplify `multi/exec` by using `server.in_nested_call`, i like this approach (but the new mechanism introduces some new edge cases and bugs...). 2. lua * ~~the script verbatim replication has already removed in #9812.~~ * similar to transaction, before this pr lua emits `multi` (in `luaredisgenericcommand()`) and `exec` (in `evalcommand()`) both by itself. * note: lua in transaction would not lead to nested `multi/exec`. * this pr also eliminates the complexity about lua. 3. modules * similar to transaction and lua, `rm_call()` with `!` emits `multi`, and `modulefreecontext()` append `exec` (complex and hard to understand). * this pr eliminates the complexity. * another api `rm_replicateverbatim()` allow modules propagate the original module command, it calls both `alsopropagate()` and `server.dirty++`, **but `server.dirty` doesn't work in `call()` on module command**. because module cannot modify `server.dirty` directly (it's hard to understand). * **some other callbacks can call `rm_call()` and lead to bugs, like `notify_callback` this pr described (module system is too flexible)**. 4. expire * before this pr, expire deletion would call `propagate()` directly, and doesn't need to check if it's in transaction/lua or not. * after this pr, expire deletion use `alsopropagate()` mechanism * **to make lazy expire deletion in transaction be wrapped in `multi/exec`, or the deletion would be propagated before `multi/exec` and lead to bug**. * **to make active expire deletion be *not* wrapped in `multi/exec`, set `server.propagate_no_multi` to 1 in `activeexpirecycle()`** 5. eviction * same as expire. 6. block * before this pr, the block system calls `propagate()` directly. * after this pr, the block system uses `alsopropagate()` and needs call `aftercommand()` to execute propagate to avoid `multi/exec` wrapping. # other edge cases in addition to the two general categories, there are also some other edge cases, especially about transaction, maybe modules. ## before this pr in transaction: 1. use `bgsave` in transaction, like `multi; set foo bar; bgsave; set foo bar2; exec` * after `bgsave` done, key `foo` in `rdb` file is the old value `bar`, which break transaction's atomicity imho. 2. use `bgrewriteaof` in transaction, like `multi; set foo bar; bgrewriteaof; set foo bar2; exec` * after `bgrewriteaof` done, the base aof file contains the transaction's first half `set foo bar` without `multi` (because the child process dump all data to disk), and the inc aof file contains the transaction's second half `set foo bar2` without `exec`, **we lose the `multi`**. 3. use `config set appendonly yes` can cause the same problem. i don't think call **admin command** in transaction is a good choice, maybe we can disable them in transaction, but it's a breaking change, not sure. ## after this pr in transaction: 1. the `bgsave` problem remains. 2. the `bgrewriteaof` problem is more serious, `multi; lpush list a b c; bgrewriteaof; lpush list d e f; exec` * the base aof contains single `set foo bar`, but the inc aof contains the whole transaction (since we use `alsopropagate()`) and the first part `lpush list a b c` would be executed twice when loading, which lead to data inconsistency. 3. use `config set appendonly yes` has the same problem. ## modules 1. use `rm_call(write command)` with `!` in `notify_callback` may let some commands be propagated unexpected, * `config set maxmemory` this pr described and fixed, but not easy to understand. * a read command like `get` touch a stale key and triggers module's `notify_callback`. 2. **modules may have more problems, not sure, someone may add supplement.** # summary we can see that, this pr sorted out mess around **single command** scenario, but in **special features** and **edge cases** scenario, it introduced some new messes... and maybe more complex, like how and when to use `server.propagate_no_multi` and call `aftercommand()` is hard to handle. and i'm worried maybe only and me understand the new propagation (and i'm afraid even us may forget and write some bugs :downcast_face_with_sweat: ), that's not what we want, i hope the propagation mechanism can be easy to understand and develop base on it, because a lot of systems needs propagate, and we hope more developers can join in.",0,0,0,0.8073915839195251,0.8256443738937378,0.8941698670387268,0.0,accept,unanimous_agreement
1001643934,9890,"thanks for the thorough summary! i agree we should always use alsopropagate() and preventcommandpropagation() to reach deterministic propagate and eliminate all ""command vector"" functions we should eliminate all `server.dirty++` in module.c, they are useless just a side note, `aftercommand` was already there before my pr. the first version used to call `propagagtependingcommands` just once, at the end of `handleclientsblockedonkeys` (with server.propagate_no_multi=1) snapshot-creating commands in transactions: we have [a link] but it doesn't handle the `config set appendonly yes` case modules: [a link]",1,1,1,0.976815700531006,0.990464448928833,0.9909778833389282,1.0,accept,unanimous_agreement
1001664550,9890,"thanks for the summary of the background and history. with regards to your ending statement (the **summary** section), i don't think we can make the propagation mechanism as simple as we would have hoped, and it'll always require some understanding of the details and corner cases, but imho the question is it didn't become less dangerous with this pr. i.e. the previous code may have looked simple in some places, but it had unexpected bugs due to two mechanisms working separately, and i think in the new mechanism it's safer. with regards to the bugs in the **edge cases** section, taking a snapshot (fork or foreground save) inside a transaction was a bug before this pr. this pr changed the way that bug was manifested, and the solution should be just to block them: #10015 i don't see any other way around that, and i don't see a problem blocking them. with regards to the problems in the **edge cases** section about modules, this was also a bug before this pr. this pr attempted to solve some of them (and did), but you pointed out the solution is not complete and is ugly. i can live with fixing part of the problem without solving it completely, but i can also live with reverting the (ugly) fix since it's not complete and leaving it for some future day: #10013. if you have a suggestion of how to completely fix things in a clean manner, i'll be glad to hear. but since these are modules that do awkward things, i think we can just document that they shouldn't do that. with regards to the points in the **special features** section, i'm not sure what's the problem. i see bullets 1 to 6. in some of them you stated that this pr changes things for the best. in others you stated that the solution seems complicated to you (in which case i return to my opening paragraph, which says that it's safer). i see two bullets are highlighted: **3 - modules** - isn't this the same one that's discussed in **edge cases* which i responded to right above this paragraph? **4 - expiry** (and **eviction**) - i'll try to respond below. i'm not sure i understand this text. are you claiming that this pr introduced a bug? can you describe the scenario in more detail? (i assume / hope it's something that can be easily resolved). again, i'm not sure i understand. are you describing a bug, or complaining that you don't like the solution? (in which case i'll argue again, that it's safer) all in all your text is quite long and detailed, but i think in this case i'd appreciate less background (which some of us are aware of), and more details that are specific about: 1. did this pr introduce a regression 2. is there something missing that can be solved with additional work. 3. if you think this pr is completely wrong and / or have a better suggestion. i really wish you would have joined this discussion sooner, i know you were busy, but this pr was outstanding for quite a while, begging for attention and eventually we decide to move on. so now we need to decide how to proceed, if we can build on top of what we have now? if we have a different plan which we can try to implement before the release? or if we revert it to go back to the old mechanism? (if the new one is more problematic, or if we're afraid of unexpected issues). bottom line, you might be right that there is more to understand in the new approach, but i think it's safer than the old approach, which seemed obvious but had many side effects.",1,1,1,0.902537167072296,0.9212960600852966,0.9581564664840698,1.0,accept,unanimous_agreement
1001852489,9890,"yes, that's the problem, actually i like the approach this pr proposed, but as you said it didn't become less dangerous, i think we can try to optimize some scenarios based on this pr to make it simple and clear, ping . it's about bullets 4 to 6 (expiry eviction and block), `server.propagate_no_multi` is too implicit (i don't think it's a good idea that we break the `alsopropagate()` mechanism, think about that this pr try to sort out mess around `multi/exec`, but make expiry/eviction/block become more complex), maybe just use `propagatenow()` here is clearer. module is too complex, i'm still thinking of it. my bad, this pr is very big, i was trying to point all problems, maybe next time i can leave my questions first before look deep into it.",0,0,0,0.5300366282463074,0.6194234490394592,0.8764688372612,0.0,accept,unanimous_agreement
1001935180,9890,"i'm sorry, that was a typo. i meant to say: i.e. i think it did become less dangerous. i don't think it really complicated these that much. and i certainly don't think we shifted the complication from one place to the other. but anyway i'm ready to hear suggestions for further improvement to make it safer and / or clearer.",-1,-1,-1,0.9862165451049804,0.9870132803916932,0.9514002203941344,-1.0,accept,unanimous_agreement
1002020920,9890,"another side effect about expiry is, when a write command delete stale key, the expire deletion and the write command would be wrapped in `multi/exec`, this doesn't lead to data inconsistency but could make aof and replication stream too fat, it's easy to reproduce: [code block] and then aof contains `multi; del a; set a xx; exec`",0,0,0,0.9509374499320984,0.9858353734016418,0.9870052337646484,0.0,accept,unanimous_agreement
1002038992,9890,"that's intended, when changes happen atomically and in a certain order, they should be propagated like so. if you have had two write commands (the second one does the expiration), it would be wrong to emit the del out of order. i.e. it's not a side effect. that was on purpose.",0,0,0,0.9746004343032836,0.9875401258468628,0.9864197373390198,0.0,accept,unanimous_agreement
1002042204,9890,"i don't think it's right and necessary, the expire deletion doesn't have relation with the write command, it's just because of the lazy-expire mechanism, and they are not atomic operation (aof recovery and replica doesn't depend on the atomicity), what we should guarantee is only propagate the expire deletion before the write command.",0,0,0,0.9551251530647278,0.9816898107528688,0.9718872904777528,0.0,accept,unanimous_agreement
1002113229,9890,"ohh, i think i misunderstood you (or got you and then lost you again). i think in my last argument i was referring to expiration that happens inside a transaction, and you were referring to a plain command that's executed (no transaction), and gets wrapped in transaction. i think we could add some special case handling that will relax this in some cases (i.e. excluding exec, eval, and maybe modules). it'll be a bit ugly, but i could argue that if the other complicated paths are handled correctly by the generic mechsnism, and we just relex a specific case, then it's safer than doing it the other way around.",-1,-1,-1,0.6967269778251648,0.6537392735481262,0.5681687593460083,-1.0,accept,unanimous_agreement
1038460166,10293,"i think the slot ranges should be represented as two integers (as suggested in the issue) rather than a string on the form ""start-end"". this way we use resp for all structure and no extra parsing of a string is needed.",0,0,0,0.9877712726593018,0.991862177848816,0.9875251054763794,0.0,accept,unanimous_agreement
1038468750,10293,"actually madelyn and i discussed this offline and thought with an extremely defragmented slot distribution, this would be cheaper for network out and has minimal parsing for the client.",0,0,0,0.9326254725456238,0.9777117967605592,0.986569046974182,0.0,accept,unanimous_agreement
1038483200,10293,"i think has a good point. dealing with exceptions always adds complexity. based on my quick back-of-the-envelope calculation, i think we could achieve an overall similar footprint (if not better) using the original proposal with one trick, which is to encode the **length** of the slot range as opposed to the end slot number. here is an example 1. encode a slot range of [1000, 1000] using the special string with ""-"" $9\r\n1000-1000\r\n the total length is 15 bytes. 2. encode the same range using the original proposal but with the range length *2\r\n:1000\r\n:1\r\n the total length is also 15 bytes. note that a 4-digit slot number is where the two encoding schemes result in the same footprint. when the slot number is greater than 9999, the second encoding scheme yields a smaller footprint while, when the slot number is smaller than 1000, the first encoding scheme yields a smaller footprint. therefore, in the most extreme case when no two slots are next to each other in the same shard, the second encoding scheme would actually yield an overall smaller footprint while remaining friendly to generic resp parsing.",1,0,1,0.8258170485496521,0.8710497617721558,0.9465222358703612,1.0,accept,majority_agreement
1038504146,10293,"~~i like 's idea. it also a nice property of being easier to understand whether or not the boundary is inclusive.~~ edit: after looking at the code some more, it seems inconsistent to have different mechanisms return with different values. the intention is this a ""replacement"" of sorts for cluster nodes, which actually gives slots in the form ""a-b"" or ""a"", so i think it's actually reasonable to re-use the code that generates the nodes output.",1,1,1,0.9418929815292358,0.9913238286972046,0.963448703289032,1.0,accept,unanimous_agreement
1038707175,10293,"did you actually mean a replacement for **cluster slots**? i thought one of the issues called out in #10168 was about the cluster nodes output not being resp compliant. also, the output format implemented here is modeled after cluster slots than cluster nodes. that said, the use of ""length"" instead of the ""end slot number"" does break away from the cluster slots norm. we are probably looking at an additional ~55kb (6,4k\*4 + 9k\*3 + .9k\*2) overhead in the most fragmented case if we were to go with full resp compatibility. assuming each node taking up 300b in the output, a 100-node cluster, and 15b per slot range with the ""-"" representation, the total output size is about 300b*100 + 15b * 16k = 270kb. 55kb is about 20% increase but again this is an extreme case. so here is what i see the tough call lies 1. how likely would one run into this extreme case? it is probably on a long tail if i could guess 2. if something has to give, would it be a. breaking away from the norm established by cluster slots and using ""slot range length""; or b. using a non-resp compliant representation (the ""-""), which by the way also diverges from the cluster slots norm, though in a different way",0,0,0,0.9786993265151978,0.986052632331848,0.9898685216903688,0.0,accept,unanimous_agreement
1038828014,10293,"in case a node owns a single slot, the network output would be something like this for the dash `-` representation. `$4\r\r1000\r\n`instead of `$9\r\n1000-1000\r\n`. dash `-` comes into the picture once there are more than one contiguous slot owned by a node.",0,0,0,0.9886815547943116,0.9945606589317322,0.9930505752563475,0.0,accept,unanimous_agreement
1038926784,10293,"i think the difference of 5-10 bytes per slot range is not very important, even if the difference is 50k. the real difference is when compared to cluster slots, where the complete nodes are repeated for each slot range. assuming a maximally fragmented cluster with one master and two replicas per shard, assuming 100b per node info, cluster slots is 3\*100\*16k = 4.8m. so, even if cluster shards is 100k, we save 98% compared to cluster slots. among the options we have: | | range | single slot | | |--|-----|----|--| | 1 | `""1000-1340""` | `""1000""` | yossi's suggestion | | 2 | `1) 1000 2) 1034` | `1) 1000 2) 1000` | madelyn's original suggestion | | 3 | `1) 1000 2) 35` | `1) 1000 2) 1` | ping's suggestion: start and length | i think 1 and 2 are acceptable, but 3 is rather obscure.",0,0,0,0.9527706503868104,0.9652518630027772,0.9475240111351012,0.0,accept,unanimous_agreement
1039399284,10293,+1 agreed and 2) would be my preference.,0,0,0,0.7380823493003845,0.8673211336135864,0.9363309741020204,0.0,accept,unanimous_agreement
1039419482,10293,/ do you guys have any preference ?,0,0,0,0.9485952258110046,0.912261426448822,0.9836318492889404,0.0,accept,unanimous_agreement
1039787610,10293,"i'm fine with 1 or 2. i have a slight preference towards option 2 because it's the simplest for clients to implement. i did mean cluster nodes. anyone that didn't like the cluster slots output switched to using `cluster nodes`, so we were trying to provide a better output to get them back.",0,0,0,0.9468074440956116,0.943139910697937,0.9028403759002686,0.0,accept,unanimous_agreement
1042829830,10293,"with the new pubsub feature we decided to use ""sharded pubsub"" as the terminology to describe a master/set of replica sharing the same slot. shall we continue using the same definition here ?",0,0,0,0.9845224618911744,0.99463152885437,0.9936903715133668,0.0,accept,unanimous_agreement
1042875608,10293,"i think ""sharded"" and ""sharding"" is not a problem, my only concern is around the current ambiguity around ""shard"" and whether it refers to a single cluster node or a set of nodes with the same hash slots (master + replicas).",0,0,0,0.9711098670959472,0.9897661805152892,0.9246622323989868,0.0,accept,unanimous_agreement
1043205510,10293,"i think the term shard in this sense is very convenient. the result of this command is an array of info per shard. do you prefer topology or do have another suggestion(s)? then we can try to compare pros and cons... another possible term is ""slice"". the term ""network slicing"" is used in telecom with a similar meaning.",0,0,1,0.9278614521026612,0.5779703855514526,0.828532338142395,0.0,accept,majority_agreement
1043246865,10293,"the notion of ""shard"" was probably unofficially introduced already based on the [a link]: i feel that it is very natural to go ahead and make it official. besides, ""shard"" is a well established term in distributed systems and i don't see the use of ""shard"" in this context diverge from the norm. the other (less) popular term that i have seen is ""partition"" but that is quite foreign to redis, to my best knowledge. i have not seen ""slice"" used in distributed storage/database systems.",0,0,0,0.5934457778930664,0.9188957214355468,0.9787722826004028,0.0,accept,unanimous_agreement
1045107342,10293,"while we are here, can we also consider having the node's epoch included in the output as well? the client application can bounce between nodes at run time so there are times it'd receive conflicting ""shard"" topology. an embedded epoch value would go a long way to help the client application resolve the collisions. this is somewhat related to our discussion on your topology change pubsub proposal #10150.",0,0,0,0.9825411438941956,0.981667935848236,0.9924748539924622,0.0,accept,unanimous_agreement
1046158083,10293,"sorry for taking so long to weigh in, but i just also wanted voice that i think it's good to make the term ""shard"" more official. the only downside i can think of is it will become more complex if we ever want to introduce more flexible configurations. for example, consider the scenario if we wanted to allow masters to be replicas for other masters, so that on node failure we have the data replicated but all nodes are able to take writes.",-1,-1,-1,0.9872105121612548,0.9849040508270264,0.9713537693023682,-1.0,accept,unanimous_agreement
1046164618,10293,"just to make sure i understand the epoch case. you want to be able to call `cluster shards` on multiple different nodes, and resolve them with the epoch? it seems more like a best practice to keep calling it against one node since eventually all nodes will be consistent. i didn't read through all the conversation on pubsub subscribe slots, i'll try to read through that tomorrow.",0,0,0,0.9456887245178224,0.9831684827804564,0.9796655774116516,0.0,accept,unanimous_agreement
1048588368,10293,"my point is not that ""shard"" is an incorrect term, but that it's already used in different contexts as a synonym to ""cluster node"" (or `redis-server` process) and i'm concerned about overloading it and creating confusion.",-1,0,0,0.9458244442939758,0.7251495122909546,0.6386898159980774,0.0,accept,majority_agreement
1048597839,10293,"where is ""shard"" used as a synonym to ""cluster node"" (or `redis-server` process)? can't we changes those occurrences and clarify the terminology?",0,0,0,0.9818226099014282,0.9944555163383484,0.994038462638855,0.0,accept,unanimous_agreement
1048612230,10293,", it is in my opinion that **not** requiring the client application stick to a single node for topology updates improves the client application's resilience to failures on the redis side, such as when the redis node is network partitioned from the majority of primaries or when the redis node is overloaded by data requests. therefore, i would imagine that an application maintain connections to all redis nodes in the cluster and retrieve cluster topology from any ""good"" node (neither partitioned nor overloaded). my only concern here is that without knowing the epoch of the node associated with the slot, the app might accidentally end up working with stale routing information and worse still the (stale) owner node might've even left the cluster permanently. that being said, it is true that even with epochs it is not guaranteed that the topology received is always up to date but i'd assert that, by having the ability to accept **more** up to date topology only, we could reduce a lot the likelihood of using stale routing information.",0,0,0,0.8354474902153015,0.9821199774742126,0.9725624918937684,0.0,accept,unanimous_agreement
1049133944,10293,"i did a quick search on redis.io. i think now is a good time for a rigorous definition of shards in redis - before the documentation diverges further. ones that i think mean nodes ones that are more in line with my understanding of ""shard""",0,0,0,0.7644158601760864,0.8847719430923462,0.8988329768180847,0.0,accept,unanimous_agreement
1049559245,10293,"linking pr #10163 per 's suggestion. the idea is to preserve a dead primary's slot range. i was hoping to return that information via ""cluster nodes"" but there is a potential risk of breaking existing clients. i can port over #10163 on top of the new ""cluster shards"" command after this pr is merged.",0,0,0,0.9840419292449952,0.9925572872161864,0.9895450472831726,0.0,accept,unanimous_agreement
1051004590,10293,"i actually think all 4 of the places we mention shard are consistent with my understanding of the word. a command can be routed to any node in shard that is serving the slot, depending on the client preferences. the main reason i don't want to expose epoch is that it's primarily focused around mastership generation. all other metadata associated with the cluster is not bumped with the epoch, so nodes may disagree and have the same epoch. i'm sure we can document all of this behavior, i just want to provide something that is easy to use for clients.",0,0,0,0.9158297181129456,0.9331817626953124,0.8922333121299744,0.0,accept,unanimous_agreement
1051261038,10293,"well this is even better :) it was not a super clear cut to me so i erred on the side of caution. i would like to push back on this a bit. the epoch (aka, mastership generation) is an important property of the shard imo so if we are talking about a command that returns shard information, i think it is a correct and intuitive thing to include the epoch. to clarify, i am thinking about a change like below. the parsing is trivial and the client who doesn't care about the epoch can easily ignore it. thoughts? [code block]",1,1,1,0.9628296494483948,0.9924433827400208,0.9937387704849244,1.0,accept,unanimous_agreement
1053943167,10293,"it's worth also mentioning that a nodes perspective on another node may not necessarily align with any ""epoch"", since clients often explicitly override it with `cluster setslot`. replicas can also disagree on what is the ""epoch"".",0,0,0,0.986748218536377,0.9899159073829652,0.990949809551239,0.0,accept,unanimous_agreement
1053972930,10293,"/core-team ptal. the top comment is updated with the new command which intended to provide a better interface for clients to get topology information. it should be a strict superset of `cluster slots`, and provides similar information to `cluster nodes`.",0,0,0,0.9854546785354614,0.986295759677887,0.9727120995521544,0.0,accept,unanimous_agreement
1053991719,10293,"good callout on the consensus-less setslot. i agree this is a case where epoch can be non-deterministic. however, the conflict, which would require an automatic failover while completing slot migration, would be resolved quickly with the winner gaining a higher epoch so next `cluster shards` would quickly catch me up. in my situation, rolling upgrade is a much more common scenario than slot migration. my current solution to reduce the possibility of using stale routing information is to get the epoch from `cluster nodes` and then get the topology from `cluster slots`, which is worse than the proposal here even if they are executed against the same node (which i am not sure we can guarantee), because the epoch can in theory change between the two commands. and then there is the additional parsing logic. since redis already outputs the epoch in `cluster nodes`, the exposure of the disagreement is not a new issue to whoever needs this information.",1,0,1,0.6840725541114807,0.8798074722290039,0.8231813311576843,1.0,accept,majority_agreement
1054103513,10293,"build failed: `e: failed to fetch [a link] could not connect to azure.archive.ubuntu.com:80 (52.250.76.244), connection timed out` just restart the job? (someone who has permission to do that)",0,0,0,0.9115936756134032,0.9860659241676332,0.9860511422157288,0.0,accept,unanimous_agreement
1055136879,10293,"to summarize the next steps, we reviewed with the core group and wanted to update the following. * deprecate `cluster slots` indicating that this command could be used instead. `cluster nodes` will not be deprecated since it contains extra debugging information. * start the documention for this new command, since there are some nuances. * * even though there is no way to distinguish shards today, we want to leave it open for that. replicas should be able to replicate from masters even if they own no shards. * * note that both the shard information as well as the node information is extensible.",0,0,0,0.9579521417617798,0.9914611577987672,0.9675610065460204,0.0,accept,unanimous_agreement
1056012730,10293,"+1. what do you think about my justification for having epoch in the shard information? also even though the new output uses a dict structure for extensibility, there is a slight concern of mine that some client might just look for a partial match of ""fail"" instead of the whole word. clients like this can misinterpret ""pfail"" as ""fail"" in the future if pfail is introduced at a later time.",0,0,0,0.9303581714630128,0.9810355305671692,0.922210156917572,0.0,accept,unanimous_agreement
1058424589,10293,"this is an interesting topic. if the last slot is migrated away from a master for the purpose of shutting it down, upgrading it, etc. it doesn't work as expected. it immediately turns itself into a replica of the new slot owner and starts replicating. the assumption in the comments below are wrong. (it is not always a failover causing this.) [code block] can we change this without breaking backwards compatibility? this is the source of #9223.",0,0,1,0.8723497986793518,0.967073142528534,0.521173894405365,0.0,accept,majority_agreement
1061709582,10293,"just want to make sure i understood it correctly - client that used to use the 'cluster slots' command to map the cluster slots/nodes, didn't retrieve nodes in 'fail' status with the slots command. failed nodes are only available through the cluster nodes command (correct?). so, when moving to the cluster shards command, failed nodes should be ignored when building the cluster mapping, right?",0,0,0,0.9805840253829956,0.9928066730499268,0.9726002216339112,0.0,accept,unanimous_agreement
1062131843,10293,yes. that is my understanding too.,0,0,0,0.9439717531204224,0.9258103370666504,0.9848301410675048,0.0,accept,unanimous_agreement
1062159939,10293,"yes that is correct. it doesn't! most people are confused by this i agree. the conversation we had in the core group was just that ""logically"" the api should support it. i don't think we immediately plan on changing this though, that issue is interesting though, i'm going to read it next. we touched briefly on epoch, and no one had a strong opinion about it but me. we diverged briefly into a conversation about how we want to make config generations include all of the state information, so i think we settled on adding it in a future version once we have that. this would be independent of epoch though, so we don't want to proactively add it. i don't know we can defend against people matching against something incorrectly, they should be doing exact matches and not partial matches.",0,0,0,0.6320180296897888,0.4936423003673553,0.5175683498382568,0.0,accept,unanimous_agreement
1062545901,10293,"trying to get a better understanding of ""make config generations include all of the state information"". are you talking about adding the remaining states like `ping-sent`, `pong-recv`, `link-state`,etc along with `config-epoch` in an all or none way? or this is something else? sure. i don't have a strong opinion here and i haven't seen updates to #10195 either. agreed we can revisit this later when there is a real use case. forgot to mention that i think we'd also need my changes (#10163) to ensure that failed nodes are included after a replica takes over the primary-ship in the shard. i can port it over after this pr is merged.",0,0,0,0.8886396288871765,0.9211670756340028,0.945115089416504,0.0,accept,unanimous_agreement
1066273859,10293,"include other metadata such as which replicas are serving data from which masters, updates for announced hostnames and ip addresses, any other future config state changes.",0,0,0,0.9866393208503724,0.991209864616394,0.9949407577514648,0.0,accept,unanimous_agreement
1066314262,10293,added documentation: [a link],0,0,0,0.9851863980293274,0.9853854775428772,0.994648277759552,0.0,accept,unanimous_agreement
1067351151,10293,"thanks . i would like to clarify that there is a legit and real scenario for having epochs in `cluster shard` as it further reduces the likelihood of getting stale topology. this information is currently available via `cluster nodes`, whose replacement (#10168) is also one of the reasons why the new `cluster shard` command is introduced. with `epoch`, applications do not need to invoke both `cluster shard` and `cluster nodes` any more. happy to add the epoch along with the change to preserve failed nodes next.",1,1,1,0.9766663908958436,0.9870870113372804,0.9903171062469482,1.0,accept,unanimous_agreement
2112598554,13243,"-kalish , if it is not too much effort at this point in time, please consider to `rdbsave()` / `rdbload()` of `rdb_type_hash_metadata` tuples of type [ttl][field][value], instead of [field][value][ttl]. we will be able to optimize later on the allocation of `fields` such that from first place will be allocated with ttl support.",0,0,0,0.9813989996910096,0.9905123114585876,0.9799600839614868,0.0,accept,unanimous_agreement
2115233061,13243,full daily run: [a link],0,0,0,0.9760568141937256,0.909427046775818,0.9907498955726624,0.0,accept,unanimous_agreement
2115507512,13243,"lgtm! if there are flaky tests in the daily, we may fix it later on the way (or you may comment out for now, or merge later with another pr after resolving the issue etc). it is up to you.",1,0,1,0.55430668592453,0.793596088886261,0.5853157043457031,1.0,accept,majority_agreement
2115519294,13243,", i'm not sure the flaky tests in the daily are related to this pr, unless we're sure they weren't flaky before. is this indeed the case?",0,0,0,0.6138534545898438,0.8197107911109924,0.926112949848175,0.0,accept,unanimous_agreement
2115536598,13243,"hmm, i'm not sure. i see there is one hfe defrag and a corrupt/dump fuzzer failure. maybe you can comment on that? could you take a quick look? [a link]",0,-1,0,0.897436261177063,0.5920678377151489,0.8584134578704834,0.0,accept,majority_agreement
2115556235,13243,", , if anyone can approve this pr once done reviewing, it'll make very happy...",1,1,1,0.947920262813568,0.9839085936546326,0.9902149438858032,1.0,accept,unanimous_agreement
2132049661,13243,we need a document pr for this in [a link].,0,0,0,0.987332046031952,0.9896088242530824,0.995414972305298,0.0,accept,unanimous_agreement
2132124066,13243,"hi , can you please point to where rdb format is documented? or is there another aspect that need to be documented? thanks!",1,1,1,0.93142831325531,0.9185357093811036,0.9113858938217164,1.0,accept,unanimous_agreement
2132169954,13243,"-kalish the rdb format doesn't need to be documented, just add document for `rdb_last_load_hash_fields_expired`.",0,0,0,0.9889840483665466,0.9940823912620544,0.985900104045868,0.0,accept,unanimous_agreement
2139020488,13243,", now that has removed hash field expiration on load, this specific field is gone and there is no need for documentation.",0,0,0,0.9877277612686156,0.990216851234436,0.9916070699691772,0.0,accept,unanimous_agreement
2139034095,13243,"-kalish yes, i can't remove the needs-doc-pr tag. i created a doc pr here: [a link]",0,0,0,0.9718611836433412,0.979373574256897,0.9810264706611632,0.0,accept,unanimous_agreement
923904384,9530,"nice! if you ever get hit by a bus, it better be a cluster bus. :-) i haven't looked at the code yet. i think sni verification between nodes might be useful, just as it is useful between client and cluster, for deploying a system in an untrusted network. we use mutual authentication instead though. `cluster-prefer-hostnames` sounds good to me. if redirects use hostnames, that can already break clients, so if that's enabled, we can as well enable it for the first arg in cluster slots. but even better may be to let the client announce its capabilities (e.g. `hello 3 hostnames`). if we ever want to add more fields to cluster slots, perhaps consider making the last argument a map. it may be secondary ip addresses (ipv6 and ipv4). a hostname can be resolved to multiple ip addresses though, if dns is used, so it might not be needed for that use case.",1,1,1,0.9900749325752258,0.9956717491149902,0.9964005947113036,1.0,accept,unanimous_agreement
925833600,9530,"great to see this making progress! i didn't look at the implementation yet, but i suppose that any approach we take to support cluster bus upgrades should be flexible enough to support additional upgrades in the future. i'm sure we'll need that when we proceed with the clusterv2 plans. i support the `cluster-prefer-hostnames` all-or-nothing approach, so if we use hostnames we use them for everything. there will definitely be some client breakage but i think it's an opportunity to refresh them, and also migrate all to `cluster slots` while doing so. agree about not using dns names for intra-node connectivity, and i think sni validation is also not really that important there (adding other basic cert validation configuration is easier and just as good imho).",1,1,1,0.9880065321922302,0.9918602108955384,0.9940642714500428,1.0,accept,unanimous_agreement
925863787,9530,it would be nice if we also could use hostnames in create cluster process. right now you need to use ips for that.,0,0,0,0.8999952077865601,0.9897908568382264,0.9826839566230774,0.0,accept,unanimous_agreement
930806557,9530,"yeah, i said it's breaking but it's really not as long as you're being deliberate. the danger here is that the extension is grouped with the ping/pong messages themselves, so that failure to parse the extensions means that the entire ping will also be rejected. is there something specific you have in mind here that is useful? it is an extension, but it's meant to jump on the existing ping/pong structure that already exists to spread data around. the module interface is already extensible in that you can add new messages if you want. (we could implement hostnames that way as well) there is also no strong reason this mechanism couldn't be generalized to add arbitrary additional data to any of the other existing messages. i'll also mention that i think long term this type of gossip isn't very efficient, and we probably want to figure out a better way to distribute this information in the cluster for cluster v2. this is mostly right, but we do have atomicity because gossip data isn't that comprehensive. the gossiped information (ip, node name, flags, health information) is just enough so that nodes learning about a new node can reach out and ping it, it's not enough to know detailed information about the node. specifically slots are missing, which disqualifies it from showing up in `cluster slots`. once it has exchanged a single ping/pong message, it will then know all the information it needs to display it in cluster slots, which is where we can inject the new hostname. this is why i made a very specific point about `cluster nodes` as well as sni for intra-node communication. cluster nodes requires very deliberate parsing to understand the state, which most clients don't do very well, but the node will show up immediately without the hostname. we also can't do sni for intra-node based on the current implementation, since we reach out to the node *before* knowing it's hostname. there is no hard blocker for gossiping the hostname, just seems like extra data.",0,0,0,0.62266606092453,0.7886967062950134,0.811824381351471,0.0,accept,unanimous_agreement
932833342,9530,- also any thoughts on that idea you and i discussed to create `cluster health` command so that users would not have to parse `cluster nodes` looking for `fail`?,0,0,0,0.9829285144805908,0.9902260899543762,0.991828203201294,0.0,accept,unanimous_agreement
932860903,9530,"it's in one of the checkboxes ;) my thought was to decouple your ask from this specific pr. this is mostly code complete to my satisfaction for the core. (also, i'll be out for a couple of weeks, so won't respond quickly)",1,1,1,0.971123218536377,0.9940797090530396,0.9873673915863036,1.0,accept,unanimous_agreement
932970159,9530,"my mistake, did not notice",-1,-1,-1,0.9250460267066956,0.9219863414764404,0.978206992149353,-1.0,accept,unanimous_agreement
936231921,9530,"the only issue i had with the extensions is that it's a bit weird to have the flag and count at the `clustermsg`, but still have to deal with extensions per `clustermsgdata`, but i suppose that's really the easiest way to maintain backwards compatible ping payloads. and i agree we'll probably want to move away from the gossip as it works right now anyway.",-1,0,0,0.8954660892486572,0.8175418972969055,0.86379474401474,0.0,accept,majority_agreement
950985842,9530,"something that is related to this work and came up in a recent discussion: a primary use case for hostnames is to deal with network topologies where the cluster does not have good visibility into what addresses are exposed to clients, but assumes that a hostname will resolve to the right address on the client side. if we stretch scenario further - the hostname itself may also not be known, or be dynamic and different for different clients. in that case, it could be useful to return something like `-moved :: ` (just an example) and expect a well behaved client to reuse the same address/hostname but just a different port. there is an inherent assumption here that the client only uses ports to distinguish between cluster nodes, and that the hostname/address is identical - but i believe that is becoming the case with some network topologies that involve a service mesh proxy / load balancer / gateway / etc. there's practically no work on the server side for this, it's only about setting a convention and communicating it to clients as part of the hostname support change. any thoughts about this?",0,0,0,0.952019989490509,0.98505437374115,0.9648749232292176,0.0,accept,unanimous_agreement
951100898,9530,"- you are absolutely correct, hostname can be different per node. cluster can be composed of server1.domain.com:6379, server2.domain.com:6379 and server3.domain.com:6379.",0,0,0,0.9559528827667236,0.9903226494789124,0.9617274403572084,0.0,accept,unanimous_agreement
951375187,9530,"that is a good insight. an alternative to what you proposed is we could add a client config so that a client can tell the cluster the hostname/ip that is should always respond with. i think that would require a bit less client changes, as they would more focus on sending an additional command on startup as opposed to changing how interpreting the cluster slots/redirects function. not sure i followed your comment, is having the server side not know the hostname a better solution for what we talked about? i'm going to rebase and address my changes today in either case. we should be able to quickly add the changes outlined. once this has general buy in, i'll close off on the tooling improvement.",1,0,1,0.8338595032691956,0.6043090224266052,0.949920892715454,1.0,accept,majority_agreement
951767733,9530,"the example you provide is already part of this work, i was actually referring to something else. for example assume there are clients a and b behind different load balancers, both pointing to the same redis cluster. the clients may use different, locally known and locally resolved hostnames to reach those load balancers, but the cluster does not know where to redirect each client. [code block] this is a good point, it involves less parsing changes. on the other hand, we're anyway introducing parsing changes, not just due to hostnames but potentially also pushing clients to finally move from `cluster nodes` to `cluster slots` so we could try to get this all done together. i don't feel strongly either way though.",0,0,0,0.8656014800071716,0.9790434837341307,0.8715533018112183,0.0,accept,unanimous_agreement
952318282,9530,thank you for clarifying . i misunderstood.,-1,1,1,0.9132782816886902,0.5435821413993835,0.9615726470947266,1.0,accept,majority_agreement
965691807,9530,"regarding the unknown hostname issue, i'm ok with both options and we should probably try to get some client author feedback to assess what makes more sense. i think the important bit is to bundle this along with hostname support and maybe a general migration from `cluster nodes` to `cluster slots` because we don't get too many chances to realign clients.",0,0,0,0.9660462737083436,0.96517151594162,0.9057977795600892,0.0,accept,unanimous_agreement
976099179,9530,"finally got time to update this, also pinged the client channel.",0,0,0,0.9784729480743408,0.960056483745575,0.9851094484329224,0.0,accept,unanimous_agreement
977795087,9530,"great, so we need to agree on the open issues above: i also tend to say no, move clients away from it to `cluster slots`. i'm ok with hostname. i'm good with this implementation. the last one is port based topology, how to handle it in a way that's easy for clients and whether or not it should be part of this pr. the benefit of pushing it together is we'll have a single pr to relate to when discussing redis 7 cluster protocol changes.",1,1,1,0.8368046283721924,0.9686870574951172,0.9284026622772216,1.0,accept,unanimous_agreement
986483911,9530,"while talking with the client folks, one of them mentioned that they didn't see the value in a custom protocol since clients could just do that anyways. they could add an argument to their clients that indicates they are talking to a nat gateway, and just assume all redirects will go to the same port today. so i suppose the question is do we still think we need a client side change to support this functionality, or do we let clients implement it themselves? /core-team i suppose your approval is needed here now. please refer to the top comment. note: i will update the name of the test so it's in the right order later, there are several inflight cluster mode prs and will just wait until this is ready to merge.",0,0,0,0.928949773311615,0.9484145045280457,0.940978229045868,0.0,accept,unanimous_agreement
986503134,9530,"i don't know enough about cluster to really approve or reject it, so lgtm (didn't review the code). one question i have about upgrades, is whatever we can somehow prevent this from doing damage in case one of the nodes still doesn't support the new feature? (either block the config, or just avoid sending messages that would break something).",0,0,0,0.807590126991272,0.8361863493919373,0.952094316482544,0.0,accept,unanimous_agreement
991913376,9530,"i agree that technically clients could do that, but that assumes they're well aware of their network topology and configuration. i think in most cases we'd actually try to achieve the opposite - minimize the amount of configuration we need to feed the app down to some connection string.",0,0,0,0.9679290056228638,0.9811351895332336,0.9780223369598388,0.0,accept,unanimous_agreement
992023275,9530,"ack, let's close on this during on sync meeting, it's probably faster than gathering feedback here.",0,0,0,0.9187349677085876,0.8758925795555115,0.9652135372161864,0.0,accept,unanimous_agreement
995479178,9530,"so, for the ""unknown hostname support"", i don't think we can necessarily use nil since for the default case for cluster slot will probably put (nil) as the new last argument. i'm going to propose we use ""?"" to indicate that there is no designated host for a node. so redirects will loo like `+moved 10000 ?:1234` and we can populate cluster slots with ""?"". does that seem reasonable? if so, i'll create an issue documenting that and hopefully we can merge this pr.",0,0,0,0.9673720002174376,0.9773144125938416,0.9838926792144777,0.0,accept,unanimous_agreement
996043565,9530,"what i had in mind is `-moved 111 :1234` for non-hostname redirects, and a cluster slots that contains nil (or an empty string) for *both* ip address and hostname in that case.",0,0,0,0.9880675077438354,0.9915850162506104,0.993643879890442,0.0,accept,unanimous_agreement
996170568,9530,"i'm not the biggest fan of the null/empty string, but i also don't feel that strongly about it. i think in the end this isn't a significant distinction, so i'll go create a doc pr for this.",-1,-1,0,0.9476290941238404,0.5709800124168396,0.662941575050354,-1.0,accept,majority_agreement
997334314,9530,ptal at the attached documentation and verify we aren't missing anything else.,0,0,0,0.9845747947692872,0.9860840439796448,0.980895221233368,0.0,accept,unanimous_agreement
997463167,9530,"fwiw, in relative uris and paths, omitted parts (uri scheme, host or path) are meant to be the same as the base (current or previous) uri or path. in this spirit, an omitted host as in `:1234` could be interpreted as the current host with a different port. if `?` is returned as the host, it doesn't give this idea to the user. (i didn't follow the discussion so i don't know what applies in this case.)",0,0,0,0.9777100086212158,0.8499467968940735,0.9701080918312072,0.0,accept,unanimous_agreement
998044596,9530,"i'm not crazy about that either, but i felt it's better than any other arbitrary convention (and thanks to i now know why). regarding the documentation, i'm not 100% sure it's clear what is the correct client behavior: * do we say a client must always use the secondary endpoint if listed, or is it discretionary? * in case resolving fails, are clients allowed to fall back to the primary one? i tend to think about it in terms of ""if the secondary is listed, use it and only it"". but then perhaps the term ""secondary endpoint"" is misleading?",0,0,1,0.4141987264156341,0.7496245503425598,0.8710303902626038,0.0,accept,majority_agreement
998306865,9530,"definitely sounds like the syntax is ambiguous and not clear. how about this instead: [code block] i originally didn't like this since it's the most verbose, but it's definitely the clearest. the reason i want to provide both, is we have a specific user at aws that wants both ip and hostname. they like to directly connect to the ip but do sni on the hostname. a note about aws is that for tls clusters we always show hostnames, and for tls disabled we always show ip addresses.",0,-1,0,0.789712131023407,0.8829775452613831,0.9142634868621826,0.0,accept,majority_agreement
998586854,9530,what about multiple ip addresses (ipv4 and ipv6) and multiple host names?,0,0,0,0.9835934042930604,0.9871019721031188,0.992917537689209,0.0,accept,unanimous_agreement
998973255,9530,"i'm not sure what to do about multiple-hostnames, that seems like it would be very difficult for clients to support. i suppose they could try them all, so i would be inclined to not try to support them. ipv4 vs ipv6, do we want to support dual stack where you can talk to redis with either ipv4 or ipv6? it sounds like you could change the the announce ip (i think that supports ipv6, but i didn't check) to control which one you expose, but make either available.",0,-1,0,0.6842503547668457,0.701400101184845,0.8486732244491577,0.0,accept,majority_agreement
998975286,9530,"another thought is we could do the same thing that you did for tls. if you are requesting the command over the ipv6 address, we could return the known ipv6 address.",0,0,0,0.987945556640625,0.9933098554611206,0.986333668231964,0.0,accept,unanimous_agreement
998992282,9530,"maybe multiple hostnames are useless for redis. let's ignore it. :d regarding dual stack, your idea of serving ipv6 to ipv6 clients and ipv4 to ipv4 clients sounds like a good idea. i won't be surprised if we'll get requests sooner or later. here is something related: #6358. multiple ip addresses could also be used for multi-homing, i.e. multiple network cards installed on the same redis machine with redundant cables to different isps. (something similar, sctp with multi-homing, is used for high availability in telecom, between radio base stations and core network.) also related: using dns, a hostname can already be resolved to multiple ipv4 and ipv6 addresses.",1,1,1,0.9040178656578064,0.6791844367980957,0.9809974431991576,1.0,accept,unanimous_agreement
998996964,9530,"if we ever want to add more endpoints, i suppose they can be added after the other ones in the cluster slots array, unless we want to add something else there. we can simply keep that in mind when designing the protocol, that's all.",0,0,0,0.9831578135490416,0.988645315170288,0.9896913170814514,0.0,accept,unanimous_agreement
999604232,9530,"so, to summarize the last few comments: * multiple hostnames are not required. * multiple ips are nice to have. in most cases they'd be covered by a single hostname or auto-sensing of ipv4/ipv6 but in other cases may still be useful. i also don't like the verbosity of repeating a very-long-hostname-in-some-region twice. how about: [code block] and we can avoid repeating ourselves so if our preferred endpoint is an ip, we can choose to only include the hostname in the additional info, etc. we could just drop those additional info fields arbitrarily, but that would be counting too much on the client to parse and make sense of them, and won't be very extensible in the future. this makes `cluster slots` a bit more complex, but i think we're headed in that direction anyway if we want to optimize it for non-contiguous hash slot mapping to make it a viable alternative to `cluster nodes` in those cases.",-1,0,0,0.7103698253631592,0.5466018319129944,0.6715861558914185,0.0,accept,majority_agreement
999688466,9530,"sounds good to me. i'm also going to change that one config ""cluster-prefer-hostnames"" to ""cluster-preferred-endpoint-type"" and make it an enum, and that will control what shows up in the preferred field.",1,1,1,0.9524464011192322,0.8742344379425049,0.910192370414734,1.0,accept,unanimous_agreement
1002982006,9530,i just realized there was another extension topic to this - getting `cluster slots` to efficiently handle hash slot mapping that is not contiguous. did you have any thoughts about that?,0,0,0,0.9472059011459352,0.98825603723526,0.9868699908256532,0.0,accept,unanimous_agreement
1003812385,9530,"my plan was to introduce a new command, something like `cluster shards` (alternatives are `cluster topology` and `cluster health`), which produces the same result as this command but organizes the nodes into shards and documents the slots associated with each shard. there was an ask to include additional information as well in the shards output.",0,0,0,0.980684220790863,0.993111252784729,0.9919363260269164,0.0,accept,unanimous_agreement
1003829466,9530,doc pr: [a link] full test run: [a link],0,0,0,0.9819489121437072,0.9529303908348083,0.9915982484817504,0.0,accept,unanimous_agreement
1005297504,9530,great! :party_popper: what is the release 7.0 schedule?:eyes:,1,1,1,0.9913474917411804,0.995593011379242,0.9972180128097534,1.0,accept,unanimous_agreement
1008262060,9530,please have a look [a link] [a link] [code block],0,0,0,0.986781895160675,0.9682148694992064,0.9901130199432372,0.0,accept,unanimous_agreement
1008263216,9530,another one: [a link],0,0,0,0.9855125546455384,0.9877277612686156,0.9950900077819824,0.0,accept,unanimous_agreement
1008439148,9530,"ack, will take a look.",0,0,0,0.9832897186279296,0.6620522141456604,0.909179151058197,0.0,accept,unanimous_agreement
1008461278,9530,fix submitted: [a link],0,0,0,0.9884361028671264,0.9929124712944032,0.9913927316665648,0.0,accept,unanimous_agreement
1068707524,9530,this is on 7.0-rc2 [code block] mention my self for issue filter :eyes:,0,1,1,0.873950183391571,0.8434410095214844,0.8444095849990845,1.0,accept,majority_agreement
1068832634,9530,it seems as `cluster meet` does not accept a hostname. i guess we need to implement that.,0,0,0,0.9807778596878052,0.9503835439682008,0.988695502281189,0.0,accept,unanimous_agreement
1079782816,9530,also seeing this,0,0,0,0.9742739796638488,0.9687862992286682,0.9919413328170776,0.0,accept,unanimous_agreement
1079876073,9530,"this is resolved by #10436, right?",0,0,0,0.9854277968406676,0.9910902976989746,0.994273602962494,0.0,accept,unanimous_agreement
1080310775,9530,that's right. i created the issue #10433 to track it too.,0,0,0,0.9736945033073424,0.9774963855743408,0.9927321076393129,0.0,accept,unanimous_agreement
1236261650,9530,seen a failure in a test introduced here. i assume timing issue. [a link] [code block],0,0,0,0.9792758226394652,0.9780972599983216,0.9840809106826782,0.0,accept,unanimous_agreement
1704781331,12453,ready for review :) yes already done will do,1,1,1,0.9833207726478576,0.9938269257545472,0.9961214661598206,1.0,accept,unanimous_agreement
1763336576,12453,"regarding the async-signal-safe functions: we were using unsafe functions before this pr. `pthread_mutex_lock`, `serverlog` uses `vsnprintf` and the linux manual doesn't explictly mention 'backtrace()' though as for the latest, i found a stackoverflow thread that claims that it is safe to call it if `libgcc` is already loaded. i assume they rely on this note from [a link] explicitly, but they are part of libgcc, which gets loaded dynamically when first used. dynamic loading usually triggers a call to [a link]. if you need certain calls to these two functions to not allocate memory (in signal handlers, for example), you need to make sure libgcc is loaded beforehand. * i can avoid using malloc for the **`threads_mngr` output arrays** (one array of output pointers, and one allocated by each thread for its backtrace). allocating the backtraces array on the stack and assigning the address to a global variable will do the work. :check_mark_button: * the **semaphore** can be replaced with a busy wait. :check_mark_button: * regarding `tids` list allocation - since we know the number of threads only at runtime, we can either: ** decide on a maximum hard-coded number of threads ** write the tids to a temporary file (write() and read() and safe) - i actually prefer this option. * **opendir, readdir and closedir** : there is no other option to get the process's threads and the threads' mask during runtime. for this issue i have 2 suggestions: ** introduce `register_thread` api: allows to register the thread to a global array upon creation. a`redismodule` function should be part of this api. should be taken into consideration: the **memory** to maintain this global structure and lots of **changes** in the existing code (replace each pthread_create). it is important to mention that this option allows the threads_mngr to be supported on additional platforms, as we can use posix pthread api instead of linux syscalls. but, as i said, more complicated. ** or - well, i can't see we call any of these functions in redis. assuming they are unsafe since they use static variables, i think we can take the risk.",0,0,0,0.921742022037506,0.9908430576324464,0.990907907485962,0.0,accept,unanimous_agreement
1763369563,12453,"* the pthread_mutex_lock calls are new, one is your recent work, and the other one is from #7700, i suppose we must find another way. * as for serverlog, we have serverlogfromhandler that's intended for this use case, we need to convert the overlooked ones to use it. * i suppose we should regard backtrace as ok, unless we find somewhere that explicitly says otherwise. * as you said, the backgrace allocation can be moved the the calling thread's stack * replacing the semaphore with e sleep-loop wait sounds ok to me. * regarding the tids list, limiting a maximum number of threads seems reasonable * i've seen some advise in so to call opendir in advance, and keep the dir pointer for later, if we can't use that, then i suppose your thread registration api would be ok (although unoptimal). i wouldn't want to draw any guess work conclusions, or rely on us not calling these in other places in redis.",0,0,0,0.956444263458252,0.9839794635772704,0.8951966762542725,0.0,accept,unanimous_agreement
1763708858,12453,"`register_thread()` api will also need `deregister_thread()` as well right? modules can start/kill threads in between. one caveat is if module uses a library and the library starts a thread, then you are going to miss it. regarding making existing impl safer, maybe we can replace these functions with signal safe versions: - fopen(), fgets(), fclose() --------> open(), read(), close() - prctl() --------> open(), read(), close() for `/proc/self/task/ /comm` - snprintf(), atoi(), strtoul() --------> maybe we can implement these in place or write a small version of these functions. if i'm not mistaken, we just use these for positive integers, probably we can get away with a few lines of code. - opendir(), readdir(), closedir() ----> syscall(sys_getdents): i guess we can assume this syscall is async signal safe. please, see this discussion and the code that shows how it parses: [a link] - i assume you are going to fix those explicit malloc calls and semaphore usage as well. if we do these changes, we will be doing three things we are not supposed to do: `syscall(sys_tgkill)` , `syscall(sys_gettid)` and `syscall(sys_getdents)`. probably, it is okay to assume these are safe to call in the signal handler.",0,0,0,0.9745498299598694,0.9930114150047302,0.9920979738235474,0.0,accept,unanimous_agreement
1763964689,12453,"[a link] this pr fixes the heap allocations. regarding the api, yes of course it should include a remove function. let's see if we can fix the current implementation without introducing a new api. * saving the dir pointer won't solve `readdir`. suggestion lgtm. * atoi(), strtoul() are already implemented in src/util.c. * instead of using snprintf() we can directly write to the log file. i suggest dividing serverlogfromhandler into 3 functions: one function `startserverlogfromhandler` for preparations and writing the title (` :signal-handler ( )`) and `addmessageserverlogfromhandler`to add the message. `endserverlogfromhandler` to print the new line and close the file sequential calls to messageserverlogfromhandler can replace `snprintf` in most if not all cases. [code block] additional callback that are unsafe: logconfigdebuginfo uses malloc for the sds string, and serverlograw. dofastmemorytest: * killthreads -> calls pthread_join(), pthread_cancel(), serverlog * memtest_test_linux_anonymous_maps() -> fopen, fgets, strtoul, snprintf",0,0,0,0.9630019664764404,0.9764795899391174,0.9898409247398376,0.0,accept,unanimous_agreement
1764234933,12453,"lgtm. let's also change logconfigdebuginfo to print directly instead of using sds (add a comment as to why) regarding dofastmemorytest, let's ignore that crap.",-1,0,-1,0.8722972869873047,0.8392037153244019,0.7308251261711121,-1.0,accept,majority_agreement
1784060599,12453,"reagrding **`sprintf` and `serverlog`.** as i mentioned `serverlog` uses unsafe functions, among them sprintf. replacing `serverlog` with `serverlogfromhandler`, and any sprintf with sequential writes raise several issues: 1. the code is very long and messy. for example, this is how printing `62220:m 26 oct 2023 14:39:04.526 # redis 255.255.255 crashed by signal: 11, si_code: 2` looks like [code block] or path generation: [code block] 2. `serverlog` begins with a different header than `serverlogfromhandler`: `62220:m 26 oct 2023 14:39:04.526 # `vs. `62220:signal-handler (1698331136)` after discussing with oran, i found out that we've been using `serverlog` in the signal handler ever since redis exists and we should reconsider if changing it now is necessary. another thing that i've noticed is that `serverlog` calls **file streaming functions** (fopen etc) which we also tried to avoid. after experimenting suggestion, **i'm unsure that `sys_getdents` is preferred over `opendir`.** when i tried using it (check out [a link] it didn't recognize this syscall, but **sys_getdents64** did work, on which i didn't find any good documentation. i ran into gnu's page regarding [a link], which is as-safe, but again, didn't find the formal definition of `struct dirent64`, i kinda guessed it. ([a link] `d_type` is the last field, but i experimentally found out that it comes before `d_name`. and again, this is on my local machine and i can't guarantee that this is compatible with all platforms. the bottom line is imo if talking about **stdio functions**, the benefits of readability, maintainability, compatibility, and simplicity are more significant than the risk of being async-signal-unsafe.",-1,0,0,0.9024059772491456,0.9609599113464355,0.9277704358100892,0.0,accept,majority_agreement
1784186857,12453,"i'm guessing that considering we had this since forever, then maybe it's safe. one observation i have is that `sigsegvhandler` (responsible for crashes that terminate the process) uses `serverlog` but `watchdogsignalhandler` that can be used on a live process without killing it, used `serverlogfromhandler` (which uses `open` instead of `fopen`, and doesn't use any malloc or sprintf) so i'd suggest to stick to that. be more permissive in crashes, but be very strict in `sigalrmsignalhandler`. wdyt?",0,0,0,0.9332475662231444,0.97755765914917,0.9762023091316224,0.0,accept,unanimous_agreement
1784217815,12453,"if we are already calling non-async safe functions (like fopen which calls `malloc()`) and no one complains, maybe it is okay to use these new ones. though, using more of these functions will increase the chance of happening something bad. (i guess a possible crash or a deadlock). if you decide to do it by the book: - for snprintf, a common practice is writing a limited version like this: [a link] so, you know it is signal safe. - i see using getdents is quite messy and requires making bunch of assumptions. here is how cpython has done it: [a link] , [a link] in any case, it is quite ugly.",0,0,0,0.5228356122970581,0.655002772808075,0.835099995136261,0.0,accept,unanimous_agreement
1784638452,12453,"well, most of the code i added is handling with printing the stacktrace so it will be called on both scenarios. * i can try this one, thanks! * yep, looks similar to what i did. anyway, i have this already implemented, and i also ran [a link] against it, seems to be ok, but as you said, i made some guesses... let me know what you think. in the meantime, you can take a look at the last commit i've pushed (wip) to see how ugly it is :grinning_face_with_smiling_eyes: [a link]",1,1,1,0.8889992833137512,0.9620566964149476,0.92159903049469,1.0,accept,unanimous_agreement
1785599947,12453,"the important bit here is to avoid regression, so we should aovid having a new unsafe function or an existing one in a new context (i.e. not right before exiting). i support 's suggestion of using a safe `snprintf` subset, i think it's both practical and not ugly.",0,0,0,0.9432010054588318,0.8322210311889648,0.8195026516914368,0.0,accept,unanimous_agreement
1797880986,12453,"hi, pr is ready [a link] please review :)",1,1,1,0.9745697379112244,0.9954718351364136,0.5722623467445374,1.0,accept,unanimous_agreement
1257424975,11290,missing `set-listpack` in [a link],0,0,0,0.9790548086166382,0.9944639801979064,0.9938020706176758,0.0,accept,unanimous_agreement
1257665844,11290,done.,0,0,0,0.975940763950348,0.9640594124794006,0.9897913336753844,0.0,accept,unanimous_agreement
1296326122,11290,"sorry for force-push. there was a typo in my last commit. i could have just added a separate fixup but, well, i amended..",-1,-1,-1,0.9896265268325806,0.992074489593506,0.9925187826156616,-1.0,accept,unanimous_agreement
1302130605,11290,"trying to do a status check, i resolved some comments that seemed already handled, i can see 4 comments that are unresolved, but it seems they're all about optional future improvements. maybe the one about lpbatchdelete is an exception (i.e. maybe we wanna handle it here). please comment and let me know what you think the status is and what's missing if any.",0,0,0,0.9731918573379515,0.97069251537323,0.7887709736824036,0.0,accept,unanimous_agreement
1302361710,11290,"awesome. yes, i think this is ready to merge. if you want me to solve any or all of the remaining comments, i can do as you want. here's a summary: 1. dict lookup using `char*` and `size_t` -- :x: leave out imo. sds creation just for dict lookup is a problem in many parts of redis, not only the sets type. it deserves a separate pr. 1. lpbatchdelete -- i can attempt it if you think it's strait-forward. now i understand what you mean anyway. wdyt :question: 2. intersection: initially large listpack allocation (lpnew with non-zero size), then scale it down at the end (lpshrinktofit). -- it seems strait-forward now that you explain it like this. i can do it if you want. :question: 3. lpfindinteger -- :x: leave out.. or implement?",1,1,1,0.9728321433067322,0.9932492971420288,0.9918183088302612,1.0,accept,unanimous_agreement
1302455302,11290,"yeah, let's try to handle these two you marked with `?`, see how it goes, and leave the other two out. p.s. i haven't reviewed the tests yet.",0,0,0,0.9738043546676636,0.9768576622009276,0.989203155040741,0.0,accept,unanimous_agreement
1305763701,11290,"/core-team please approve: 1. new rdb version 2. new output for object encoding 3. new configs (see the top comment) note that these default element count threshold is 512 (same as hash-max-listpack-entries, despite hash actually saving two listpack entries per element, so we could in theory set it to 1024). however, zset-max-listpack-entries is by default only 128.",0,0,0,0.9863154292106628,0.9903711676597596,0.9704668521881104,0.0,accept,unanimous_agreement
1305770659,11290,is there anything still pending? maybe some benchmark attempt for the sinterstore pre-allocation heuristics we added in the recent commit?,0,0,0,0.9879325032234192,0.994225800037384,0.9945490956306458,0.0,accept,unanimous_agreement
1305800509,11290,"now they are. anything particular in mind? is the github benchmark tag useful? i tested sinterstore to compare starting off with an intset vs listpack. i did this test in tcl. do you want me to commit it under the `slow` tag? the test starts with two identical sets of 100k elements, then adds some extra junk to make them non-intsets (hashtable) for the various cases. if the result is a hashtable, the time it takes is almost half (30ms vs 50ms) if we start with a listpack compared to if we start off with intset (which is later converted when a non-int is encountered). if the result is an intset, the difference is negligible, i.e. no regression.",0,0,0,0.9543326497077942,0.9736749529838562,0.9722387194633484,0.0,accept,unanimous_agreement
1306033645,11290,"i don't think we need to commit this benchmark, just wanted to manually make sure our optimization pays off, or at least doesn't add a regression.",0,0,0,0.9673551917076112,0.972493588924408,0.979539155960083,0.0,accept,unanimous_agreement
1308344988,11290,"this was conceptually approved in a core-team meeting, the only pending action is to change the default count threshold from 512 to 128 (similar to zsets). please make that change and let me know if there's anything else missing before this can be merged. thanks a lot.",1,1,1,0.978412926197052,0.967745304107666,0.9886788725852966,1.0,accept,unanimous_agreement
1308583347,11290,"sure, i'll change to 128. just thinking that a zset listpack entry is two elements (key and score), while a set only has one element set element, so perhaps 256 is a better match?",0,0,0,0.9834502935409546,0.988448441028595,0.9851227402687072,0.0,accept,unanimous_agreement
1308689320,11290,"logically you're right. as i said maybe even 1024 is right (double as what we have in hash), but as i noted [a link] we don't know anything about how these defaults were created, and we'd rather the the conservative approach, give up some potential memory savings and avoid introducing a bigger performance regression. btw. do we now need to re-do the benchmark at the top? or just mention it was done with a different config?",0,0,0,0.9431204795837402,0.9486185312271118,0.9565995335578918,0.0,accept,unanimous_agreement
1308901570,11290,"the benchmark was done with ~50 elements per set, with some randomness. i don't think any of these sets got more than 128 elements so i don't think we need to redo the benchmark.",0,0,0,0.9819613099098206,0.9710066318511964,0.9869165420532228,0.0,accept,unanimous_agreement
1309184937,11290,"thanks for another piece of good collaboration! it's been a pleasure, as always.",1,1,1,0.990391969680786,0.9957205653190612,0.994294822216034,1.0,accept,unanimous_agreement
1316520907,11290,"recently the corrupt-dump-fuzzer test often hangs in sdiff `command caused test to hang? sdiff _set` [a link] `command caused test to hang? sdiff _zset` [a link] `command caused test to hang? sdiff _setbig` [a link] `command caused test to hang? sdiff _set` [a link] i suspect it has to do with this pr, can you look into it? maybe improve the mechanism added by #8837 to print `$sent` instead of `$cmd`, to easily reproduce it.",0,0,0,0.9828216433525084,0.9902079701423644,0.9885119199752808,0.0,accept,unanimous_agreement
1316535155,11290,"i also noticed, reproduced before, but it didn't stop there with `--stop` option i tried adding something (something like --stop) early to see if i can reproduce it locally. i am running a loop in my machine. let me see if i can get some useful information",0,0,0,0.9457120895385742,0.9393234252929688,0.9736314415931702,0.0,accept,unanimous_agreement
1316544754,11290,"iirc the mechanism i implemented there kicks in only when the `--timeout` is reached and the whole thing is terminated. `--stop` works when a test fails. i'm guessing that in order to reproduce this we need: 1. create a bash loop that runs many short sessions of this test (avoid using a long `--timeout`, and run with `--accurate` since that will cause us to detect the hung a lot after it happened). 2. print `$sent` instead of `$cmd`.",0,0,0,0.9854352474212646,0.9596185088157654,0.9892567992210388,0.0,accept,unanimous_agreement
1321776911,11290,"i successfully reproduced the problem, this payload: [code block] this payload actually created a set with repeating elements (-3, see the example), the sdiff except 10 elements, but got 9 in the last, so it hang if we set `sanitize-dump-payload` to yes, we will be able to find the dup ele and report ""err bad data format"" the fix for sdiff (avoid the hang), do you guys think of anything else that needs fixing?: [code block] the fix: #11530",0,0,0,0.9191030859947203,0.9139990210533142,0.9248791933059692,0.0,accept,unanimous_agreement
1321787327,11290,"nice! (generated a broken protocol and left the client hung). i agree, this is the right fix, please make a pr. if if you have any test suite improvements that you wrote to hunt that, please add them as well.",1,1,1,0.982353448867798,0.9917469024658204,0.9939005374908448,1.0,accept,unanimous_agreement
997457092,9938,/core-team please approve,0,0,0,0.981871485710144,0.9787766337394714,0.98884516954422,0.0,accept,unanimous_agreement
1000757867,9938,"if any conflicts are detected, the operation is aborted without any modification to the server.",0,0,0,0.9803003072738647,0.9862005710601808,0.9912370443344116,0.0,accept,unanimous_agreement
754725523,8288,"thanks for this pr, haven't reviewed the code yet, but i wanna mention my initial thoughts. 1. i'm a bit uncomfortable to merge an api for just ""add"" before we have a design for the rest of the stream apis. my fear is that when we'll add the rest later, we'll have some realizations and will want to retroactively change something in an api that's already released. 2. i wonder if the ""add"" api is indeed useful on its own? i.e. if there are a lot of use cases for modules that would just like to write into a stream, and don't care much about reading (or in which performance of writing is more critical than reading). pinging and in case they can share some thoughts on this.",-1,-1,1,0.937393605709076,0.8939027786254883,0.9219605326652528,-1.0,accept,majority_agreement
754990555,8288,"sure, leave it open until we have the rest of the design in place. i created this pr to initiate a discussion. yes, it is. any module currently using rm_call() will be faster with each native api function available. my associates gave me this wishlist with the following priority: 1. xadd ""missing and very much needed, high overhead, called a lot."" 2. xread/xrange 3. xtrim xlen is already supported as redismodule_valuelength(). sounds good. i might start drafting a design for read, range and trim too as separate prs (but i'd prefer to wait for 's ""trim by minid"" pr to be merged before i start implementing a trim api).",1,1,0,0.9655977487564088,0.8056668639183044,0.7878543734550476,1.0,accept,majority_agreement
755372091,8288,"i have a feeling that one issue or pr would be better to discuss these api since changes in one may reflect the other, and they can share some flags / type. also, the code is not huge, being too large to review in one go. the way i see it, the only reason to split them into separate prs is if one is easier and more urgently needed than others, and we'd like to merge it before others are ready.",0,1,0,0.6908162236213684,0.4793711006641388,0.9333497881889344,0.0,accept,majority_agreement
758762097,8288,"pr updated to include adding, iterating and trimming. please review mainly the api design.",0,0,0,0.9815862774848938,0.9803969860076904,0.9928390383720398,0.0,accept,unanimous_agreement
762149910,8288,"sure we can, but combined with your suggestion for iterating over the fields, what should ""get"" return? if anyone needs it, it's always possible to use call. but sure, i can add streamdelete if you want. or maybe as a flag to the iterator, deleting as we go? afaik yes.",0,0,0,0.9766522645950316,0.9760427474975586,0.9825397729873656,0.0,accept,unanimous_agreement
762159284,8288,"good point. i guess an iterator is enough. come to think of it, the xread and xrange are also iterator based. ideally, we'll have a delete api that takes id, can can be used both without iteration, and as part of an iteration. but iirc the iterator is actually holding the current node, so we can't delete it before doing next. this leaves us with two a bit ugly options. 1. tell the user that if he want's to delete during iteration, he need to remember the last id, do a next, and only then delete the previous id. 2. add a separate api that does ""delete and step"". (in addition to a next and a stand-alone del). is that right? or am i missing something (sorry busy with other things so i can't afford to dive into the code and plan right now)",-1,1,1,0.362522155046463,0.8415496945381165,0.6192777156829834,1.0,accept,majority_agreement
763708353,8288,seems good.. what else is missing before this is ready to be merged?,1,1,0,0.9691984057426452,0.7969935536384583,0.8448631167411804,1.0,accept,majority_agreement
763806075,8288,"todo: * [x] set errno **(eof vs err with enoent for end-of-iterator tbd)** * [x] rm_streamiteratordelete() – requires that the current stream id is stored somewhere, probably in the key. (it can share space in the redismodulekey struct by making a `union { zset iterator stuff ; stream iterator stuff }`) * [x] out-of-bounds checking for rm_streamiteratornextfield() – requires storing numfields and a field counter somewhere, in the key struct. * [x] optimization: call signalkeyasready() on rm_closekey() instead of on every streamadd(). if you agree with the above...",0,0,0,0.9826448559761048,0.9954591989517212,0.9918453097343444,0.0,accept,unanimous_agreement
763913018,8288,"the above seem fine, but i need to look at the details / implementation to be able to judge better. i posted a comment about signalkeyasready above which i think we also still need to discuss.",0,0,0,0.9791995882987976,0.977519154548645,0.963266909122467,0.0,accept,unanimous_agreement
764979591,8288,"well, if we allow anonymous unions and structs here (which are standard in c11) we can use a union without changing the reference to the zset fields...",0,0,0,0.9877392649650574,0.9936248064041138,0.9920117259025574,0.0,accept,unanimous_agreement
766304629,8288,"sadly we can't assume c11 is supported. redis 6.0 went that way, and we had to revert that in 6.2. the other option is to create proprocessor replacements that will expand these member names, but that's ugly.",-1,-1,-1,0.9779415130615234,0.9915068745613098,0.9896285533905028,-1.0,accept,unanimous_agreement
766397945,8288,/core-team please approve the creation of new module api for stream manipulation and iteration (handing anything other than consumer groups). please make sure that the top comment is up to date (will be used as commit comment too). please make sure to run the new tests with `--valgrind`.,0,0,0,0.9705645442008972,0.9896715879440308,0.9712722301483154,0.0,accept,unanimous_agreement
769112981,8288,"thanks a lot, especially , for your fast and useful responses! i makes me enjoy working with redis. :-) see you in a different pr. next, i'm considering picking something from the [meta] modules api issue.",1,1,1,0.9927754402160645,0.9958643913269044,0.9972068667411804,1.0,accept,unanimous_agreement
769125070,8288,thank **you** viktor for taking the initiative and doing all the work.,1,1,1,0.9550018310546876,0.5591772794723511,0.9754862189292908,1.0,accept,unanimous_agreement
1780830081,12658,"one of the things we aim to solve here is to avoid memory allocations from within the signal handler. apparently getconfigdebuginfo uses the config rewrite mechanism to generate an sds with the debug configs, but i wonder why you chose to use the config rewrite mechanism for this function and not just print the key and value?",0,0,0,0.9701151251792908,0.898544430732727,0.9815566539764404,0.0,accept,unanimous_agreement
1819494881,12658,basically done daily is running [a link] tomorrow i'll edit the top description to include all the latest changes.,0,0,0,0.983292281627655,0.9870553016662598,0.9923517107963562,0.0,accept,unanimous_agreement
1819753115,12658,"out of curiosity, i've complied this branch and sent sigalrm. this is the output: [code block] maybe, we broke something on the way. this is not the expected output right?",0,0,0,0.9797163605690002,0.9489291906356812,0.945831298828125,0.0,accept,unanimous_agreement
1820288428,12658,no this is not doesn't happen to me locally + all the test passed + the test includes checking that we don't get this specific line so it doesn't happen on any system we support what os do you have?,0,0,0,0.978129744529724,0.9595325589179992,0.9942598342895508,0.0,accept,unanimous_agreement
1820412744,12658,[code block] i see it doesn't work with current unstable as well on my local. maybe there is some sort of security enabled on my system (i guess i should be happy),1,0,1,0.5734919905662537,0.5267797708511353,0.9403570294380188,1.0,accept,majority_agreement
1820623546,12658,does debug segfault or debug assert works for you (before this pr)? maybe we can at least improve the failure to print the relevant `errno`?,0,0,0,0.9885302186012268,0.9929431080818176,0.9939034581184388,0.0,accept,unanimous_agreement
1820627256,12658,apprently the condition o the signal mask wasn't correct fixed it in the last commit please confirm it fixed the problem,0,0,0,0.9873759746551514,0.9839569926261902,0.988085389137268,0.0,accept,unanimous_agreement
1820637183,12658,"thanks, it works now! btw, you may want to mention it in the top comment. the check was not correct before this pr.",1,1,1,0.981192708015442,0.9876821637153624,0.9960511326789856,1.0,accept,unanimous_agreement
1823258332,12658,"i think we're done here, please make sure the top comment is up to date, and the tests pass on the various platforms.",0,0,0,0.9732138514518738,0.9369404911994934,0.97108793258667,0.0,accept,unanimous_agreement
1823843910,12658,:clapping_hands: :clapping_hands: it's updated daily with `--single integration/logging` is green [a link] now the full routine is running [a link],0,0,0,0.6552721261978149,0.9585121870040894,0.9558658599853516,0.0,accept,unanimous_agreement
973983952,9812,"good work! ping /core-team to review. about scripts persisting, i prefer no, since we never guarantee scripts could be persisted, and as antirez said, it can make replication clearer. and i think we can allow eviction in lua now, fix issue #8478.",1,1,1,0.9909489750862122,0.9871612191200256,0.9949724674224854,1.0,accept,unanimous_agreement
974021428,9812,"thank you and welcome. i was under the impression we agreed this task is assigned to -steinberg . in any case, i think this was done too early, and will need to be re-done once #9780 is merged. in the meanwhile, we can start discussing the details here (instead of the other 3 existing open issues / prs on the same subject). there are currently arguments for both keeping scripts deterministic (i'll let post them), as well as keeping the persistence (which you argued for). regarding the persistence, the eval design was very clear from the get go that clients can't assume the script is cached, and the evalsha is an optimization that the client should be prepared for any case the script is not cached. i know users kept arguing that it's buggy, without reading the theory and design behind it (which is that the script is part of the client application, that's executed in the server, and not at all the responsibility of the server, which is why it was not named and not versioned) in redis 4.0, because of psync2, these cases where the script is missing got rarer, so users may have got accustomed to it. and now that we remove eval propagation, in theory we can go back on that, and never replicate or persist the eval scripts. this will make a the eval scripts design and the differences from redis functions clearer, and force users to realize that. but indeed some users will see it as a step backwards.",1,1,1,0.9352065324783324,0.9775901436805724,0.971166729927063,1.0,accept,unanimous_agreement
974031344,9812,"oh i didn't notice it, maybe you mean this approach [a link] but i think after this pr we can allow eviction in lua just like `multi/exec`, so the problem can be fixed smoothly and no need break atomic.",0,0,0,0.9173970222473145,0.9181444048881532,0.9772394895553588,0.0,accept,unanimous_agreement
974036907,9812,"i thought we agreed that both of these will be assigned to yoav, but maybe don't recall something. anyway, it's too early to code it, but not too early to discuss and decide... regarding the oom, even if we allow eviction, the problem still exists since the server might be configured not to allow eviction. and also, there's the other discussion to make about deterministic execution (which i mentioned above but left for yossi to present), which may lead to concluding that we still can't evict inside a script. but anyway, let's not mix these two topics, and discuss each of them in a separate pr.",0,0,0,0.9281145930290222,0.9493088126182556,0.8865959048271179,0.0,accept,unanimous_agreement
974041507,9812,"let's first debate about propagation and persistence. i remember salvatore vocally mentioning that users should not assume scripts are cached, possibly even if the same connection that loaded them is still alive. i don't remember where it was and what was the argument, but the thing is that the design is that the script and its exact content are actually part of the caller app (not the server's responsibility), and it should always be prepared to reload it. the main feature is eval, and the sha part is just an optimization.",0,0,0,0.9185125827789308,0.9671691060066224,0.971868932247162,0.0,accept,unanimous_agreement
974804563,9812,"persistence and replication: the actual guarantee as documented was never very clear and at some point ([a link] it was limited to the scope of a single connection (although `script flush` can break that as well). redis 7 and redis functions is a good opportunity to clean this up and re-set the client expectations, so i think it makes sense to completely drop script replication. eviction: i'm not in favor of that. while it's not strictly needed for consistency any longer, i think guaranteeing atomicity is still good practice, especially given that scripts are not designed to be long running. having scripts suddenly see keys vanishing keys during execution in redis 7 seems like a problem to me. deterministic execution: this too is not strictly needed but i think not being able to enforce it, at least optionally, is a potential regression. i think it's reasonable to expect identical results if you run the same script on multiple instances with the same dataset (e.g. running tests scenarios on different envs, prod vs. staging/dev, etc.).",0,0,0,0.6071037650108337,0.969031572341919,0.9289416670799256,0.0,accept,unanimous_agreement
982448378,9812,"stage summary: looks like we have reached a consensus that script persistence/replication can & should be dropped, and haven't seen any objections in the past few days. if it's done, then we can move onto the next topic. for deterministic execution, i think non-deterministic results are consistent with users' expectations when they put ""random"" commands in scripts/functions and send them to multiple instances, while deterministic may not. so why bother making the result deterministic? anyway, it would be better if someone could cite their examples about it.",0,0,0,0.946434199810028,0.9793771505355836,0.5298696756362915,0.0,accept,unanimous_agreement
982468917,9812,"we had a discussion about this these topics in a core-team meeting today. at first, we concluded that we better keep the feature in redis that allows scripts to be deterministic, but maybe let the user control if it's enabled or disabled. we considered to either: 1. if we intend to keep this feature forever, then have it disabled in functions but default, and keep it enabled in eval by default (so we don't break existing scripts). 2. or if we intend delete it some day, we can start by making it disabled by default for both functions and eval (so users can enable it if the run into trouble), and then consider deleting the code at a later stage (kinda how the lua_replicate_commands feature was introduced in baby steps too). but then we realized that since `lua_replicate_commands` was changed to default of true, it also meant that scripts where allowed to be non-deterministic by default, and we are not aware of anyone who complained. so in fact we're already half way through on our way to delete that feature, and we can take the next step now (actually delete it). regarding script persistence and replication, as was said before, it was never meant to be persisted and replicated, and even if some users started getting accustomed to it since psync2, it's actually likely that it'll cause them bugs, since the scripts are sometimes replicated, but they're never persisted. we wanna proceed and delete that feature completely. so i think the next step is to wait for the functions pr to be merged, and then re-do this deletion work.",0,0,0,0.9470683932304382,0.9865243434906006,0.8479047417640686,0.0,accept,unanimous_agreement
982478114,9812,"totally agreed. btw should i set this pr as a draft, or close it now and make a new pr after function merged?",0,0,0,0.899878978729248,0.9658556580543518,0.6962106823921204,0.0,accept,unanimous_agreement
982497760,9812,i think you can just force-push the new version into it when it's ready. no sense in opening another thread when we have the discussion here.,-1,0,0,0.5624595284461975,0.9797713160514832,0.982187271118164,0.0,accept,majority_agreement
983312497,9812,"btw, i see this pr doesn't remove the `master and slave consistency with evalsha replication` test, i guess it should. this test keeps hanging every day in the freebsd ci (i guess for being slow). not sure what was the process to decide which tests to delete, was it just ones who failed?",0,0,0,0.8748948574066162,0.7654508948326111,0.807889997959137,0.0,accept,unanimous_agreement
983332687,9812,"oh, i didn't notice this test... i just checked all tests in scripting.tcl, tests with keywords like ""lua-replicate-commands"",""redis.replicate_commands"" or else and other failed tests, which i thought are all of them. i'll check other test files later.",0,0,0,0.8981919288635254,0.972075581550598,0.8524407744407654,0.0,accept,unanimous_agreement
986180121,9812,"we discussed script persistence and propagation in a core-team meeting and concluded we want them removed. in any case i don't see a reason to keep script persistence if we trimmed the propagation part. also, we'll need to remove the excessive command flags soon, but let's do it after #9656 is merged. i added these to a `todo` section in the top comment.",0,0,0,0.9828040599822998,0.9928517937660216,0.9906721115112304,0.0,accept,unanimous_agreement
986485256,9812,"i remind you about `master and slave consistency with evalsha replication` and possibly other tests that make no sense now. on one side, as long as they pass, maybe they still have value and increase coverage, but on the other hand some may not make any sense. specifically for the one mentioned above, i'd like to see it gone since it keeps [a link] on freebsd ci.",0,0,0,0.967957615852356,0.96701180934906,0.9740731716156006,0.0,accept,unanimous_agreement
986488540,9812,"discussed about it with last week, maybe this test is not totally useless, though replication cache has gone and it is effects replication now? or you mean it is duplicated with other tests so we have no need keeping it? just some confirmation.",0,0,0,0.9673065543174744,0.6333394646644592,0.990334689617157,0.0,accept,unanimous_agreement
986507844,9812,"well, as i mentioned, we don't wanna blindly delete all the tests that where aiming to test the eval propagation, since (as long as they don't fail) they may still have value and increase coverage. but i do think we wanna evaluate them one by one and try to conclude which ones can be removed and which ones should be kept (maybe slightly changing their code and title, so it makes sense in the new reality). regarding this specific test, do you have any reason to believe it provides some value that's not covered by many other tests? what specific value does it add. i imagine that anything it tests (propagation of eval) has other tests that cover it, and the only thing that's special about this one is that it was aiming to exhaust the script cache. at the very least, we should modify this line: [code block] i.e. first, the comment is completely outdated, and secondly if we now change `numops` to 2 rather than 20k, maybe the freebsd tests will stop hanging every day.",0,0,0,0.9213470816612244,0.950230062007904,0.9668381810188292,0.0,accept,unanimous_agreement
986518094,9812,"i guess there isn't any other 'unique' points that this test covers, but i was not so sure about it(i don't like 'guess'). going to remove this test rather than erasing '0's, for simplicity.",-1,0,0,0.5162061452865601,0.6222056746482849,0.8518407344818115,0.0,accept,majority_agreement
986544493,9812,"please invest a few minutes to look for other tests that may no longer be needed (e.g. this one specifically mentions the ""script cache""), others may switch between eval and evalsha, add and remove replicas, etc. lets list them here and try to reach a conclusion if they should be trimmed, or just slightly modified / re-labeled.",0,0,0,0.985919952392578,0.991705060005188,0.9869269728660583,0.0,accept,unanimous_agreement
988563862,9812,"i took some time looking into tests with keyword ""eval""/""script""/""replication cache""/... and re-evaluating them. too many that i can't list them all, so some of them which i think may draw your attention are listed below: tests i think should be kept without modifying are marked with :check_mark_button:. ones i am not sure are marked with :red_question_mark:. last ones with :cross_mark: should be removed. still there may be some missing tests, but i think they are quite few. ## integration/psync2 psync2: replica rdb restart with evalsha in backlog issue #4483 :red_question_mark: ## integration/replication 348 master stream is correctly processed while the replica has a script in -busy state :red_question_mark: ## integration/replication-3 147 slave can reload ""lua"" aux rdb fields of duplicated scripts :cross_mark: looks like it's useless now? ## cluster/04-resharding cluster consistency during live resharding :red_question_mark: ## cluster/10-manual-failover send cluster failover to #5, during load :red_question_mark: ## unit/acl acl can log errors in the context of lua scripting :check_mark_button: keep it ## unit/multi multi propagation of script load :check_mark_button: keep it multi propagation of eval :check_mark_button: already modified in previous commits ## unit/scripting eval processes writes from aof in read-only slaves:red_question_mark: we can call scripts rewriting client->argv from lua :red_question_mark: ### 765 tags {""scripting repl needs:debug external:skip""} :red_question_mark: need detailed analyzing and judgement ### 862 tags {""scripting repl external:skip""} :check_mark_button: should keep them all",0,0,0,0.7763294577598572,0.9288593530654908,0.7366862297058105,0.0,accept,unanimous_agreement
992312419,9812,"psync2: replica rdb restart with evalsha in backlog issue #4483 :red_question_mark: **delete it** master stream is correctly processed while the replica has a script in -busy state :red_question_mark: - **keep it**, the script is runs on the slave slave can reload ""lua"" aux rdb fields of duplicated scripts :cross_mark: looks like it's useless now? **yes, delete it** together with the test above it and the entire server block. cluster consistency during live resharding :red_question_mark: **keep**, it has nothing to do with script propagation afaict send cluster failover to #5, during load :red_question_mark: **keep**, it has nothing to do with script propagation afaict acl can log errors in the context of lua scripting :check_mark_button: keep it **yes, keep it** multi propagation of script load :check_mark_button: keep it i guess now is the time to also avoid propagating script load (since we avoid propagating eval, and avoid saving scripts to the rdb). in which case we can keep that test, but negate it. (and mention this change in the top comment, i.e the fact that script load is no longer propagated) good. eval processes writes from aof in read-only slaves:red_question_mark: we need to change this test to create the aof manually, like you did for `eval timeout with slow verbatim lua script from aof` (and maybe move it to aof.tcl too) wtf? this test isn't doing what it says it does, and never did (76c31d425e797ddf5daedd29d893d3fc9c7cfc19). i think we should fix the test to do what it says it aims to do. -steinberg do you have any idea what's going on with this commit? :red_question_mark: need detailed analyzing and judgement i don't understand what you're referring to. :check_mark_button: should keep them all i don't understand what you're referring to.",0,0,0,0.9579560160636902,0.992294430732727,0.9817051887512208,0.0,accept,unanimous_agreement
992335041,9812,"i meant the tests in line 765-860 and in line 862-971 in scripting.tcl. didn't have time looking deep into them, but now i think they should all be kept.",0,0,0,0.9796469211578368,0.9712443351745604,0.9846622943878174,0.0,accept,unanimous_agreement
992345255,9812,"ok, i looked at them, and some are a little bit out of context (e.g. `before the replica connects we issue two eval commands`), but let's keep them",0,0,0,0.982952356338501,0.9805793166160583,0.9848666787147522,0.0,accept,unanimous_agreement
993238401,9812,"two quick notes: everything's ready except the ""we can call scripts rewriting client->argv from lua"" thing, i have no idea what it is doing. maybe we should wrap them with eval?... and ""multi propagation of script load"" i made some tiny changes on this case, instead of removing it. please take some time looking at the case in aof.tcl in commit 0ee9dfb (which is just above this comment). i spent some time testing about this case. the result is: before redis function pr, the write eval commands in aof can be loaded into a ro-slave with command ""debug loadaof"" successfully (which means the result is 102). however after the pr, this cannot be done (which means the result is nil and i got an '-readonly' err). is this change intended or it's out of expectation? ================temporary debugging notes here============================= upd: same problem with loading aof when starting server. i'm sure there's something not going well with the func `scriptverifywritecommandallow` in `scriptcall`. upd2: looks like the bug occurs when the `rctx = luagetfromregistry(...)` in function `luaredisgenericcommand` is called. the returned ctx `rctx->original_client` should be that aof value, but it's not. i'm not familiar with lapi.c, they are just too hard to read. could you explain what happens in this `luagetfromregistry` function (it's in script_lua.c:79) for me? much thanks to you! upd3:no need to explain it anymore.",0,0,1,0.8021615743637085,0.8293266892433167,0.6388375759124756,0.0,accept,majority_agreement
993460131,9812,i will take a look and update asap.,0,0,0,0.9829083681106568,0.9714280962944032,0.9729149341583252,0.0,accept,unanimous_agreement
995703093,9812,"this is the pr that should move ""sort_for_script"" and ""random"" to be doc-flags right? just making sure it's not overlooked also, maybe rename sort_for_script to ""random-order"" or something more explicit",0,0,0,0.9828664660453796,0.9920895099639891,0.9930480122566224,0.0,accept,unanimous_agreement
995708757,9812,"if you mean mentioning it, yes. if you mean doing the moving work, no. [a link] i didn't track it further, what i can make sure is that discussions in this thread is all of the discussions about these 'flags' works(at least in this pr).",0,0,0,0.9751734733581544,0.9777087569236756,0.9916314482688904,0.0,accept,unanimous_agreement
995709978,9812,"no, this is gonna be the responsibility of #9876",0,0,0,0.9698834419250488,0.9725818634033204,0.943808615207672,0.0,accept,unanimous_agreement
995712132,9812,"i already copied our conclusions to the other issue, there's nothing more to do about these in this pr. all it does is delete all the code that uses these hints, and the hints will be refactored (and discussed) in another pr.",0,0,0,0.9799588918685912,0.9860052466392516,0.9934076070785522,0.0,accept,unanimous_agreement
996481524,9812,/core-team please approve (see top comment for a list of what's included and excluded),0,0,0,0.982280433177948,0.982886016368866,0.9807815551757812,0.0,accept,unanimous_agreement
998241662,9812,full ci: [a link],0,0,0,0.9845400452613832,0.8597871661186218,0.9959890246391296,0.0,accept,unanimous_agreement
1954562053,12826,/redis-committers maybe one of you has time to review this one?,0,0,0,0.9692664742469788,0.9927565455436708,0.985538125038147,0.0,accept,unanimous_agreement
1964523855,12826,"i tried this feature briefely on some small dummy data. i think the feature can be useful, but probably the output and other user experience can be improved. (i haven't thought about it much yet.) here is a text-based ""screenshot"" for the record: [code block] then, i tried it again after populating 1m keys (using `debug populate 1000000`). it does a scan over the entire keyspace. it suppose it can be quite costly. the comment ""use -i 0.1 to sleep 0.1 sec per 100 scan commands (not usually needed)"" worries me a little, [edit] but `--bigkeys` does this too, so i suppose it's fine. i've seen an oddity when aborting the scan with ctrl+c. it prints the results of the keys scanned, along with a message like [code block] during the scan, there is a nice progress bar displayed during the scan. if i abort the scan after 75% and then resume using `redis-cli --keystats --cursor 784131`, the progress bar then goes from 0 to 25% and then it stops at 25%, which looks odd. i would expect it to continue from 75% and scan to reach 100%, but obviously it doesn't know that the cursor is at 75% of the scan.",0,0,-1,0.9106330871582032,0.6944494843482971,0.9033385515213012,0.0,accept,majority_agreement
1967330078,12826,"the interactive progress bar and the possibility to resume if interrupted is a nice experience. can we add those to `--bigkeys` and `--hotkeys` too? similar features should have a similar user experience imo. maybe even `--scan` can benefit from interrupt and resume using `--cursor`, but i guess it makes sense only if stdin is a tty but stdout is not a tty, i.e. if it's invoked like `redis-cli --scan > file`. possibly we can display the progress bar and message on the tty in that case, where it isn't mixed with the returned keys. does it make sense? the design of the output (headings and tables) is not similar to the output of the other features like `--bigkeys`, `--memkeys` and `--stat`. i think we should use a similar design as the existing features.",1,0,1,0.7903715968132019,0.8831900358200073,0.9448981285095216,1.0,accept,majority_agreement
1968321433,12826,"you are correct about restarting at a cursor other than 0. it will be nicer to restart at the last percentage, but as you mentioned, i am not sure we can find a way to do so. i guess the user should be ok knowing that using `--cursor` will restart from 0% and will probably not reach 100%. in addition, between the scans, we might have added or remove some keys. `--keystats` should have all the information from `--memkeys` and `--bigkeys`. to keep the existing code behavior and output, i did not want to change it too much (except to fix the issue with `--memkeys-samples 0`). the worry being to break any automations that rely on the output of existing commands. i am not sure the output of `--memkeys`, `--bigkeys`, and `--hotkeys` is designed to be parsed, but some users are creative. if we think we should change the output of existing commands to be more consistent, we could do it in another pull request. for the redis data type summary difference, i did the change for readeability purpose. [code block] vs [code block] `--keystats` has more data to present as we combine `--memkeys` and `--bigkeys`. i think it is more readeable using a table, but the inconsistency comment is valid, if we think consistency is more important than readeability. i would say that most data presentation in `--keystats` breaks from `--memkeys` and `--bigkeys`.",0,0,0,0.955184817314148,0.9622871279716492,0.9677956700325012,0.0,accept,unanimous_agreement
1968882480,12826,"i guess you're right. i don't have a better idea, so i think this is fine. i think we cannot break the non-interactive output which the creative user can use in scripts, but in interactive mode it's ok to change things like progress bar. yes, but my experience says if we postpone it, it will not be done at all. bigkeys is faster than keystats when i tried, so some users may still want to continue to use it. of course. i like the table, but it just the design of the heading and table header row (start with three dashes, pipe between columns) which is not a style we used elsewhere. i think it looks like some kind of markup/down syntax. how about this style? (more graphic, less like a markup syntax) [code block] for headings, instead of three dashes in the beginning of the line (like `--- top 10 key sizes`), how about dashes in the beginning and in the end of the line (like `--- top 10 key sizes ---`)? it is more similar to the heading in bigkeys (`------- summary -------`).",1,1,1,0.737849771976471,0.9800044894218444,0.9021221399307252,1.0,accept,unanimous_agreement
1970524597,12826,"your table presentation and trailing dashes are indeed looking better. let me know if the output below is ok, and i can push a commit for that. [code block] to generalize the usage of the progress bar and allowing to use `--cursor` when interrupted, i can start implementing it for `--keystats`, `--memkeys`, `--bigkeys`, `--scan`, and `--hotkeys`. not to break the non-interactive output, we need to only display the progress bar and cursor information in tty, and make sure we keep the current output when redirecting to a file. when redirecting to a file, you would like to still keep some information on the tty. if we take `redis-cli --memkeys > file` for instance, we will see in the terminal: [code block] and in the file: [code block] if this is what you meant, i need to find out how to do that.",0,0,1,0.6476053595542908,0.9529635310173036,0.7510554194450378,0.0,accept,majority_agreement
1971108689,12826,"yes, that looks good. i'm glad you agree. :) if we take redis-cli --memkeys > file for instance, we will see in the terminal well, probably only for `--scan`. here the output can be a very large list of keys, which the user probably wants to parse or use for something, while it can be nice to have some progress indicator if we are still on a tty. it's different to the bigkeys/memkeys/hotkeys feature, and i'm not even sure it's a good idea, so i guess you're right that we should to leave this part for another pr. (an idea is `if (isatty(stderr_fileno) && !isatty(stdout_fileno)) { /* show progress bar on stderr and print data on stdout. */ }`.) for bigkeys/memkeys/hotkeys, i think we can simply use similar logic as you did for keystats, i.e. show the progressbar if stdout is a tty; otherwise hide it. i see you're checking `config.output == output_standard` to determine if the progress bar animation should be displayed or not. this can be messed up by `redis-cli --keystats --no-raw > file` (progressbar updates stored inside the file, which becomes huge). so i think we should instead check `isatty(stdout_fileno) || getenv(""faketty"")`.",1,1,1,0.9892756342887878,0.9959496259689332,0.9972811937332152,1.0,accept,unanimous_agreement
1972590943,12826,"i have pushed the new format for the tables and titles. you are right, when redirecting to a file we only need the percentage and not the progress bar. i actually show the progress bar in the file. i will remove it. good catch on `redis-cli --keystats --no-raw > file`, i did not think about `--no-raw`. refreshing the output requires to be more careful. thank you for the help, `isatty(stdout_fileno) || getenv(""faketty"")` works like a charm. i will test some more and check where to add the progress bar to `--memkeys`, `--bigkeys`, and `--hotkeys`, only if stdout is a tty.",1,1,1,0.9630069136619568,0.9805429577827454,0.9936942458152772,1.0,accept,unanimous_agreement
1987689440,12826,i updated `findbigkeys()` and `findhotkeys()` to have the progress bar in tty and the previous output when redirecting to a file.,0,0,0,0.9889287352561952,0.9905981421470642,0.9937790036201476,0.0,accept,unanimous_agreement
1990226834,12826,"-binbin raised two points: we should. i added some checks after using `strtoll()`. the function returns false if the value we are trying to insert is is negative or larger than the highest_trackable_value. we are inserting key sizes. we use `unsigned long long` for a key size and therefore cannot be negative. in keystats, the highest_trackable_value is set to 1 tb. it is unlikely we will have a key that large but we rather know if we cannot insert a key size. speaking of not checking a return value, i realized that i do not need to check the return value of `zmalloc()` and i did it... let me know if i should remove the check.",0,0,0,0.9030426740646362,0.9673600792884828,0.8137697577476501,0.0,accept,unanimous_agreement
1991986974,12826,"-binbin i have pushed a new commit to remove the unnecessary check of zmalloc(). let me know if there is anything else i should do. thank you for your help, i appreciate it.",1,1,1,0.9759648442268372,0.9909844398498536,0.976764976978302,1.0,accept,unanimous_agreement
2016979534,12826,[a link] all committers have signed the cla.,0,0,0,0.9875962734222412,0.9764910340309144,0.995100438594818,0.0,accept,unanimous_agreement
2044116818,12826,"i submit a code style commit, please have a look.",0,0,0,0.9838334918022156,0.9707521200180054,0.9918521046638488,0.0,accept,unanimous_agreement
2044148217,12826,thanks for all the fix up!,1,1,1,0.9408631920814514,0.9820410013198853,0.896804690361023,1.0,accept,unanimous_agreement
2048996010,12826,"let me know if you have more suggestions. one comment. as suggested previously, the progress bar from `keystats()` was added to `findbigkeys()` and `findhotkeys()`. as we did not want to break some potential automations created by the users, we kept the original output when redirecting to a file. however, this makes `findbigkeys()` and `findhotkeys()` a bit messy. i guess that's ok as we should see it from the user point of view.",0,0,0,0.9430317878723145,0.9321406483650208,0.5953593254089355,0.0,accept,unanimous_agreement
2049063856,12826,"thanks, i'm now busy in other place, come back later.",1,1,1,0.5099887847900391,0.5865201950073242,0.8648917078971863,1.0,accept,unanimous_agreement
862064585,8974,"i must say i expected better results. i see your table contains the value size, total db used memory, and peak cow with and without the commit. can you add these measurements: * the final cow * the average cow * duration of the fork * ops / sec of the write traffic to save time, let's just do that for one of the rows, not all. some random thoughts: * i suspect that the fork duration is long, and the ops/sec is very high too, so we reach the peak right after the fork starts (before serializing much data). * maybe it is better to test this when both the master and replica use diskless replication (so that the fork is not disk-bound) * maybe the ops / sec throughput is not realistic and we'll get far better results with a more realistic (slower) write throughput. * which tool did you use to generate the traffic? was it using uniform random access or sequential? maybe memtier_benchmark with --key-pattern=g:g (gaussian standard distribution is more correct)",0,0,1,0.817190945148468,0.933769166469574,0.4926395416259765,0.0,accept,majority_agreement
862975856,8974,"yes, i agree, i think in usual realistic cases, write traffic is not always so high, i used `redis-benchmark` to write redis, it makes keys changed quickly. in my tests, i didn't find this commit increase dump time on disk, maybe my disk is slow, as you said, i also need to test for diskless replication. for traffic, i will try to use `memtier_benchmark`, thanks.",1,1,1,0.5720502138137817,0.771622896194458,0.9334214329719543,1.0,accept,unanimous_agreement
877840186,8974,"i've experimented with this branch, using memtier_benchmark and bgsave (on nvme). * i tested payloads of 512b, obviously there was no impact (can't free a single full page). * i tested payloads of 4096, impact was low since they allocate 5k chunks which which jemalloc stores in a run of 4 in 5 pages, so 2 out of 4 chunks can't release a single full page. * i tested using `free` in hope the allocator will be able to release full pages eventually. it did do some good, but considerably slowed down the bgsave. * i tested libc malloc which uses `sbrk`, on which there was no impact either. so only posting the interesting results, for 4,000 bytes payload (which are more in line of what i expected). the test created 10gb worth of data (some 2.5m keys), then called bgsave while running write-only traffic that overrides the existing keys. cow in mb. | test | final cow | peak cow | avg cow | bgsave time | ops/sec | notes | |-------|----------------|----------------|---------------|-----------------|-----------------------------|:---| | default before | 3547 | 3547 | 1849 | 10994ms | 100764| 200 clients \ | default after | 155 | 1433 | 718 |15264ms|102836 | doing writes| |slow client before| 1568|1568|791|10521ms|37489 | one client \ |slow client after|152|636|422 |15337ms|31312|doing writes |fast clients before |5079|5079|2498|10488ms|183474|200 clients with \ |fast clients after |160|2060|1240|14446ms|183002|pipeline of 10",0,0,0,0.8513336777687073,0.9705497622489928,0.949556827545166,0.0,accept,unanimous_agreement
878368163,8974,"i can't find a way to accelerate the bgsave slowdown when using madv_dontneed. but on the bright side, it appears that the slowdown only happens when it makes a positive impact. i.e: 1. calling `madvise` on all keys, doesn't slow down the bgsave if there's no write traffic modifying the pages on the parent process. 2. obviously this mechanism isn't causing any slowdown when the allocations are less than a page or not page aligned (i.e. when it doesn't end up calling `madvise`)",0,0,0,0.9048323035240172,0.9761419892311096,0.9603951573371888,0.0,accept,unanimous_agreement
879561891,8974,"the difference between oran's tests mine is caused by our different environments, my disk is sata that is very slow. we will have less earnings if disk is slow or network(diskless replication) is not fast, because, on heavy writing traffic, child process doesn't have enough time to release pages but all keys may be changed and cow already happened.",-1,0,0,0.5972276926040649,0.6186569333076477,0.9738845229148864,0.0,accept,majority_agreement
882543563,8974,"i consulted about a few concerns and here's what we concluded: 1. we think it is likely that the big savings will mostly be on string type, so a second iteration on let's say all quicklist nodes, or hash fields (dictiterator) can be wasteful (waste of time and no gain), on the other hand putting the madvise code into the rdb serialization code is ugly. one idea that came up is to add some heuristics like measuring the destination file offset before and after serializing the key, and doing that extra iteration and madvise only in case we estimate that it had potential for gain (good ratio between dictsize and the size that was serialized) 2. my paranoia about not iterating on all clients and their output buffers may be wrong (maybe it's just a few milliseconds), let's benchmark that on an extreme case (30k clients?) and see how much time it takes. even if it does take time, we can maybe apply some heuristics of looking into some global server metric to decide if that iteration is worthwhile (likely to release memory). note that unlike advising on keys / fields, which can be a waste of time if the clients aren't doing any writes, advising on client output buffers is probably worthwhile, since the clients are likely to consume these buffers while the fork is active (unless the clients are dead)",-1,-1,0,0.8908316493034363,0.7504559755325317,0.7511593699455261,-1.0,accept,majority_agreement
883409996,8974,"it seems a good idea, but there is a trouble, we can't know real size of key/value before compressing, generally, we always use lzf compression algorithms. it is safe to do extra iteration and madvise only when average field/member size is more than page size. client has 16 default static buffer, we will change this buffer even if there only are reading requests, and we also easily change some fields of client struct, that also triggers cow.",0,1,0,0.8988493084907532,0.5941280722618103,0.7591967582702637,0.0,accept,majority_agreement
883609822,8974,"regarding lzf, i know it'll not be mathematically correct, we care about the used memory, not the serialized size, but since this is just heuristics, i think it's ok. regarding clients, as i said in my last post, it could be that i was just paranoid over nothing. let's benchmark it, we will probably find that even in extreme cases, it only slows us down by a couple of milliseconds.",0,0,0,0.8283824920654297,0.9237226247787476,0.6389487981796265,0.0,accept,unanimous_agreement
885599021,8974,"should we add some tcl tests for this commit? and how to check? on my machine, i added some debug logs for dismissing six data types, and make sure child process dumping rdb is right.",0,0,0,0.9875718355178832,0.9922212958335876,0.9950225353240968,0.0,accept,unanimous_agreement
885878561,8974,"i don't think a test is needed to make sure the memory is released and cow is low, or test performance. for both of these we need to compare the results to a version with the advise disabled... the test we may wanna add is one that's makes sure all the new code is covered (reachable). usually, in our other tests, the allocations are small, so the efficiency ifs we added cause much of our code to be skipped..",0,0,0,0.9278779029846193,0.9563307166099548,0.9290180206298828,0.0,accept,unanimous_agreement
887286255,8974,server.c and object.c lcov [a link],0,0,0,0.9873600602149964,0.9935237169265748,0.9949799180030824,0.0,accept,unanimous_agreement
887431009,8974,"let's add a top comment in the new test file that explains that all it does is aim to get coverage on all the ""dismiss"" methods, and check for crashes, and dump file inconsistencies. i.e. there are not many assertions in that file, so it's not clear what it tests. are there any other things missing from this pr? if not, then i suppose what's left is a cleanup / code clarity review, to improve the readability of our work.. maybe some big comment above all the ""dismiss"" functions that explains what that that whole thing does and why.",0,0,0,0.9121478796005248,0.9542705416679382,0.9703444242477416,0.0,accept,unanimous_agreement
888198339,8974,"currently i have no new things i want to add. i add some comments, please review, feel free to ask me to modify or supplement somethings, or you change directly.",0,0,0,0.9380748867988586,0.9619089365005492,0.8976851105690002,0.0,accept,unanimous_agreement
889852049,8974,triggered full daily ci: [a link],0,0,0,0.9852478504180908,0.8278221487998962,0.9950826168060304,0.0,accept,unanimous_agreement
889872065,8974,"/core-team please approve.. interface wise, there's only one change here, a new info metric. please see the top comment for details, and the link it has to one of the other comments with a benchmark. as far as we can tell, this should not slow down the bgsave unless it is able to release cow.",0,0,0,0.938987910747528,0.8855862021446228,0.9117568731307985,0.0,accept,unanimous_agreement
889872958,8974,"can you please run another benchamrk on the latest using some complex data type, once with big members, and once with small members?",0,0,0,0.9886248111724854,0.9898703694343568,0.9949650168418884,0.0,accept,unanimous_agreement
890377092,8974,"i modified redis-benchmark, for every hash, just set one field [code block] now my testing machine has ssd disk. redis would `bgsave` since of default save config. for big member size, i used `./src/redis-benchmark -t hset -d 8000 -r 1000000 -n 30000000` to write redis, the result is similar with `string` type, current solution can reduce cow without increasing much time. | test | final cow| peak cow | average cow |bgsave time | qps | ---- | ---- | ----|---- | ---- | ----| | release | 71 mb | 2726 mb|972 mb| 44s | 58632| | not release | 7048 mb | 7048 mb|4647 mb| 43s |55952| for small member size, i used `./src/redis-benchmark -t hset -d 1000 -r 8000000 -n 50000000` to write redis, there is no obvious difference since of no release actually. | test | final cow| peak cow | average cow |bgsave time | qps | ---- | ---- | ----|---- | ---- | ----| | release | 7530 mb | 7530 mb| 5021 mb| 57s | 77370| | not release | 7584 mb | 7584 mb|4915 mb| 57s |76648|",0,0,0,0.9377418160438538,0.9765154123306274,0.976211965084076,0.0,accept,unanimous_agreement
890998197,8974,"i am not sure, afaik, we prefer not to add more configs if possible. if systems where madvise is particularly slow, adding a config makes sense.",0,0,0,0.8531038761138916,0.6150482296943665,0.832278311252594,0.0,accept,unanimous_agreement
891001971,8974,"i think releasing it in a major version with all the release candidates and careful upgrades is enough. it's completely non-user visible, and unlike `sanitize-dump-payload` we're not aware of any significant regression.",0,0,0,0.9597857594490052,0.9548256993293762,0.9712488055229188,0.0,accept,unanimous_agreement
891474678,8974,"it absolutely is useful for dismissing replication backlog and client buffer even there only are reading traffic. for dismiss object, that really depend on the data traffic of users, it is hard to evaluate earnings. our operational system usually found big string, or big fields/members, but the possibility is small since we may ask them to optimize big key. in current pr, it won't cost time to iterate if there is no big members of complex data type. for list type, maybe it is also helpfully, `list-max-ziplist-size` is -2 by default, means max size is 8k of internal list node, and page size is 4k generally.",0,0,0,0.5967987775802612,0.9742963910102844,0.8544094562530518,0.0,accept,unanimous_agreement
891548695,8974,"sure, i suppose i should rephrase my comment as there is only upside with this change. i think a lot of workloads won't see a big improvement, but it's a great improvement none the less.",1,0,1,0.6497838497161865,0.5022822618484497,0.9526249766349792,1.0,accept,majority_agreement
892938024,8974,merged :tada: thank you for implementing and testing it (so many years after i had this idea it became a reality).,1,1,1,0.9408680200576782,0.9413453340530396,0.9891873002052308,1.0,accept,unanimous_agreement
893121452,8974,"welcome, it is my pleasure, truly thank you for bringing this idea, and we also did more optimizations in our extensive discussion. actually, for me, one of the biggest problems of redis always is **memory**, redis suddenly each too much memory in some cases such as memory fragmentation, cow, client output buffer, dict rehash, we must reserve more memory to guarantee stability of service that is expensive. now we gradually optimize them or mitigate influence of them one by one, i am really really excited, cheers :party_popper:",1,1,1,0.98947411775589,0.995630979537964,0.9956477284431458,1.0,accept,unanimous_agreement
893122068,8974,"btw, less average cow and short duration time of peak cow also make senses, i think, since that may reduce the performance loss duration time if we enable swap/zram.",0,0,0,0.9719050526618958,0.9873124957084656,0.985537588596344,0.0,accept,unanimous_agreement
893171120,8974,"i don't think i understand your last post, can you explain with more detail?",0,0,0,0.7400181889533997,0.7067775130271912,0.5920581817626953,0.0,accept,unanimous_agreement
893201944,8974,"although we already reserver more than `maxmemory` memory for redis, in some cases, we still may enable swap/zram mechanism of os to avoid oom since redis may suddenly cost enormous memory. in our current solution, the fork child used private memory increases first and then decreases, and peak memory duration time is short, so i think, os would gradually reduce swap memory when child starts to reduce used private memory. moreover, if there are several redis instance on one physical machine, they share the total memory, less average cow and short duration time of peak cow may would lessen the possibility that peak cow memory of them happen at the same time. my thoughts",0,0,0,0.944025218486786,0.9803712368011476,0.9859180450439452,0.0,accept,unanimous_agreement
893229530,8974,"ohh, ok.. so basically saying that this pr is great!. 8-) and also that measuring the peak is not enough, and we can also measure the peak time, but that's hard to define, so maybe we need to add the average.",1,1,1,0.9903267025947572,0.9958521127700806,0.9852038621902466,1.0,accept,unanimous_agreement
968671192,9780,"/core-team please review / approve and state what's missing or needs a change. there are a few additional minor changes that should be done on this pr before being merged, but i think the majority of the additional changes can come in a few followup prs.",0,0,0,0.9760465025901794,0.9882543087005616,0.973067343235016,0.0,accept,unanimous_agreement
970207096,9780,"i noticed the functionsctxgetcurrent gets a global value. does it make sense that it could live within the redisdb struct? same style as slots_to_keys, that was moved there recently. reason for that is that passing around, initializing, swapping and destroying server.db and tempdb + related objects is cleaner and more explicit when the related objects come together (at least things that share same life cycle). methods like functionsctxswapwithcurrent would be called or have the logic in the main function that swaps whole db.",0,0,0,0.9768303632736206,0.9896129965782166,0.9782333374023438,0.0,accept,unanimous_agreement
970219703,9780,"the problem is that functions are global (not per db, in a multi-db server). unlike the cluster slots mapping, which is global, but only valid in a single db configuration. that's why we had to create `rdbloadingctx`, if we had that one before, maybe we would have put the slots mapping there too. note, i did review this code before being posted publicly, and did attempt to follow the cluster slots footsteps as much as i could. if you still think there's a possibility for an additional cleanup, please suggest it.",0,0,0,0.9787819981575012,0.985490083694458,0.9429588317871094,0.0,accept,unanimous_agreement
977713963,9780,thanks for the review. fixed most of the comments (pushed a new commit) and i added comments where i needed more input about how to proceed. regarding pre-load of functions (not from persistence). this is listed as first task to handle on following pr's. i am working on the design and will share in a few days. once you (the core team) will approve the design i will work on the implementation. regarding this [a link] i have another design coming to allow better code sharing and i hope to address it (or at least suggest a possible solution). will share soon.,1,1,1,0.9532672762870787,0.9884605407714844,0.9711022973060608,1.0,accept,unanimous_agreement
980440116,9780,"my point is that i'm not convinced rdb is the right place to be storing functions, or at least we should take a deeper look into the rdb storage to think about how it interacts with other features. one example is cluster mode, when you want to split a cluster, you add n new nodes with the same configs of the other nodes and then migrate the keyspace data off for some slices. this won't include functions until we add them to configs, but then you run into the issue where they need to only be loaded on primaries and replicated (or perhaps we allow primaries and replicas to diverge?). that's why i want to understand why it's not being built more analogous to the config file or to acls. the only real benefit here is that storing it in the rdb is actually really convenient (since you can just restore the rdb to another node). but it would also be convenient for acls, so maybe the right answer is we should be storing them both in the rdb. (also, i didn't actually review the code but i assume we replicate by the effects of the script and not the script itself. if that isn't the case, then having replica and primary agree on the state of a function matter more) i suppose i would want an answer to this before i would signoff on it.",0,0,0,0.9352761507034302,0.8594610691070557,0.9039486050605774,0.0,accept,unanimous_agreement
981049396,9780,"the entire point of functions is that they are replicated and persisted so you will have them on fail-overs for example (this is also how we defined it in the issue [a link] i do agree though that we need a way to allow users to also load them from config in case the user runs without persistence for example. hope to publish my design for that today/tomorrow and we will be able to discuss it. yes, on functions we replicate only by effect.",0,1,0,0.7798468470573425,0.8876979351043701,0.958871841430664,0.0,accept,majority_agreement
981115848,9780,"agree about having them available on failover, if they were stored in a config file they would also be available though.",0,0,0,0.9810753464698792,0.990151584148407,0.986278235912323,0.0,accept,unanimous_agreement
982620284,9780,"for the record, we discussed the persistence issue in a core-team meeting today, we concluded that unlike acl (which is more an admin related feature), functions are an application related feature, and should be persisted together with the data. we do need to resolve the problem with redis cluster in some way (possibly propagating them on the cluster bus, or adding some tools that will allow cluster admins to easily load them on all nodes), we will look into that in a followup pr.",0,0,0,0.9766317009925842,0.9923719167709352,0.9815099239349364,0.0,accept,unanimous_agreement
982921969,9780,"there was one other point in the meeting last night that was mentioned that i forgot to circle back to, which was the ephemeral use cases. like, spinning up a cache but not using replication, we should make it easy for functions to be available there. it was mentioned that this is not common, but i disagree and think that use case is extremely common. i also think clusterbus propagation of functions isn't a redis 7 feature, so we should aim for something that solves cluster + ephemeral workloads, which is probably just passing in a startup config and maybe some rewrite functionality (or listing all the functions with their descriptions) so they can be loaded on a new cluster node. we don't have to implement this other mechanism, but i would like to understand how it will work before we merge this pr.",0,0,0,0.6000181436538696,0.92510724067688,0.9499409198760986,0.0,accept,unanimous_agreement
984104896,9780,"squashed all reviews commits to their relevant location, kept only the original 5 commits.",0,0,0,0.9779727458953856,0.9889413714408876,0.99041348695755,0.0,accept,unanimous_agreement
984430577,9780,triggered full ci [a link],0,0,0,0.9888983368873596,0.9780800342559814,0.995606243610382,0.0,accept,unanimous_agreement
986295969,9780,loading functions on cluster and/or startup time is discussed here: [a link],0,0,0,0.9867042303085328,0.985233724117279,0.9953086972236632,0.0,accept,unanimous_agreement
987068867,9780,code sharing on functions is discussed here: [a link],0,0,0,0.9868749976158142,0.986282765865326,0.9953697323799132,0.0,accept,unanimous_agreement
895924782,9309,"redismoduleuserid added. another q, do we need to log acl errors (addacllogentry) in the context of a module (the 3 new api's and rm_call with 'c')?",0,0,0,0.9900954961776732,0.9935779571533204,0.9940286874771118,0.0,accept,unanimous_agreement
897502002,9309,"i think that maybe it's the responsibility of the module to log these explicitly (i.e. we need to add an api). maybe in case the module pass a `c` flag to rm_call, it could be done automatically, but the other 3 apis shouldn't do that automatically, and instead the module may decide to call a 4th api. wdyt?",0,0,0,0.9861313104629515,0.9510894417762756,0.9841398000717164,0.0,accept,unanimous_agreement
897577036,9309,"i agree, acls in modules are already explicit so logging should be the same. i think it makes sense to make the `c` flag an exception, as this already serves as a simplified alternative.",0,0,0,0.9821380376815796,0.9529885053634644,0.9789551496505736,0.0,accept,unanimous_agreement
913124665,9309,/core-team please approve the new module apis.,0,0,0,0.98447185754776,0.9867373108863832,0.9880940318107604,0.0,accept,unanimous_agreement
913910691,9309,"i assume the acl code all still works, and not really going to look into the refactoring that much. i looked through all the apis, and had a couple of comments.",0,0,0,0.9819926023483276,0.9539150595664978,0.978888988494873,0.0,accept,unanimous_agreement
917889966,9309,"that's an an interesting point, but i'm not certain it's a problem or what's the solution. the design is that as soon as acl allows the module command to run, it can do whatever it wants, and we added these apis for modules that do complex things (like doing a series of rm_calls evaluating a script?) so if a module is implementing a certain command that uses rm_open (not rm_call), which command name should it pass to such a check? maybe a valid use case for that is this. imagine a module with a command like `mod.compute` that takes a single argument like `key1+key2/key3`. the module command then breaks that single argument, and extracts key names from it, so it can't support the rm_iskeyspositionrequest thing. then imagine an acl rule that says that a certain user can use the mod.compute command but only on certain key names. then using rm_aclcheckkeypermissions with mod.compute as command name makes sense?",0,0,0,0.9033011198043824,0.9536694288253784,0.8808937072753906,0.0,accept,unanimous_agreement
918644152,9309,"a little bit of a tangent, but reading this conversation makes me wonder if the direction of acl v2 is maybe not right. the initial version of acls gave a user access to commands and access to keys, and a user needed access to both to execute a command. this is limiting. i've heard several of our customers say that they want ""read access to ~foo* and write access to ~bar*"". read/write has nothing really to do with the underlying commands a user can execute though, what they are really asking for is control over how the data can be accessed. so maybe what we really want to do is decompose access to keys into read/write. to use `get foo`, you need read access to foo. to call `set foo bar`, you need write foo. to call `append foo bar`, you need read and write access to foo. this fits more cleanly into the module api context being described. a module wants to ask the question, ""can the user calling me read from key x and then write to key y"", and if so, they can go and execute the command.",-1,-1,0,0.9075645208358764,0.613232433795929,0.6555931568145752,-1.0,accept,majority_agreement
918859221,9309,"this is an interesting observation. good thing you caught it before we merged both of these prs. it actually sits nicely with the read and write flags we intend to add to the key-specs: [a link] so this is really a last minute change. to adjust the api in this pr for this idea, we only need to add a read/write flag to rm_aclcheckkeypermissions (instead of the command name i suggested to add in my previous comment). however, adjusting the acl roles pr will be a lot of work, right?",1,1,1,0.8790521025657654,0.9299387335777284,0.9336094856262208,1.0,accept,unanimous_agreement
919941645,9309,"are you suggesting to completely drop command reference from acls? the classic access control model deals with subjects, objects and operations. to me, using read/write is just another way simplified way to describe operations, which can be considered complementary to commands. i definitely see how it's going to be super useful, but i'm not sure it can substitute commands for a few reasons: * backwards compatibility * restriction of administrative commands (e.g. `flushall`) * other nuances - e.g. allow write but disallow `del`, append-only lists, etc.",0,0,0,0.9141727089881896,0.9522054195404052,0.9701384902000428,0.0,accept,unanimous_agreement
920078122,9309,"since this pr is applied on what's currently in unstable, i suggest to keep that api as is (no command name), and merge it. i.e. that's the right api for aclv1. then in the pr that adds roles, we can modify that api and add another argument (since it wasn't released yet). i.e. we'll discuss roles / aclv2 in the other pr, and do any necessary adjustments.",0,0,0,0.9863479137420654,0.9920733571052552,0.9900339245796204,0.0,accept,unanimous_agreement
920333589,9309,"i agree with oran that we should close on the acl v2 path, and we can decide if this api doesn't make sense at that point. going to respond here just for continuity, but also going to paste the same comment on the main acl thread. i'm not saying we should drop commands, i'm saying that instead of using policies/complex access controls we instead make more complex ways to describe access to keys. an acl v2 access control might instead look like: [code block] where (~ describes the key spaces that have readonly access, )~ describes key spaces that have write only access, and ~ describes keyspaces that have both read and write access. in this model, a user would be able to execute: [code block] but not [code block] the thinking is that it might be easier for end users to reason about key access as opposed to what operations can be accessed on which keys.",0,0,0,0.9526740908622742,0.9764675498008728,0.9571579694747924,0.0,accept,unanimous_agreement
922480414,9309,"following the comment of (thanks) about that the new api's will fail against users created by a module (rm_createmoduleuser()), i had a discussion with oran and yossi and we decide to get rid of the redismoduleuserid. all the new api's will get a redismoduleuser as an input. for a user that the module created, the redismoduleuser already exists as a return value of rm_createmoduleuser. but for a general acl user, the module will obtain it by two new api's: rm_getcurrentusername - retrieve the user name of the client connection behind the current context. rm_getmoduleuserfromusername - get a redismoduleuser from a user name in the case of general acl user (rm_getmoduleuserfromusername), the underlying user pointer in redismoduleuser, is a reference for an existing acl user from the global acl users list (users), and thus can be used only in the context where the redismoduleuser obtained (the acl user can be deleted at any time). another point is that, now that a module can get redismoduleuser also for a general acl user, he can use it with the already existing api rm_setmoduleuseracl(), to change the rules of such user. with all these changes, please take a second look",0,0,0,0.801353931427002,0.8080651760101318,0.775261640548706,0.0,accept,unanimous_agreement
922644519,9309,triggered daily ci on unit/acl and moduleapi: [a link] (passed),0,0,0,0.9882806539535522,0.9810536503791808,0.9948346614837646,0.0,accept,unanimous_agreement
922984054,9309,i just realized that we already have [code block] so i guess this makes `rm_getcurrentusername` excessive.,0,0,0,0.788066029548645,0.9808053970336914,0.9899951219558716,0.0,accept,unanimous_agreement
924360230,9309,"having rm_getcurrentusername seems fine to me, it seems like enough of convenience function and it's a primitive we want to expose.",0,0,0,0.972318947315216,0.6587361097335815,0.9862455129623412,0.0,accept,unanimous_agreement
993295537,9940,"[code block] using the module above, quick bench on my local : memtier_benchmark -t 2 --command=helloblockperf.hello before pr : 193187 ops/sec, 0.52 msec latency after pr : 245091 ops/sec, 0.41 msec latency",0,0,0,0.9797173142433168,0.9877790808677672,0.9911310076713562,0.0,accept,unanimous_agreement
997641908,9940,"regarding your comments, created a generic function `clearclient()` to clear the client state. hopefully it does it properly, please take a look. also, we have couple of other temp client usages in module.c (`modulefreecontextreusedclient` and `server.module_client`) . i've changed these to use same temp client cache. pros : single place to handle temp clients, simplifies code a bit, probably more secure as we clear it properly. cons: a bit more work to alloc/release temp client compared to current version. let me know if this makes sense or if you think no need for that?",0,1,1,0.8808501958847046,0.8856495022773743,0.6736742854118347,1.0,accept,majority_agreement
1000112782,9940,"maybe i am wrong but from what i understand we need the mutex on the client pool only because it is used from within the rm_getthreadsafecontext which can be called without the redis gil. maybe we can set the client on the redismoduleblockclient when we create it (which must be when redis gil is takes) and then just use it when we create the threadsafectx. in case the threadsafectx is created without blockedclient we can just create a client for it (and not use the pool), i guess that in such case the threadsafectx can be (and should be) reused by the module anyway no? let me know what you think.",0,0,0,0.9132577776908876,0.98749440908432,0.9536621570587158,0.0,accept,unanimous_agreement
1000122945,9940,"iiuc, you are suggesting allocating another client instance on `redismoduleblockclient()` to be used for a potential `rm_getthreadsafecontext()` call. i asked this but looks like `rm_getthreadsafecontext()` can be called multiple times. at least, this is not strictly prohibited? i don't know if anybody is doing this though.",0,0,0,0.9791339635849,0.9621875286102296,0.986488401889801,0.0,accept,unanimous_agreement
1000128740,9940,"i believe that if its created multiple time than we can allocate the client (and not use the pool), i guess we want to optimise the common use-case and i believe that creating multiple threadsafectx from the same blockedclient is supper rare (maybe not even exits?)",0,0,0,0.8433910608291626,0.9749895334243774,0.9751655459403992,0.0,accept,unanimous_agreement
1000209433,9940,"we'll need an atomic variable to detect multiple threadsafectxs if i'm not mistaken. i feel like current cache with mutex will be uncontended and should perform similar to this change. (maybe i'm wrong but i don't expect contention on the mutex). just another question, similar to your suggestion or same : [a link] for threadsafectxs of blocked clients, we are always returning `ctx->blocked_client->reply_client`. so, we can assume one cannot create multiple threadsafectxs and use them in different threads etc. (sorry my previous reply says the opposite). it can be created multiple times in the same thread, it is okay. [code block] we can make this change. if bc is not null, we can use `ctx->blocked_client` as `ctx->client`, no need for a new client as replywithxxx api always return bc->client anyway. in this way, client object cache will be local to redis main thread, no need for mutexes. is this correct?",0,-1,-1,0.8256605267524719,0.9602378606796264,0.9157968759536744,-1.0,accept,majority_agreement
1000217377,9940,"so iiuc `bc->client` is the actual client that sends the command so with your change, if inside the thread we will use `redismodule_selectdb` with the `threadsafectx` generated with the `blockclient`, we will change the db the original client works with (this is not what would happened today so i guess it can be considered a breaking change?). but i do agree with your claim that creating threadsafectx from 2 threads simultaneously is not safe even today so my opinion is that it should not be done and then we can go with the original suggestion? (putting another client on the blockedclient and use it?) wdyt?",0,0,0,0.8723995089530945,0.981964111328125,0.9905318021774292,0.0,accept,unanimous_agreement
1000232369,9940,"hmm, so people can call functions other than `replywith..` :( yeah, your suggestion is the best option if we can assume `getthreadsafecontext()` is not multi thread-safe. it can be called/used with a single thread only, not many.",-1,-1,-1,0.8329638242721558,0.9911518692970276,0.9951785802841188,-1.0,accept,unanimous_agreement
1000270684,9940,"haven't followed this discussion, but had a short talk to meir for an update. what i understand is: 1. getthreadsafecontext can be used by multiple threads in parallel, and it's likely to assume each thread will create just one, or maybe a few at startup, so their creation may not be expensive compared to the creation of the thread (so maybe there's no need for the pool here?). 2. getthreadsafecontext on a specific blocked client, is only allowed from one thread (per blocked client) 3. blocked client can be used without a thread (replying from the unblocked callback after being released by another command. 4. we don't wanna lock another mutex in the main thread (while the gil is locked) when we do rm_call. 5. maybe the way around it is that we pre-""allocate"" a fake client (from the pool) when we create the blocked client (rather than do that when we create the thread safe context, this way we always take things from the pool when the gil is locked, so there's no need for that mutex. did i get this right?",0,0,0,0.9532254338264464,0.9869891405105592,0.9518484473228456,0.0,accept,unanimous_agreement
1000372648,9940,"yeah, seems correct. the question is case 2. user can call getthreadsafecontext() from different threads for the same blocked client. it just won't work as there is no proper synchronization for these threads (all threads will use same client object without a mutex). so, currently, assumption is getthreadsafecontext() should not be called from the different threads for the same blocked client. so, if we are okay with this assumption, we can implement case 5.",0,0,0,0.9673630595207214,0.9737977981567384,0.8897104859352112,0.0,accept,unanimous_agreement
1000421467,9940,"i think we're ok with this assumption, and we can clear that up in documentation if we want.. modules can do worse thing if they want, we don't attempt to block them from doing silly things, except for cases that are really slippery. (where common sense will lead you the wrong way)",0,0,0,0.5574749112129211,0.7858570218086243,0.7278368473052979,0.0,accept,unanimous_agreement
1000430387,9940,"ok, i'll update the pr",0,0,0,0.9811581969261168,0.9733399748802184,0.9946369528770448,0.0,accept,unanimous_agreement
1001098733,9940,"rebased and updated the pr, rm_getthreadsafectx() will now use a temp client from blocked client.",0,0,0,0.9889274835586548,0.9935896396636964,0.9948787689208984,0.0,accept,unanimous_agreement
1009759291,9940,"changed code a bit to free clients faster(i'm open to suggestions if you have a better idea). there is now `moduletempclientmincount` which is the number of unused clients in the pool for a cron period. still, even all these clients are unused, i'd like to keep at least a few clients in the pool if this is okay. if requests are coming with an interval, we'll need to allocate clients, it may hurt latency. currently, min count is 16, i can lower it to 8 or 4 if this is too many.",0,0,0,0.945411503314972,0.9786002039909364,0.96888667345047,0.0,accept,unanimous_agreement
1239762568,11248,did you consider the alternative of using `o_direct` so written data doesn't occupy the buffer cache in the first place?,0,0,0,0.9878771305084229,0.9955892562866212,0.9945106506347656,0.0,accept,unanimous_agreement
1240986256,11248,"yes i discussed this problem with the guys in my company before the pr, our thoughts is direct io is somehow more complex for use, you must align the buffer and file offset, and fill the buffer to the multiple of sector size, but the performance may be even poorer than buffered io, for eliminating the page cache `fadvise` may be a more easy way. i also find the default option for mysql innodb and rocksdb to write log file is also with plain buffered write. i simply tested the two modes in my dev ec with fio(but not garantee the parameters matching the workload accurately since i dont have much time to dive into it...) [code block] the result shows the buffered write is more faster.",0,0,0,0.8469322323799133,0.9342515468597412,0.9492682814598083,0.0,accept,unanimous_agreement
1246139695,11248,"hi i'm quite familiar with the linux i/o subsystem. in my experience, `o_direct` is not suitable for redis: * during dumping into db file, redis iterates all the kv and write the kv into db file one by one, this means redis call lots of write syscall. i also tested this by `strace -ff -p pid` during `bgsave`, the test result also confirms this. * the typical iops of 4k random write: hdd 150(avg lat 10ms), ssd 50k(agv lat 200us), nvme 100k(agv lat 20us), this means that direct io slows down the speed of writing db file, especially on a hdd. * to improve the performance of this scenario, using buffered io(page cache, but not buffer cache, they are different on linux) is suitable. also to reduce the page cache by `fadvise` is a good choice, this is better than kernel reclaims. because a user process knows which file(or part of file) will not be used recently, but the kernel has to scan all the pages of cache by lru. so from the point of my view, this patch is reasonable.",1,0,0,0.7490979433059692,0.91241055727005,0.9322717189788818,0.0,accept,majority_agreement
1246261074,11248,"thank you for your informations! i have seen your post in the forum of bytedance, and heard your speech about our works on `htop`, i'm a big fan of you aha.",1,1,1,0.9938511848449708,0.9922271966934204,0.9966816306114196,1.0,accept,unanimous_agreement
1246282955,11248,"about your comments i also want to add some solution/ideas: - we can buffer the kv into in the user space buffer, and `write` data to kernel when buffer is full, with this we could reduce the frequency of `write`, but i'm suspicious even so the direct io is slower than the normal io as the `fio` test shows. - rdb(or aof) need to guarantee the duration of the data, so a `fsync` is needed with either direct io or buffered io, so the cost flushing data to disk maybe is unavoidable... - i can't agree more, this is the main reason of the patch. we found a performance thrash when the memory is scarce and kernel reclaim thread start to work in out production environment(a bare metal machine with tens of redis instances deployed on it). i don't figure out the details why kernel reclaiming memory would slow down the service, but i think with this patch we could reduce the memory kernel will scan and reclaim.",0,0,0,0.8392931818962097,0.8164267539978027,0.8831743597984314,0.0,accept,unanimous_agreement
1246367531,11248,"because the kernel maintains memory zones, and there is a `watermark` of each memory zone, when the *free memory* gets less than the watermark, the memory allocation on this zone needs to wait reclaim done. once a redis process hits page fault and allocates page, it has to be uninterruptable state(d state in `ps aux`), wait the memory reclaim by kswapd, then continue to run. this is the reason of latency spike. using `fadivse` to reclaim page cache, this syscall gives a hint to kernel to drop some page cache. it's helpful to keep the free memory more than watermark to avoid kernel memory reclaim.",0,0,0,0.9702238440513612,0.9836632013320924,0.9867908358573914,0.0,accept,unanimous_agreement
1350418697,11248,"add more context. the page cache issues may happen in old kernel like 3.10, before which [a link] is not involved. as the space between low watermark and min watermark is quite small, usually kswapd is too late to wake up before trigger a direct reclaim, which would stall the whole memory allocating process. this can be observe more obviously via /proc/vmstat, `allocstall_normal` has a significant increment. as for the more modern kernel, like 4.18, this will not be a problem. low watermark is far away from min watermark so kswapd can work as expect to avoid direct reclaim. we find this problem when we try to upgrade the redis cluster, with the rdb generated, transformed and persisted, which occupy lots of page cache, the `memfree` exhausts. this pr could not resolve the direct reclaim issue ultimately in old kernel, it only slows down the process. a side effect is it can make `memfree` looks more, and offload the work of kswapd. if this proposal is adopted, maybe aof cache should also be addressed, which this pr doesn't cover.",0,0,0,0.9670343399047852,0.988452136516571,0.9870085120201112,0.0,accept,unanimous_agreement
1351785199,11248,"yes, we noticed that in newer kernels the stall involved by direct reclaim doesn't occur in our production environment, as [a link] would guarantee kswapd begin to work before the free memory reaching low watermark and direct reclaim. but i think it still have some values in newer kernels, firstly the program knows better which cache should be evicted, in contrary kswapd maintains two lru lists for all pages, and scan them to find out which page could be reclaimed, so active reclaim is more efficient than passive reclaim in theory. secondly, it can make more `free` and less `cache` in memory stats like `free` command or `/proc/meminfo`(though i'm not sure it's value..) in my test `fadvise` cannot completely guarantee the cache backed by file is evicted, it's a hint to kernel. besides, there may be other processes in the host machine which produce cache, when cache grow up and free memory exhausts, the stall still will happen finally.",0,0,0,0.9451906681060792,0.9851476550102234,0.9851326942443848,0.0,accept,unanimous_agreement
1352449148,11248,"i suppose that this pr has no dependence on `watermark_scale_factor`(but yes, this kernel parameter help to keep more free memory), so let's focus on `fadvise` only. `fadvise` is a hint(not a command), we can tell the kernel to try to reclaim the specific page cache(because we know which file we will *not* access recently) rather than kernel scans the full lru blendly. this leads more effective memory reclaim.",0,0,0,0.9616875052452089,0.9876232147216796,0.9913151860237122,0.0,accept,unanimous_agreement
1352632338,11248,"so in that light. what do you think about my comments to incrementally advise (free pages) as we go rather than at the end? i remember an old project i worked on (kernel 2.6.17 with not a lot of ram), i had to fadvise incrementally while copying large files in order to avoid disruptions (executable code being evicted from page cache). but maybe if in our case, the excess system calls would cause more harm than good.",0,0,0,0.9393222332000732,0.8352794647216797,0.9580039381980896,0.0,accept,unanimous_agreement
1352663805,11248,"from the point of my view, your suggestion incrementally advise (free pages) seems better. in my experience, i optimized a log library several years ago: use fadvise to reclaim page cache every 1m, it works fine until today. if so, don't worry about the performance of the handful additional syscalls. (a single cpu can do 1m+ syscalls per second on a modern x86 server)",0,0,0,0.7887408137321472,0.9075350761413574,0.6696059703826904,0.0,accept,unanimous_agreement
1353313922,11248,"i also agree the incremental fadvise, in my implementation i found the fadvise in one go may block for some time if the file is huge enough, so i put it into bio, but with incremental fadvise we could amortize the work to many fadvise, so no background fadvise is needed, it's pretty cool! i tried to find some inspiration in rocksdb just now, rocksdb also range_sync and invalidate cache every 1mb. i find the way sync_file_range used in rocksdb seems more clear and proper over the one in our current implementation, so i wanna port it in next pr as a following of #11150",1,1,1,0.9753485321998596,0.992691457271576,0.9916846752166748,1.0,accept,unanimous_agreement
1353394970,11248,"oops... it seems like background fadvise is required. the incremental reclaim is feasible for write cache, but a little messy for read cache, so i would rather do it in one go in the background. btw i have solved all the comments, thanks for the suggestions!",1,1,1,0.9351436495780944,0.9611068367958068,0.9553009867668152,1.0,accept,unanimous_agreement
1402311535,11248,"i think the suggestion is fair, and i add a github ci action to test it, but i'm not familiar with it, so please help me have a review!",0,1,1,0.6406676173210144,0.9016219973564148,0.9097257256507874,1.0,accept,majority_agreement
1404868413,11248,"i'm sorry if i wasn't clear, i don't think we need a test for the `fadvise` detection, but for the actual implementation.",-1,-1,-1,0.9867171049118042,0.9917769432067872,0.9852303862571716,-1.0,accept,unanimous_agreement
1407592178,11248,"please avoid force-pushes, it makes it harder to do incremental review. since we're gonna squash-merge this into one command, it's ok to just add more and more commits instead of amending existing ones. it also looks like github doesn't wanna tell me what's the last revision i reviewed, can you please tell me what's new?",0,0,0,0.8649510145187378,0.9589617252349854,0.8793159127235413,0.0,accept,unanimous_agreement
1407606369,11248,"i didn't rewrite the historical commit, i just `git rebase origin/unstable` to solve the conflict between this branch and the trunk, but github would rewrite the timeline and regard all commits as newly commited ones... [a link] is the new one.",0,0,0,0.9701045751571656,0.987686812877655,0.9900425672531128,0.0,accept,unanimous_agreement
1411834381,11248,"yes, rebase always requires a force-push, in this case, you should use merge, like `git merge origin/unstable`",0,0,0,0.9867222309112548,0.9946677684783936,0.9942017197608948,0.0,accept,unanimous_agreement
1411845076,11248,"in your redis fork, you should have an ""actions"" tab where you'll be able to ""dispatch"" the daily workflow with some filters. the other alternative, is to just modify ci.yml temporarily (it has the `push` trigger), push into a temporary branch and watch the actions there. let me know if you have some difficulties with this and i'll help.",0,0,0,0.9451143145561218,0.935487985610962,0.986530303955078,0.0,accept,unanimous_agreement
1420725762,11248,"-binbin thanks for you explanation. `git rebase` is the convention in our company, but seems it's not compatible to github's pull. i finally figure out the test in github action, it's quite hard to debug... after tens of fix-gitpush-trigger-checklog circles, i finally got a relatively stable version. it involves three cases: 1. rdb save 2. replication 3. restart, and check the cache backed by rdb files, as well as the overall cache in the host. by now i generate about 1gb data, and the check threshold for overall cache is 500kb.",1,1,1,0.8518880605697632,0.9137768745422364,0.9027777314186096,1.0,accept,unanimous_agreement
1422181231,11248,"can you please update the top comment with details on everything we ended up doing (i.e. in which cases we reclaim gradually, in which we reclaim at the end). and also any interesting things we can mention about the implementation and certain concerns (like platform support, logging, don't remember what else)..",0,0,0,0.957553505897522,0.9845759868621826,0.96634441614151,0.0,accept,unanimous_agreement
1435950573,11248,"i made a pr to stabilize this test, please take a look [a link]",0,0,0,0.9511538147926332,0.928215265274048,0.9916683435440063,0.0,accept,unanimous_agreement
776367951,8474,"/core-team ok, probably time for a wider review. nan outlined the two major additions in the top comment. the first one is more should be mostly agreed upon. the second item is just a mechanism to test that the absolute timestamp is happening correctly, the current implementation minimizes the amount of code.",0,0,0,0.977102756500244,0.974157989025116,0.9128810167312622,0.0,accept,unanimous_agreement
776921884,8474,"i'm sorry, i didn't follow the discussion in this pr and issue (wasn't aware of them). since this goes against past decision, i would like to study both the correspondence here (didn't read it yet), and the past discussions in other issues and prs (where salvatore responded). however i'm currently a bit short in time, and and in any case i'm not sure if we wanna merge such a change to 6.2 (already past it's last rc). since this is not an urgent change, i wanna propose that we'll hold this for a future version. does this make sense? i might be overthinking of this, but why take the risk?",-1,-1,-1,0.9867361187934875,0.9803876876831056,0.972551167011261,-1.0,accept,unanimous_agreement
777145941,8474,", i was under the impression this would not be in 6.2 given that we said only bug fixes for the last rc. you can take as much time as you need to think it through :)",1,1,1,0.788730800151825,0.9833848476409912,0.9357074499130248,1.0,accept,unanimous_agreement
797827051,8474,"i just read through the other related issues on this topic #8433 and #5171. i am not entirely convinced about the original argument that salvatore was making that the ttl should be the relative time since the moment key was written onto the redis server which is subject to the replication delay on the replica. from the point of view of the redis user, i am writing a key with an ttl onto the master, and the expectation is that the key should be expired after that time duration from this moment on. so i also lean towards having the replica attempt to expire the item at the same point in time as the master regardless of the replication lag between master and replica. i just looked at this pr from and i think the implementation looks good mostly. i raised a few minor comments for him namely around introducing the new `expiretime` command. please take a look. thanks!",0,0,0,0.5798600316047668,0.8824050426483154,0.963639795780182,0.0,accept,unanimous_agreement
800922841,8474,"sorry for the delay, also sorry to be the last to join the party. tl:dr i'm voting in favor of this pr (didn't read the code yet). the main reason is that we're trying to fight two contradicting issues here: 1. a significant clock de-sync between master and replica. i'm not sure such a thing really exists, and if there is, there are ways to fix it (ntp and others). 2. significant replication delays (either due to psync reading from the backlog, or due to large replica-buffers accumulated during full sync). these are very real, and there's no easy way to mitigate them. item 2 can also be said to be affected by trivial network latency, but that's not in the same ballpark of the other two replication delay issues. on top of that there's the concern of code simplicity and consistency, and it would certainly be better to replicate the same to aof and replicas, and have the same type of (absolute) value in full-sync and replication command stream. i don't agree with some of the arguments that were posted, but the bottom line is that these two concerns contradict each other, and one of these concerns can be eliminated while the other can't (at least not easily). now that we seem to agree about this change we need to decide when it's safe to merge. if we conclude that we don't wanna release this change in 6.2.x, it would be better to leave it out of unstable for a while, so that unstable doesn't diverge too far from 6.2, making it harder to cherry pick bug fixes. ------------ footnotes: i think what matters here is actually the wall-clock of the client machine. requesting a key to expire in 3 minutes means 3 minutes since the command was sent, or since the time it was processed by the master (the client machine's wall-clock at that time). and as salvattore mentioned if the replica clock is 2 minutes ahead of master's clock, and the client is using the replica to accelerate reads, from the client's perspective, the key will appear to expire on the replica after one minute (and remain in the master for another 2). so putting aside full-sync, psync, and failovers and other complications, this is what we break with this pr. but on the bright side, i think clock offsets can and should be resolved with tech, so this should not be a real problem. if we really wanted to solve it anyway, what we can do with quite a lot of extra complexity is: 1. when the replica first connects to the master, it sends a time command before it attempts to sync (this response will be processed right away, no backlog or replica-buffers delays). this way it knows both the clock difference between them, and also the network latency. 2. when master replicates relative ttls to replicas (goes into the backlog and replica buffers which can be received with a huge delay) it replicates both the relative ttl and its current clock. this way the replica knows how to respect the client's intent (imagine it knows the client machine's wall-clock at the time the command was processed by the master). the replica will need to translate both the absolute ttls it gets from rdb or command stream, and also the relative ones (which come together with the master's clock) with it's own clock diff (add 2 mins in our example) i don't think we wanna this way...",-1,-1,-1,0.98934143781662,0.9916627407073976,0.9915830492973328,-1.0,accept,unanimous_agreement
801472429,8474,"appreciate the thoughtful comments. i agree. two things: 1. re: when it's safe to merge. i understand your argument. the counter-argument is that merging it sooner gives us a longer runway for its release - it gives us more time/opportunity to see how it meshes with the rest of redis and see if it has bugs or unforeseen consequences. 2. re: out-of-sync clock on replica, the issue you described in ""footnotes"". this is a good point. i agree this is what this pr ""breaks"". if we do care about such issues, i see at least two options that could help: (1) stop replicas from expiring(hiding) keys independently based on local clock. this is basically what madelyn was referring to in [a link] with `makes expire ""linearizable"" across the cluster`. so basically, all expiration are entirely driven by masters. replicas would keep serving ttl keys until it receives the `del` command for them from master. with this scheme, ttl will essentially become a logical-time-based concept - a ttl key’s life span is measured in terms of replication offsets. its starting offset is when a master first received the ttl. its end offset is when a master expires it. the key lives for the same span of offsets on every node, regardless how much wall time it takes each node to replicate/traverse through that span. (2) another option is like what you suggested - replicate master's local time. e.g. master would periodically send its current local time to its replicas via replication stream. upon receiving these timestamps, replicas would reset their local clock accordingly. this way, as long as time's ""velocity"" is roughly the same between master and replicas, ttl keys should live relatively the same lifetime on them. either way, even though it is related, i think it's better if we tackle this issue separately and leave it out of scope for this particular pr. i would love to hear what you think.",1,1,1,0.9630357027053832,0.7331915497779846,0.9922745823860168,1.0,accept,unanimous_agreement
850949243,8474,finally merged (not sure why it was waiting for in the last month). would you care to create a [a link]?,0,0,0,0.9853390455245972,0.9925651550292968,0.980250895023346,0.0,accept,unanimous_agreement
851029958,8474,nice. i will create a doc pr this week and reference the link here. thanks again for the reviews.,1,1,1,0.9814552664756776,0.9915117621421814,0.9922996163368224,1.0,accept,unanimous_agreement
851209486,8474,fyi: [a link] is so slow that more than 10 seconds passed from the time the command was sent to the time it was executed: #9010,0,0,0,0.9189158082008362,0.6566383838653564,0.979778528213501,0.0,accept,unanimous_agreement
851719932,8474,follow-up pr for api documentation on `expiretime` and `pexpiretime` commands: [a link] please review. thanks.,1,1,1,0.947857141494751,0.986003577709198,0.8101776838302612,1.0,accept,unanimous_agreement
852921025,8474,"i see the valgrind run [a link] despite my [a link] (to change from 10s to 100s). but more interestingly, note the timestamps (it's not as slow as i thought it is): [code block] maybe you can please look into that?",0,0,0,0.9775492548942566,0.9774978756904602,0.9901138544082642,0.0,accept,unanimous_agreement
855346434,8474,happened again: [a link] [a link],0,0,0,0.9817107319831848,0.9875265955924988,0.993315041065216,0.0,accept,unanimous_agreement
856474299,8474,i am taking a look on this,0,0,0,0.9549269080162048,0.7242442965507507,0.9087671637535096,0.0,accept,unanimous_agreement
859104257,8474,found the root cause and fixed: [a link] please take a look,0,0,0,0.9805759787559508,0.9148578643798828,0.9642489552497864,0.0,accept,unanimous_agreement
749212826,8217,"also related to oran's top level comment, we don't always use ""addreplyerror*"" for all of the errors, we need to go through and audit them.",0,0,0,0.9851173162460328,0.9869601726531982,0.9922667145729064,0.0,accept,unanimous_agreement
749305811,8217,"i believe we can see value out of both groups right? meaning the more errors we can keep track without paying a penalty the better. , following what you've mentioned i've updated the code to use the error prefix ( the first word after the ""-"", up to the first space ) as the rax key (lowercased). in that manner we have the desired implicit support for all errors. can you guys check the new commit. sample output of errorstats now: [code block]",0,0,0,0.9582488536834716,0.9524196982383728,0.95335590839386,0.0,accept,unanimous_agreement
749625679,8217,"looking at the example in your last post (didn't read the code yet), i think these two cases should fall into the standard `err` since the error code is `-err` (""wrong"" and ""auth"" are just part of the text). i.e. [code block] not [code block] if we think it is important for redis to distinguish between them, then it's also important for client libraries, and we should change the code and promote them to have their own error code.",0,0,0,0.9804938435554504,0.993293523788452,0.9882982969284058,0.0,accept,unanimous_agreement
749834459,8217,"agree. in the latest code push i assume that if no `-` is present than the error is the default `err` (same as done in `addreplyerrorlength`), otherwise we use the first after the `-` until the first space. i've also removed the error name from the error struct and i'm using the error prefix as is ( meaning no lowercase ). i believe all comments have been address. wdyt?",0,0,0,0.9717485904693604,0.9026668667793274,0.9825971722602844,0.0,accept,unanimous_agreement
750571563,8217,"it looks like you added a bunch of commits, and a rebase/merge from unstable, and then squashed it all, and i can't seem to be able to review the changes easily. please avoid it in the future. i'm ok with amending a commit and force pushing it (github let's me view the diffs), or to add more and more commits which will then be squashed when the pr is merged. rebase from unstable or squashing your commits should be done when no one is still looking at the pr, and if you have to grab fresh code from unstable, do it with merge and no squashing, so i can skip that commit.",0,0,-1,0.9369564652442932,0.9317893385887146,0.4694853723049164,0.0,accept,majority_agreement
750579077,8217,did you look at the code where we are temporarily attaching a flag to a command? that doesn't seem like a very maintainable assumption to make.,0,0,0,0.9560198783874512,0.9743788838386536,0.9789661169052124,0.0,accept,unanimous_agreement
750587204,8217,i agree. i'm still editing my cr post when i'm suggesting a better alternative.,0,0,0,0.927563726902008,0.7252801656723022,0.953803777694702,0.0,accept,unanimous_agreement
751443558,8217,/core-team please approve new info fields: a per command stat `rejected_calls` and `failed_calls` a per-error type counters (new info section) and a new stat for total error count. details at the top. can you please make a redis-doc pr? thank you!,1,1,1,0.9622824788093568,0.9838592410087584,0.950205624103546,1.0,accept,unanimous_agreement
751507702,8217,with regards to the redis-doc pr can you guys check [a link] ?,0,0,0,0.9826918840408324,0.9798011779785156,0.99500834941864,0.0,accept,unanimous_agreement
752168654,8217,addressed the comments in the new commit.,0,0,0,0.9795290231704712,0.9919346570968628,0.9941387176513672,0.0,accept,unanimous_agreement
752802742,8217,"i was getting the numbers for the standalone and cluster variations and noticed the following: - performance is not affected here as you can check bellow - one thing i believe we need to fix prior merge is that moved/clusterdown/crossslot/... ( all error replies caused within `clusterredirectclient()` are being accounted on the errorstats section, but not on the commandstats ( given we're not incrementing the rejected_calls there ). i believe we should... . if you agree tomorrow i'll fix it and include tests to verify it... as a quick note , to test the oss cluster variation i needed to update locally the branch to include the already merged changes by #8223. ----- ## standalone numbers of an ooo error test used command and error replies: [code block] without errorstats: [code block] with errorstats: [code block] ----- ## oss cluster numbers of an moved errors worst case scenario ( all moved ) used command and error replies: [code block] without errorstats: [code block] with errorstats ( notice errorstats is populated with info but not commandstats ): [code block]",0,0,0,0.7799946069717407,0.8892461061477661,0.8132973313331604,0.0,accept,unanimous_agreement
752878876,8217,"you meant #8226, yes, you should probably rebase this pr (can be a simple merge from unstable, since we're gonna squash this pr later) regarding clusterredirectclient i think it's a deeper problem, created: #8274 to discuss that. meanwhile, i suggest you'll explicitly increment cmd->rejected_calls in processcommand after the call to clusterredirectclient. if my pr will be merged, we'll get a merge conflict and resolve it properly.",0,0,0,0.9722212553024292,0.9871059656143188,0.9904106855392456,0.0,accept,unanimous_agreement
1976159196,8217,"we have encountered such a problem here. the user used such lua code: [code block] we can see `last:%s`, which will format the parameters. in errorstats, it will be used as an error code, causing more and more data saved in `info errorstats`, and then when using info, the server will block since there are too many errorstats. [code block] is there any way we can control this incorrect usage?",0,0,0,0.8346936106681824,0.9601921439170836,0.9865680932998656,0.0,accept,unanimous_agreement
1976249979,8217,"i'm not sure there's anything we can do. the api is clearly documented that the first word is the error code, and i don't think we have any way to distinguish between a misuse and a valid one. the only thing we can do is maybe put some maximum cap on the extent of damage it does, and disable the error stats mechanism completely if there are more than say 1000 different errors. i.e. print them to the log file, empty the dict, and set a flag to avoid populating it from now on.",0,0,-1,0.7299252152442932,0.6000185012817383,0.5358771681785583,0.0,accept,majority_agreement
1976270768,8217,"yean, the only way i thought of is the same. in info errorstats, use raxsize and only part of the data is output to break out of the loop. note that it may also potentially consume some memory (if it has been accumulated) or set a parameter to limit the number of errorstats we save",0,0,0,0.9805908799171448,0.9857650399208068,0.9876514673233032,0.0,accept,unanimous_agreement
1976285851,8217,"if we do that, i'd rather stop collecting them, release the memory and permanently disable error stats. not just filter them at reporting time.",0,0,0,0.9143176078796388,0.9739010334014891,0.9652193188667296,0.0,accept,unanimous_agreement
1976297995,8217,"yes, i think so too, just.. i can’t describe it very well. ok, what i think is similar to only saving up to 1000 errorstats, and deleting some of them when it exceeds, similar to our earliest repl_scriptcache_fifo, something like that but just some random ideas, just to throw the discussion out there first",0,0,0,0.8081711530685425,0.8646834492683411,0.931800663471222,0.0,accept,unanimous_agreement
1976311888,8217,"i don't think we'd want to delete some. i think we can detect a misuse and completely disable this feature preventing the damage it can cause, and losing the functionality. to help users figure out what happened, i'd print the existing values to the log file, add a single entry to replace them that mentioned it's disabled due to misuse, and prevent any further accumulation.",0,0,0,0.9702991843223572,0.911820948123932,0.9705078601837158,0.0,accept,unanimous_agreement
1006284160,10061,pat -steinberg,0,0,0,0.9696080684661864,0.9785451889038086,0.9455110430717468,0.0,accept,unanimous_agreement
1006348472,10061,"i didn't review the code yet, but i'd like to respond to the top comment. 1. i think redis-check-aof should still support checking old aof files (preamble or non-preamble files without manifest). i think someone may be using a new redis-check-aof with on an aof file from an old server. (if this complicates the code too much, maybe we can drop that, but i'd like to try supporting it). 2. i think your reasoning above about not truncating or fixing incr files other than the last one, are acceptable. but i think we need to print a message to a user trying to use `--truncate-to-timestamp`, telling him what to do (edit the manifest file manually). i.e. if we find the timestamp in a non-last incr file, we should tell him in which file we found it, and instruct him to edit the manifest and re-run.",0,0,0,0.816728413105011,0.908487856388092,0.9496105313301086,0.0,accept,unanimous_agreement
1006349290,10061,"before i review here are some questions: 1. i don't think the tool needs to be atomic. if we have base+incr1+incr2 and the specified timestamp is in the middle of incr1 then we need to delete to truncate incr1, delete incr2 and update the manifest. i think it's ok not to be atomic here. the user should always backup the directory before running this tool. 2. don't we still support a **non** rdb base? if so then the tool can also truncate the base file in such a case. 3. is the tool still backwards compatible with the old (non multi-part) aof file?",0,0,0,0.9215457439422609,0.9786412119865416,0.9052060842514038,0.0,accept,unanimous_agreement
1006355372,10061,ahe aof file produced by rewrite won't have timestamps. and note that corruptions are only expected at the last incr file (due to fsync or bad mount options),0,0,0,0.9877774119377136,0.9851667284965516,0.9911158084869384,0.0,accept,unanimous_agreement
1006355970,10061,"unless we add special parameters, i can't think of any effective way to identify whether this file is an old aof or a manifest. it is not reliable to rely on the file name alone. i thought, in this case, the user will see the following message: [code block]",0,0,0,0.9496205449104308,0.9423982501029968,0.9764417409896852,0.0,accept,unanimous_agreement
1006360623,10061,"regarding the detection of an old aof file, or a manifest i'm not sure it's that bad to rely on the file name (we do control the name of the manifest file, and it's unlikely that users will name their old aof files with a ""manifest"" suffix). however, i think it's not that complicated to detect automatically by the content. either we try to match the first word we know must be on the first line of the manifest, or even add some new unique header to our manifest files (it's not too late). regarding the truncation message, that's great. two suggestions for improvements: 1. we can take this opportunity to mention the offset we found in that file (so user can use other tools to manually truncate it, rather than go though the slow aof file processing again) 2. do we really have to instruct them to delete the files? isn't it enough to just modify the manifest?",0,1,1,0.9296221137046814,0.7128927111625671,0.9666323065757751,1.0,accept,majority_agreement
1006360916,10061,"we only recommend that users back up but it is not mandatory. without the guarantee of atomicity, i don't want to destroy the original data. in addition, if it crashes due to some reason in the middle, and we have no chance to print out enough log, may cause the manifest and aof to be incompatible, the user will not know whether the problem is caused by the `redis-check-aof` tool itself or the original data error. this will be confusing. of course we can (if base is the last file). no, we are discussing this.",0,0,0,0.5623837113380432,0.6280142664909363,0.949227809906006,0.0,accept,unanimous_agreement
1006362693,10061,"the manifest must start with `file` , like: `file appendonly.aof.1.incr.aof seq 1 type i` so, we can do like this: [code block] yes, only modifying the manifest is enough, but a better suggestion is to delete the files together, otherwise these files will never be seen by redis and will not be recycled by the gc mechanism.",0,0,0,0.9855544567108154,0.9924861192703248,0.99493670463562,0.0,accept,unanimous_agreement
1006367954,10061,"ok, so i think we can agree on: 1. no need to automatically handle truncation of non-last files (for both atomicity and complexity concerns) 2. detect old aof files vs manifest files like you just described. 3. keep the message about delete (or move) files when you manually remove them from the manifest. 4. let's add the offset inside the file where the truncation is needed in the log message.",0,0,0,0.9681825041770936,0.985689640045166,0.960148811340332,0.0,accept,unanimous_agreement
1006397988,10061,"for the record i think a recovery tool shouldn't worry about atomicity. for two main reasons: 1. when dealing with production issues that require manual fixes to get back up online we always perform non-atomic operations. we don't have the luxury to stop and consider what happens if we have another failure (causing atomicity issues) while performing the recovery process. this is simply in order to fix things and get back online as soon as possible because developing an atomic custom recovery plan during a crisis is never practical. 2. to mitigate the risk in _1._ we backup our data or any configuration before performing a risky operation. sometimes the cost of this is large (copying a 500gb aof file is expensive) but there's no practical way around it. i think this tool should in general disregard atomicity concerns but should definitely warn the user to **backup** the files before applying any fixes to them. i'm also fine with not editing the manifest file at this point to keep the tool simple (in 99% of the cases we won't need to truncate a non last incr file), but i don't think atomicity has anything to do with it.",0,0,0,0.7996556162834167,0.9781498312950134,0.8436412215232849,0.0,accept,unanimous_agreement
1006438839,10061,"-steinberg aside from atomicity, but it does bring a lot of complexity. thinking from another perspective, if the user really wants to truncate the non-last incr file, we print a friendly message and ask him to manually delete these files (this time he may realize that these files should be backed up first). i think this is a safe practice, and deleting files is a very easy thing for user, and the probability of this happening is very small.",0,0,0,0.6829477548599243,0.9231163859367372,0.8916205763816833,0.0,accept,unanimous_agreement
1006439737,10061,"1,2,3,4 all done.",0,0,0,0.977538764476776,0.9899600744247437,0.9856115579605104,0.0,accept,unanimous_agreement
1046318404,10061,please take a look at this failure [a link] [code block],0,0,0,0.9793835282325744,0.97562974691391,0.9869239926338196,0.0,accept,unanimous_agreement
1046961796,10061,"another similar error: [a link] [code block] i don't understand how we could get ""aborting..."", if we didn't pass the `--fix` argument.",0,0,0,0.6155861616134644,0.8631907105445862,0.9613806009292604,0.0,accept,unanimous_agreement
1047369752,10061,"got, i have added more logs (printf the `errno` and aof abs path) in my branch and re-run the workflow, but so far it has not reappeared.",0,0,0,0.9867606163024902,0.9820767641067504,0.9690032601356506,0.0,accept,unanimous_agreement
1047395750,10061,"finally reappeared. it looks like we are getting a wrong path, is there a bug in the dirname function? [code block]",0,0,0,0.512717604637146,0.960020124912262,0.978549063205719,0.0,accept,unanimous_agreement
1047460271,10061,"i add this log: ![a link] and finally reproduced it ([a link] ![a link] so it's `dirname` that is returning us the wrong path, i'm not sure that `dirname` will have some kind of exception under `sanitizer=address`, at least i haven't found a similar answer yet. copy a glibc implementation: [code block]",0,0,0,0.6063857078552246,0.9389455318450928,0.9920228719711304,0.0,accept,unanimous_agreement
1047478008,10061,do you have any relevant insights on this?,0,0,0,0.984298825263977,0.9873269200325012,0.9913086891174316,0.0,accept,unanimous_agreement
1047507777,10061,"finally found the bug: [code block] since `temp_filepath` is allocated on the stack and not initialized, and we did not copy `'\0'` when executing `memcpy`, so when a lot of `'\' `are left on the stack, `dirname` may get a wrong result.",0,0,0,0.9256343841552734,0.9880782961845398,0.9877606630325316,0.0,accept,unanimous_agreement
1016786314,10127,"/core-team listing sub-commands in acl cat and command list. complementary act for the sub-command work in 7.0. if you see any reason not to do that, please speak.",0,0,0,0.9804834127426147,0.9665869474411012,0.9868021607398988,0.0,accept,unanimous_agreement
1017726807,10127,-binbin i'm done reviewing the lat batch of changes,0,0,0,0.9814735054969788,0.9248052835464478,0.9748801589012146,0.0,accept,unanimous_agreement
1017740532,10127,"thank you very much for the review.. sorry for not doing it perfectly, i will take a deep look tomorrow",-1,-1,-1,0.9028295278549194,0.9639301896095276,0.9820765256881714,-1.0,accept,unanimous_agreement
1018724137,10127,trigger a full ci in this branch: [a link],0,0,0,0.9879553318023682,0.9870542883872986,0.9959399700164796,0.0,accept,unanimous_agreement
1019258215,10127,"-binbin gave it another top to bottom review, and last round of comments. there's one real issue here, one that existed before your pr, but i just found it now (iterating on the subcommands list instead of dict). all other comments are just some small suggestions for improvements. i also added some details to the top comment. please also go over all the earlier unresolved comments make sure they're resolved and mark them as such. then let's merge this.",0,0,0,0.9536965489387512,0.870438814163208,0.8885461688041687,0.0,accept,unanimous_agreement
1019314815,10127,"thanks for the review / patience. i checked all the comments. i admit this pr getting bigger and bigger with each small change, with too much cleanup mixed in... will avoid doing this in the future, or at least split the pr (avoid irrelevant changes)",1,1,1,0.952109158039093,0.989925503730774,0.986333429813385,1.0,accept,unanimous_agreement
1019326939,10127,"a little unrelated cleanup is ok.. and in this case much of it is not unrelated. in retrospect, i'm not sure we should have made all the error messages uniform (inducing many changes in the tests) we could even consider not to rename `name` to `fullname`, but i figured it could be dangerous for some merge branches / forks, and once we did that, many of these lines where already modified anyway...",0,0,0,0.8184311985969543,0.7382686138153076,0.8715248703956604,0.0,accept,unanimous_agreement
897733068,9356,"i didn't initially think of this, but i think we also need a change in defrag. here [a link] we might reallocate the dictentry, but the clustermeta data pointers will still be pointing to the previous invalid locations.",0,0,0,0.98518306016922,0.9890196919441224,0.9873017072677612,0.0,accept,unanimous_agreement
898183836,9356,"/core-team probably worth pinging the group about this, as it's a rather intrusive change and i can see the argument the complexity is not worthwhile. if you get a chance, i also would expect this to perform better as well. if you don't get a chance i can try running some benchmarks tomorrow.",0,0,0,0.8648210167884827,0.8336716890335083,0.9122859239578248,0.0,accept,unanimous_agreement
899461061,9356,"## benchmarks cluster of 3 nodes, no replicas, running locally on laptop. ### latency `redis-benchmark -p 30001 --cluster -t set,get -n 1000000 --threads 8 -p 100 -q` result: no significant difference compared to `unstable`. **[edit] i forgot to use -r here, so only one key was created. :-/ see new benchmark below!** ### memory `debug populate 1000000` on one of the cluster nodes. then `info memory`. | result | `used_memory_human` | |------|-----| | unstable | 116.37m | | this pr | 93.51m | | difference | 22.86m (20% save) | the keys generated by `debug populate` are on the form `key:nnnnnn`. with a longer common key prefix (10 bytes), the memory saving is around 18%. for long keys without a shared prefix, the saving is expected to be even higher but i haven't written any script for generating such data.",0,1,-1,0.8045446276664734,0.5743837356567383,0.8934724926948547,,review,no_majority_disagreement
900016749,9356,?,0,0,0,0.9320514798164368,0.9557723999023438,0.9296892285346984,0.0,accept,unanimous_agreement
904543611,9356,"haven't done a cr, but this looks good as long as there's no regression in performance - the rax was put there to replace a skip list that didn't perform well ([a link]",0,0,0,0.8521443605422974,0.5783535242080688,0.9468284249305724,0.0,accept,unanimous_agreement
904935073,9356,"regarding the performance regression concerns, i increased the numbers and ran some more benchmarks. benchmark: set and get number of requests (-n): 10,000,000 keyspace length (-r): 1g, 1m, 1k (see table) cluster: 3 masters, 0 replicas command: `redis-benchmark -p 30001 --cluster -t set,get -n 10000000 --threads 8 -p 100 -q -r $r` the database was flushed on all nodes between each run. | keyspace length | | unstable | this pr | |-----------------|-----|------------------------------------------------|----------| | r=1,000,000,000 | set | 1327140.00 requests per second, p50=3.351 msec | 1990049.75 requests per second, p50=2.239 msec | | | get | 3619254.50 requests per second, p50=1.159 msec | 3317850.00 requests per second, p50=1.239 msec | | r=1,000,000 | set | 1473839.38 requests per second, p50=2.167 msec | 2094240.75 requests per second, p50=2.167 msec | | | get | 3620564.75 requests per second, p50=1.263 msec | 3320053.00 requests per second, p50=1.367 msec | | r=1,000 | set | 2844950.25 requests per second, p50=1.623 msec | 2840909.00 requests per second, p50=1.647 msec | | | get | 4424779.00 requests per second, p50=0.879 msec | 4422822.00 requests per second, p50=0.895 msec |",0,0,0,0.9376392364501952,0.97782564163208,0.9887856245040894,0.0,accept,unanimous_agreement
905169463,9356,"ooh, that is more like what i was expecting. so it looks like we see about a ~40/50% speedup for writes (which is great) at the cost of about a 10% get performance regression. for the r=1000, almost everything is probably in l1/l2 cpu cache, so that likely explains why their wasn't much difference there. i think that 10% regression is acceptable for the memory savings and the write speed improvement. /core-team going to ping everyone again since this is definitely something to discuss.",1,0,1,0.7076488137245178,0.6037250757217407,0.7436622977256775,1.0,accept,majority_agreement
905179213,9356,"the read performance degradation is marginal and not a major concern imo. on the other hand, the current approach of using a rax to store the slot to keys structure ensures that the keys in the slot are strictly ordered in lexicographic order. with this approach that property is foregone. there is no longer any ordering that can be done given a specific key which makes async iteration on the keys in a slot to be harder when we need to resume the operation later on after the slot is modified. looked through `db.c` but can't seem to find a good place where redis is currently relying on that today though. more specifically, i was thinking of a use-case when we are iterating the keys in the slot, and we do that in an incremental manner with an iterator. when the iterator is somewhere in the middle of the slot, is there an efficient way to determine if a given item comes ""before"" or ""after"" the iterator? with rax when the keys are ordered, we can simply do a `>` comparator on the key name with the iterator key name, which i think is kind of nice. also today, the `raxiterator` is able to store the current key name so that even if the underlying slot_to_keys changed across iterator runs, we can do a quick re-seek the next time around and resume iteration. i don't think we will have the same safe iteration mechanism available on the double linked list with this change. does anyone know why `slot_to_keys` is ordered today? there might be a reason for it that i am not aware of. i wonder if there can be future potential use-cases for keeping the keys in the slot ordered which will be harder to support after this change.",0,0,0,0.8399643301963806,0.9669705629348756,0.9232180118560792,0.0,accept,unanimous_agreement
905235868,9356,"i agree that considering the memory improvement and the write speed improvement we can accept the read degradation, but maybe we should give it a quick optimization attempt to see if we can solve that too. i.e. invest a small effort rather than give up without trying. it could be an extra indirection that's unavoidable (although rax had an abundance of these), but maybe there's something simple that can be done to make it more cache friendly, or do some function inlining tip / macro. can you do some profiling to see if you happen to find something.",0,0,0,0.946702003479004,0.9627824425697328,0.9108405113220216,0.0,accept,unanimous_agreement
905424401,9356,"if you do look into performance degradation of the read operations, i would suggest finding out where the additional latency comes from. looking at this cr there is nothing that changed directly in the `lookupkeyread` code path apart from the additional metadata in the `dictentry` so i am curious about it. it could potentially be coming from the way benchmark was set up but i am not sure assuming you are keeping all other factors the same in your test run besides the redis-server binary. thinking about this more, the key range of 1000 where everything fits into l1/l2 cache is a very small redis instance and quite unusual usage scenario nowadays especially given the increase in the memory capacity over time from various service offerings. it is very likely that people would store much larger number of keys in redis in the range of millions, where they will see this 10% slow down in the read path.",0,0,0,0.8621782660484314,0.9147126078605652,0.9677281379699708,0.0,accept,unanimous_agreement
905540954,9356,"i can try, but i can't think of any obvious optimization. my guess is reads are slower because dictentry is larger, which means more cache misses for reads. the slot-to-key mapping is not used for reads. well, i ran the benchmarks on a laptop, which is not very good in terms of stable cpu frequency, other background processes, etc. if you have a better environment to run tests, i'd be happy if you repeat them. it's very easy to set up the cluster using the utils/create-cluster script and redis-benchmark.",1,1,0,0.492199182510376,0.6414708495140076,0.6718056201934814,1.0,accept,majority_agreement
905621187,9356,"maybe you can consider doing the benchmark by launching redis instances on aws ec2 instances, and generate benchmark traffic from another client ec2 instance? that way you have dedicated resources to run the redis server and benchmark processes with sufficient amount of cpu/memory. i think we should repeat the experiment and look at data sets that consists of a million keys and 10 million keys.",0,0,0,0.9771384000778198,0.986952543258667,0.9876388311386108,0.0,accept,unanimous_agreement
905834691,9356,"i repeated the tests a few times on laptop and they show consistent enough results of +50% write and -10% read performance. i don't have directly available access to aws, but if you have, maybe you want to run them on ec2 instances? i think pref and flamegraph would be better to pinpoint where the cpu time is spent.",0,0,0,0.9583753943443298,0.9618656039237976,0.9756606221199036,0.0,accept,unanimous_agreement
905912942,9356,+1 on flamegraph.,0,1,0,0.8474614024162292,0.5699644684791565,0.8538655042648315,0.0,accept,majority_agreement
906024601,9356,"several of us were discussing this yesterday. we theorize that the slight degradation is how the dict entries fit into cache. the old dictentry was 24 bytes. a cache line is 64 bytes. so there are likely to be 2 dictentries (and part of a 3rd) in every cache line. the new dictentry is 40 bytes. so only 1 dictentry can fit in a cache line. if the alignment is just right, you can fit the first de (40 bytes) + the first 24 bytes of the next entry (the main bytes of the dictentry) into a single cache line - so, in some cases we still get 2. but usually not the case. given this, we would expect less performance degradation with a larger database. the larger the database, the less likely a random item is to already be in cache. also, note that this is partly related to the test itself. if the only 24 byte allocations are dictentries, we are very likely to have multiple dictentries in a cache line. however, if values and/or keys require 24 byte blocks, we would be less likely that the cache line would have 2 dictentries.",0,0,0,0.9673953056335448,0.9755723476409912,0.9268968105316162,0.0,accept,unanimous_agreement
906124362,9356,"very interesting! i have one thought, want to share with you, but without test, i am sure it is effective. now entry size 40, according to jemalloc bin size, actually allocs 48 byte, old dictentry was 24 bytes, allocs 32byte, cache line could contain 2 entries. based on this idea, i come up a idea, we separate real metadata with its pointer, that is to say, in dict entry, we only store a metadata pointer, and alloc another memory to store metadata. so new dict entry is 32 bytes(jemalloc has 32bytes bin), metadata is 16bytes(jemalloc also has 16bytes bin). this solution doesn't cost extra memory compared with current pr, but it brings one extra memory allocation when creating new keys and extra one free when deleting keys.",1,1,1,0.9819781184196472,0.9898832440376282,0.9849542379379272,1.0,accept,unanimous_agreement
906128538,9356,"we do have a bin for 40 bytes, and also one for 24 bytes (#2510) also, that extra pointer **will** consume extra memory (reducing the savings), and also it'll add another indirection, probably slowing down the cases where this data structure is used.",0,0,0,0.9865074157714844,0.985770344734192,0.9766589999198914,0.0,accept,unanimous_agreement
906135136,9356,"oh, thanks i miss that, please ignore above thoughts",1,1,-1,0.9728804230690002,0.7675151824951172,0.8568596243858337,1.0,accept,majority_agreement
906442803,9356,"flame graphs, 10m keys. unstable, set ![a link] this pr, set ![a link] diff, set ![a link] unstable, get ![a link] this pr, get ![a link] diff, get ![a link] how to reproduce * compile redis with `cflags=-fno-omit-frame-pointer`. * start a cluster of 3 masters, no replicas. * start `perf record -f 99 -a -g -p $pid` where pid is the pid of one of the redis nodes (as root if you have to). * in another terminal, run benchmark `redis-benchmark -p 30001 --cluster -t $test -n 10000000 --threads 8 -p 100 -r 1000000000` where test is either `set` or `get`. run `set` first to populate the database and later `get` on that data. * stop `perf` using ctrl+c. perf writes `out.data`. * see [a link] on how to generate the flamegraph from this. * see [a link] on how to generate a diff flame graph.",0,0,0,0.7427239418029785,0.9616212844848632,0.9230642914772034,0.0,accept,unanimous_agreement
906811065,9356,"the keys in the test are 22 bytes (`key:{sss}:nnnnnnnnnnnn`) and the values 3 bytes, so for each key, we have the following allocations: | what | details | size | alloc size class | |------|-------|-----|-------| | key | sdshdr5 (1) + key bytes (22) + nullterm (1) | 24 | 24 | | value | robj (16) + embedded sdshdr8 (3) + value bytes (3) + nullterm (1) | 23 | 24 | | dictentry | | 40 | 40 | | dictentry* | in db->dict.ht[0] per key (one large allocation) | 8(+) | 8(+) | | total | | 95 | 96 | on one of the redis nodes after running the set benchmark, info gives `used_memory=320489744` and `db0:keys=3241757` (1/3 of the 10m keys), which implies 99 bytes per key. only 3 bytes overhead left to other stuff or i missed something. anyway, assuming each key-value pair takes 100 bytes is not too far-off. given a random key of 10m keys, i estimate its dictentry is almost never in the cache line. the flame graph ""diff, get"" shows dictsdskeycompare in red, meaning it takes more time than before. this is where the key is loaded from ram during lookup, so there can be cache misses here too. let's calculate the probability that the entry (or parts of) is in the l3 cache then. my i9 cpu has 12mb of l3 cache. since i was running redis-benchmark on the same machine, let's say i had 10mb of l3 for the redis nodes. given 100 bytes per key, we can fit 100k keys in 10mb of l3 cache, so we have a probability of 0.01 that a random key of 10m keys already sits in the l3 cache. with 16 bytes less per dictentry (i.e. without this pr), that probability is 0.012. this difference still can't explain the 10% higher latency. if we had a way to simulate real traffic with a mix of set, get, del and other commands, based on statistics from a real production system, that would be pretty nice.",0,0,0,0.972886085510254,0.9903297424316406,0.9940604567527772,0.0,accept,unanimous_agreement
906811802,9356,", got some further interesting data on the get command top on-cpu consumers by function and loc. ( using 3 primaries, 0 replicas, 100m keys populated with memtier via: `taskset -c 4-11 memtier_benchmark -d 3 --threads 8 -c 20 --key-pattern=p:p --ratio 1:0 --pipeline 100 --cluster-mode -p 30001 --hide-histogram -n allkeys --key-maximum 100000000` ). the 3 redis process are pinned to individual cores 0,1,2. using commit 4e1ea907c3cb5ccf7c73cdc3adcaf958d856a150 of [a link] ## top on-cpu consumers call graph by loc focusing specifically on call stacks that represent > 1% on cpu - 1) we see that dictfetchvalue->lookupkey->dictfind is the top stack. `dictfind` line 522: `if (key==he->key || dictcomparekeys(d, key, he->key))` takes 12.21%. basically the dictcomparekeys is the most expensive call in terms of cpu time. - following up on the above point, within dictsdskeycompare, the line src/sds.h:88 `l2 = sdslen((sds)key2);` takes 9.50% cpu time. if we can preserve the sdslen value on the object it would improve ~10% the cpu time. [code block]",0,0,0,0.959488034248352,0.9588146209716796,0.8832295536994934,0.0,accept,unanimous_agreement
906857629,9356,"nice ! the call to sdslen (in dictsdskeycompare) is the first access, so it's what loads the key from memory (unless it's already in the cache). [code block] when a key is added to db, the dictentry is allocated in dictaddraw just after the sds key in sdsdup. [code block] ... so if they're in the same cache line, the sds key is already loaded when the dictentry is loaded. if they're in the same bin size, they're more likely to be allocated next to each other, thus to be in the same cache line. if this hypothesis is right, this branch should beat unstable on reads for key lengths 31-36 bytes (the 40 bytes bin when sds overhead is added). we can use #9394 to test it, or `--key-prefix` for memtier.",1,1,1,0.9903132915496826,0.8454819321632385,0.9051368236541748,1.0,accept,unanimous_agreement
907000670,9356,"it's possible that we're focusing on the slowest thing we see, but that's also as slow in the benchmark on unstable, and thus maybe we need to run a similar proofing on unstable and compare them. if i read the calculations above correctly, the chances of a dict entry or sds to be in the cache in that benchmark are slim, right?",0,0,0,0.9501327276229858,0.958901345729828,0.9828261733055116,0.0,accept,unanimous_agreement
907073622,9356,"i repeated the benchmark with 33 bytes keys and voilà the large dictsdskeycompare is gone again, as predicted. flamegraph get with 33 bytes keys ![a link] benchmark details for the above graph preparation: `redis-benchmark -p 30001 --cluster -n 10000000 --threads 8 -p 100 -r 1000000000 set 'key:{tag}:__rand_int__:0123456789' 'abc'` benchmark: `redis-benchmark -p 30001 --cluster -n 10000000 --threads 8 -p 100 -r 1000000000 get 'key:{tag}:__rand_int__:0123456789'` [code block] the same benchmark on unstable: [code block] > it's possible that we're focusing on the slowest thing we see, but that's also as slow in the benchmark on unstable, and thus maybe we need to run a similar proofing on unstable and compare them. no, the same profiling was done on unstable already. we were focusing on the regression specifically. see the flame graph for ""unstable, get"" above and ""diff, get"", specifically the block ""dictsdskeycompare"" in the middle, it almost invisible in the unstable diagram (next to dictfind in the middle) and it sticks out in red in the diff diagram. the changes the key is in the cache is very high for keys of the same bin size as the dictentry, i would say, since they're allocated just after each other in dbadd by the same slab allocator. even if they're not always in the same cache line, the sds key is very likely to be in the l2 and l3 caches already once the dictentry has been accessed. kudos jim for the cache line idea and for bringing up the bin sizes and everyone else for the motivation to investigate this! :clap: if we'd want to optimize this regardless of key length, i can see one possibility: we can embed the key in the dictentry allocation. :-) it's an obscure idea, but it would increase the changes it's in the same cache line as the dictentry.",0,0,0,0.9213536381721495,0.948327898979187,0.9394294619560242,0.0,accept,unanimous_agreement
907080328,9356,btw: i believe keys of length 15-22 (the 24 bytes bin) are more common than keys in the 40 bytes bin.,0,0,0,0.9370394945144652,0.9905320405960084,0.989980399608612,0.0,accept,unanimous_agreement
907098302,9356,"another idea hits me, could we store expire time into dict entry metadata, expire time dict cost too much memory, especially, rehashing on big hash table. the sum of `expire time, pre pointer, next pointer` is 24 byte, you designed new entry is 40, total size is 64(one cache line). and that may accelerate accessing key expire time.",0,0,0,0.93215674161911,0.9856510758399964,0.9843559861183168,0.0,accept,unanimous_agreement
907118027,9356,"expire is only used for keys with ttl. if you don't use ttl, this is wasted space.",0,0,0,0.7827707529067993,0.9812588095664978,0.9087051749229432,0.0,accept,unanimous_agreement
907148365,9356,"yes, i initially think we have different dict entry for keys with ttl or keys without ttl, we allocate new dict entry(with ttl metadata) when setting expire time, but i am not sure there is a clear way to implement.",0,0,0,0.968629777431488,0.8978226780891418,0.9883466362953186,0.0,accept,unanimous_agreement
907297349,9356,"i think trying to poke around to see if we can embed the key into the dict entry efficiently is perhaps a good idea. we prototyped it at aws few years ago but didn't like the complexity at the time. the counter idea was to try to prefetch the key into the cpu cache using gcc builtins. as an aside, within aws we've found that the average key size is about 17 bytes, so that sort of lines up and is what we do benchmarking on",0,0,0,0.8539024591445923,0.9529106616973876,0.6728346943855286,0.0,accept,unanimous_agreement
907299003,9356,"i'm happy merging this given the above information, we can always do follow up to continue diving into key specific performance optimizations, but that doesn't seem like an ideal quick win that redis likes to focus on :)",1,1,1,0.9850289225578308,0.9948034882545472,0.9925611615180968,1.0,accept,unanimous_agreement
908105175,9356,rebased and squashed to two commits (see top comment). don't know why mac build failed.,0,0,0,0.5781601071357727,0.5924268364906311,0.872201144695282,0.0,accept,unanimous_agreement
908669093,9356,"i kicked off the runs again, i can't tell what went on with the mac build either.",0,0,0,0.8604702353477478,0.7345423102378845,0.9735413789749146,0.0,accept,unanimous_agreement
908786419,9356,"i just ran the same benchmark on aws ec2 with a server instance that has dedicated cpu/memory resources. here is my set-up: 1. server: one ec2 r5.2xlarge instance in vpc, cluster mode enabled, running redis-server code before and after this change. 2. client: ec2 c5n.9xlarge in the same vpc but a different availability zone, runs the same redis-benchmark command as above, mainly focus on testing for 1 million keys and 10 million keys range. below are my performance run findings: 1. if i run the benchmark only once (which takes only ~10 seconds to complete), i am seeing similar performance characteristics as what reported - set throughput increased by 25-30%, and get throughput dropped by ~7%. please note that in this set-up, the set operations are mostly adding new items into redis here given that we only run it for a very short period of time, so add operations are predominate here. 2. now i run the same benchmark in a loop (i.e. with `-l` option) to simulate the steady state read/write workload when the redis server has already been populated with data, here both set and get shows a ~6-7% drop in tps. please note that in this set-up, the set operations are mostly doing an update operation instead of an add operation. i think this mostly comes from the fact that update of an existing item in redis doesn't need to update `key_to_slots` data structure like what adding a new item does. after discussion, brought up to me the theory that the read performance drop mostly comes from the key size being 16 bytes which makes the key and dictentry fit on the same cache line without this change... so i did another run with the same benchmark testing with longer key sizes (i.e. from 16 byte to 26 byte), still the results are the same in terms of read performance. in summary, i'd like to call out the fact that the performance gain only applies to adding new items into redis, which typically happens during the initial phase when the redis server is getting warmed up with frequently accessed data. when steady state is reached, even though redis user can still add new items into redis (i.e. when they use expiry), i think that still has a performance penalty due to cache miss, and a well tuned redis application would try to avoid that (e.g. something like 80% read and 20% write is a typical scenario) **given that redis users are a lot more likely to be doing read and update operations instead of adding new items into the cache in steady state scenarios and experience performance drop, i am not in favor of merging this change.** more details on the steady state performance results below (sorry the redis-benchmark i ran didn't give a better precision in terms of latency numbers for p50) keyspace length | | unstable | this pr -- | -- | -- | -- r=1,000,000,000 | set | 645k requests per second, p50 = 7 ms | 610k requests per second, p50 = 7 ms | get | 810k requests per second, p50 = 5 ms | 760k requests per second, p50 = 5 ms r=1,000,000 | set | 710k requests per second, p50=6 msec | 660k requests per second, p50=6 msec | get | 880k requests per second, p50=5 msec | 804k requests per second, p50=5 msec",0,0,0,0.9117578268051147,0.9548156261444092,0.8498682975769043,0.0,accept,unanimous_agreement
909056594,9356,"that's a very interesting read! thanks for benchmarking in a proper environment. i agree, it's the add operation that is optimized, but also del, though we didn't test that. (deletions happen also when accessing an expired key.) it's also a memory save, though i think the benchmark is biased here too, since we use really small values, so the keys make up a higher percentage of the memory. these keys would go in the 32 bytes allocator bins. they need to go in the 40 bytes bin (the same as the new dictentry) to benefit from this, so try 31-36 bytes keys. that's what i did in [a link] that's an unusual key size, but it backs the analysis. i would like to attempt a follow-up pr to ""embed small keys in dictentry metadata"" (aka ""coallocate""). that should eliminate the cache misses that cause the regression here. i hope it can be made simple enough. regarding the typical 80%/20% reads/writes ratio, i want to share a different use case. in ericsson, we have systems that have almost only writes (though the majority are updates, not adds). most of the user/session data is cached in the application itself and redis cluster is used more like a backup. traffic for the same user is routed to the same application instance to make sure the in-application cache is utilized. reads are done mainly when users are routed to a different application instances, e.g. when scaling, updating, etc. mobile networks require really low latency and this design works for that.",1,1,1,0.9911092519760132,0.9927994012832642,0.9946318864822388,1.0,accept,unanimous_agreement
909393440,9356,"i did testing and validated the claim that del are also improved, along with inserts. qu and i were chatting about it yesterday, and he did convince me that the follow up of trying to optimize small keys (i suppose up to 32bytes, to keep it in a 64 byte cache line) could be embedded into the dict entry. let's move any future discussion to a new issue, but i'll assume you'll dive into it then :)",1,1,1,0.8510576486587524,0.9821450710296632,0.9872681498527528,1.0,accept,unanimous_agreement
909941989,9356,"meanwhile, can you please update the summary at the top (will be linked in the release notes), with a summary of what was done, what are the implications, and why we decided to accept them.",0,0,0,0.983044981956482,0.9897509217262268,0.9914194345474244,0.0,accept,unanimous_agreement
910092746,9356,"note that a uuid is 36 bytes (on hexadecimal form). it goes in the 40 bytes bin. since they lack a common prefix, there might be even more than 20% memory savings for these. those who use uuids are likely the real winners here.",0,0,0,0.9787310361862184,0.9832574725151062,0.9858927130699158,0.0,accept,unanimous_agreement
910098113,9356,"i've updated it. does it clarify why you decided to accept them? if not, please update.",0,0,0,0.9822205901145936,0.9789282083511353,0.9927151799201964,0.0,accept,unanimous_agreement
981700502,9356,"for the record, i just realized this this change also fixes an issue. in the past, defrag.c wasn't defragging the slot-to-keys, so these allocations could have prevented the allocator from releasing pages and result in a defragger that's consuming a lot of cpu and sin't gaining anything, see #9773. now since they're part of the dictentry, they'll get implicitly defragged with it, so problem solved! i'll update the top comment for the sake of release notes.",0,1,1,0.7399787902832031,0.8065105080604553,0.954585611820221,1.0,accept,majority_agreement
981745414,9356,nice! maybe that's why my colleagues didn't see any benefit of defrag and turned it off. (they also wanted to turn off the allocator statistics because of the costs to keep them updated at every alloc and free. i think i mentioned this once.),1,1,1,0.9794906973838806,0.973510205745697,0.989822268486023,1.0,accept,unanimous_agreement
981870043,9356,"that's awesome! we've also seen limited benefit in the past, good to know this might have been part of the problem.",1,1,1,0.9934084415435792,0.996093451976776,0.9963860511779784,1.0,accept,unanimous_agreement
981969427,9356,"can someone have a look at the above mentioned ticket and confirm it makes sense that this is due the cluster slots mapping? iirc it's redis 4, which used to use rax, and the fragmentation was mainly in the 24 byte bin.",0,0,0,0.9889889359474182,0.9932483434677124,0.993104875087738,0.0,accept,unanimous_agreement
1084130365,9356,excuse me for being late... wasn't it simpler to have 16384 dicts (in worst case) - one dict for cluster slot? empty slot could live without dict.,-1,-1,-1,0.6507952213287354,0.7715840935707092,0.969742715358734,-1.0,accept,unanimous_agreement
1084228386,9356,"-falcon maybe it is simpler but now the other solution is already done. :-) do you want to investigate the dicts idea and benchmark it? i think 16384 dicts (or 5400 dicts, a 3rd of the slots) is a lot of memory allocations, especially if there is only one key in each slot.",1,1,0,0.9756712913513184,0.989813506603241,0.6341944336891174,1.0,accept,majority_agreement
1084742830,9356,"the question this solves is: ""give me all the keys in slot x"". mapping slot-to-key is only used rarely - for things like slot-migration. normally, we look up keys without regard to slot. having 16k dictionaries (in place of the main dictionary) would increase lookup time as we'd need to find the correct dictionary first. it would also be fairly disruptive to the code base as there are a lot of operations which use the main dictionary which would have to be redesigned to hash to slot first and then lookup. anything that iterates on the main dictionary would have to be redesigned to iterate over 16k dictionaries (or the portion of 16k which are on a given node). having 16k dicts in addition to the main dictionary is much more memory than the linked list solution and the direct item access isn't needed for the problem being solved. imo stringing a linked list through the existing dictionary is a good solution. and, of course, this is far more efficient (both time & space) than the old rax table implementation.",0,0,0,0.6785503029823303,0.8976266384124756,0.979528784751892,0.0,accept,unanimous_agreement
1084768437,9356,"sorry, i had deal with redis instances with 6-10gb per instance - ~10m keys. and ~500 slots per instance (yes, 33 partitions). doubtfully linked list links (16*10m=160m) consumes less memory than per-slot dictionary.",-1,-1,-1,0.9856048226356506,0.9870569109916688,0.9830526113510132,-1.0,accept,unanimous_agreement
1084831332,9356,"if we replace the db dict with these 16k dicts, then i can agree it doesn't use more memory. i assumed you wanted to use separate dicts just as to reference the set of keys in each slot. if we want a two-level hash table anyway to avoid huge allocations, this could be a possible way forward. it is similar to #9517. however, in the extreme case that a node has only one slot (16k shards) there will be only one dict again.",0,0,0,0.980362832546234,0.9835404753684998,0.9879236817359924,0.0,accept,unanimous_agreement
1100451567,9356,"-falcon i'm going to promote this to it's own issue, i think it's a good idea, [a link]",1,1,1,0.8837100863456726,0.8518006801605225,0.984563410282135,1.0,accept,unanimous_agreement
1285244511,9356,"finally followed. we could fundamentally consider the application scenario of `slottokey`, basically in the scenario of cluster rehash and this function is used infrequent(in our practical experience, 90% of the clusters do not use the rehash once a year.). we can make a tradeoff and use the new migraite mechanism to completely remove the storage and performance overhead of the `slottokey` this is a preliminary design: 1. fork a child 2. scan the keys in child process, find the slot key and write to dest node 3. synchronize the incremental data of this slot this solution will increase the cpu overhead of the child process in step 2, but considering the overall roi, compared with the performance and storage that can be improved in the main process in most of the time, this overhead is worth it.",0,0,0,0.9598181247711182,0.989552676677704,0.9793932437896729,0.0,accept,unanimous_agreement
1297099585,9356,"your idea is somewhat like what #10933 proposes, or the implementation in tencent or bytedance' internal forks, which regards migration as a special full-sync replication",0,0,0,0.9759962558746338,0.9888152480125428,0.9920133948326112,0.0,accept,unanimous_agreement
1305559018,9356,"cool job! if convenient, can you tell me about the memory savings after `slottokey` be deleted? :-)",1,1,1,0.9938507080078124,0.9957734942436218,0.99655020236969,1.0,accept,unanimous_agreement
1305635008,9356,"i don't know... migrating with full replication is mainly for mitigating the impact involved by original migration like blocking with big key, or resource contention between business and operations. removing `slottokey` is not the main concern, but of course `slottokey` can be removed in that way depending on if you concern memory or cpu/time resumption more.",0,0,0,0.9719271659851074,0.9677214026451112,0.9305239319801332,0.0,accept,unanimous_agreement
1306018093,9356,i didn't see any comments from you on [a link] did you have any additional insights you wanted to provide?,0,0,0,0.9641987085342408,0.987679958343506,0.9766037464141846,0.0,accept,unanimous_agreement
1306551415,9356,"yes, i only roughly viewed the raw proposal before as when i look at it the discussion list is quite long... i think the general direction and hld is pretty well(silence is also an attitude i guess), looking forward the next moving. i will look into the whole discussions when i have some time.",0,0,1,0.8047584891319275,0.8817194104194641,0.9073479771614076,0.0,accept,majority_agreement
1288988374,11193,"i think i'd like a test that uses unlink too (and mention both flushall async and unlink in the top comment, for clarity)",0,0,0,0.714425802230835,0.9797700643539428,0.9865701794624328,0.0,accept,unanimous_agreement
1308281792,11193,"i realized we're pausing write commands coming from, but in fact we should pause denyoom commands. i.e. we want to pause clients that can increase the memory usage, but there's no need to pause del and unlink",0,0,0,0.9841228723526,0.9879289865493774,0.9807900190353394,0.0,accept,unanimous_agreement
1308308507,11193,cool. i will add another `pause_action_client_denyoom` action to `pauseactions()` (along with `pause_action_client_write`) and refine `clientwritepauseduringoom()`.,1,1,1,0.9859362840652466,0.7604194283485413,0.931490421295166,1.0,accept,unanimous_agreement
1308357358,11193,"after a discussion with : i think that this pr attempts to solve a theoretical problem (no one complained about flushall async causing eviction/oom) and creates a new semi-theoretical problem (clients using pipeline during flushasync are more likely to cause eviction/oom due to the large query/output buffers after this pr) i don’t think we will find an air-tight solution, but i think we should at least try to further minimize the impact by removing the read event of rouge clients. we may still reach oom/eviction due to a lot of medium-sized buffers if many clients are using reasonably-sized pipelines. (it makes little sense to solve one theoretical issue but leave another unsolved, if we solve theoretical issues, let’s solve both)",0,0,0,0.8935896158218384,0.963123381137848,0.9591904878616332,0.0,accept,unanimous_agreement
1308639871,11193,"i'm still worry about the rtt problem caused by this pr, it's hard to trouble shoot the timeout reason(write command paused by oom with pending lazyfree). the client client pause mechanism is used in `client pause` command and failover, they are all active behavior, but oom is passive behavior. if the oom client pause lead to client timeout, how to quickly locate the root cause and measure the paused (timeout) time?",-1,-1,-1,0.9235879778862,0.7669243216514587,0.8623175621032715,-1.0,accept,unanimous_agreement
1308819033,11193,", imho, we need some flow control mechanism when clients are overloading redis with i/o data. especially when redis reach oom. it is better to delay the client or even to make it disconnect (like we do today when client's query-buf reach some limit) rather than aggregate too much data that is more than redis can handle.",0,0,0,0.980749249458313,0.9597968459129332,0.8886080384254456,0.0,accept,unanimous_agreement
1308858228,11193,"how about adding a latency monitor event? i.e. call `latencystartmonitor` when we initiate pause, and call `latencyendmonitor`. it doesn't really reflect the average impact on latency since a similar impact can be maybe caused buy one long pause, and also many small ones, but in some sense latency history can track these, and maybe we can even add a latency stats command which will give you some average latency over time. another idea we can replicate is the metrics added in #9031 and #9377. maybe these can help identify the problem, specifically for people who know where to look.. so maybe the next thing is to add something to the dreadful latency doctor.",0,0,0,0.95922589302063,0.9861220717430116,0.9706885814666748,0.0,accept,unanimous_agreement
1309700249,11193,"flow control is needed and interesting, but i have to say the current client pause mode by blocking client cannot work well. it only postpone command processing, but cannot control the real input and output flow. if client is pipelining, it will continue eat lots of querybuf memory, and if we keep lazyfree memory (unlink command or lazyfree expire), then the waiting would be infinite in theory.",0,0,0,0.9106814861297609,0.9172274470329284,0.965272068977356,0.0,accept,unanimous_agreement
1309829562,11193,"and about the monitor, i think the info metrics is better, always latency means the server's block or execution time, but client pause doesn't eat server's time.",0,0,0,0.9777945280075072,0.9735053777694702,0.9771510362625122,0.0,accept,unanimous_agreement
1310034961,11193,"ok, maybe i will add for now `total_time_client_paused_during_oom`. limiting out-buffers: what about simply to mark all the commands that we can safely execute during oom (say, cmd_oom) and drop the word 'write' from the terminology of this feature/fix. limiting in-buffers: as for the challenge to control query-buffers fill-up (that is masking somehow kqueue/kevent), we need to decide whether it should be part of this pr, or in another one.",0,0,0,0.9776599407196044,0.9940791130065918,0.9906843900680542,0.0,accept,unanimous_agreement
1337876672,11193,"the concept for this pr raises some questions for me. is this really a problem? is this the right ""solution""? here's what i'm thinking: * in general we should expect that freeing an item will be faster than allocating an item and filling it. in the case of freeing, we're avoiding the address dereference and memory access (as we don't need to actually read/write the block we are freeing). * in the scenario given in the initial statement, we try to create a worst-case freeing situation (lots of little blocks) with a best case allocation scenario (fewer large blocks). it seems that this would be a rather unusual scenario as we would expect that typically, when flushing and reloading data, that the new data would be somewhat similar in nature to the old data. * what's missing in this analysis is any discussion of fragmentation. while zmalloc's `used_memory` is being adjusted down by frees and up by allocations, real memory is behaving a bit differently. given jemalloc is a slab allocator, new pages are being allocated for the new large items, while old pages remain as the small items are being randomly deleted. this means that no physical memory is released until very near the end of the async flush. even though `used_memory` may remain constant, we can easily generate swap usage in this case. `used_memory` is a poor proxy for actual memory. and what's the worst that can happen here? in an extreme case, where the fundamental nature of the data is being dramatically changed and we are aggressively refilling the db after an async flush, some items which have been identified by the user as evictable are evicted. that doesn't seem unreasonable to me.",0,0,0,0.7604221105575562,0.951898992061615,0.5196223258972168,0.0,accept,unanimous_agreement
1339188781,11193,"hi , you are right that it is a very unusual scenario and most probably old data in the common case will look similar to the new data and therefore removal will be faster than insertion. yet, there might be a user out there that might have a db with two big hashes such that the first one holds many small keys and the other hash holds very big keys and now database reconstruction might lead to this improbable scenario. etc. on the other hand the price is not too high for (selective) blocking clients in case of oom+lazyfree. regarding your comment about `used_memory`, i am far from having an in-depth understanding in jemalloc integration into redis, but as i get it, the current approach attempts to tackle the memory issue at two distinct levels. memory allocation vs. redis keyspace. at redis keyspace it cannot do much but to count memory without being aware to paging and fragmentation. obviously `maxmemory` is not accurate value at os level, but it is a decent approximation, constant and deterministic. so the user must keep a margin of safety.",0,0,0,0.757256269454956,0.9332327246665956,0.9125200510025024,0.0,accept,unanimous_agreement
1339714251,11193,"regarding `used_memory` and jmalloc, for the scenario constructed here `used_memory` is not ""a decent approximation"". for discussion, let's assume the original data used 8-byte blocks and the new data uses 64-byte blocks. in jmalloc, all of the 8-byte blocks are placed into the same 4k pages. each 4k page is like an array of 8-byte values. similarly, the new 64-byte blocks are placed in pages dedicated to 64-byte values. each 4k page is like an array of 64-byte values. each time redis frees an 8-byte block, it frees space on one of the pages for 8-byte values. you can allocate a new 8-byte value into that space, but never a 64 byte value (even if multiple consecutive 8-byte values have been freed). now consider that we have 1000s of pages for 8-byte values - and we start randomly freeing. we would expect that all of the pages would be decreasing in fullness at about the same rate. so, at the point that half of the values have been deleted: * all of our pages are about half full * `used_memory` has decreased by 50% * **not a single page of memory can be released/reused** we need a page to be completely empty before it can be released or reused. for the given scenario, actual memory will increase as new key/values are loaded - quite possibly driving the system into swap. near the very end of the async flush, lots of memory will suddenly be released. the result is that this proposed mechanism solves an artificial problem. it keeps the `used_memory` statistic in check, while doing nothing to address the real problem of growing memory usage. this possibly prevents a few evictions of the new values (further increasing swap). however, the scenario seems so contrived that i question the value of trying to prevent such evictions.",0,0,0,0.9790512323379515,0.989025056362152,0.9840011596679688,0.0,accept,unanimous_agreement
1340584060,11193,"so before this pr, in some case that use flushdb async would risk oom kill, and others (if there's sufficient memory on the machine, or swap configured :nauseated_face:), would only risk eviction or oom errors (which imho is a logical bug that we can fix). swap trades oom kill with performance degradation, but still risking eviction and oom. i do think that's unreasonable, if the new data is ""evictable"", then it could be that after population the user can see half of the new data evicted, but he's well blow the memory limit. but maybe the more severe effect is oom (possibly terminating the workload, unless the user implements a retry on oom error). but anyway, we have 3 options: 1. leave it as it was 2. keep this pr to fix one problem without the other (accepting some complexity). 3. extend our efforts to improve the solution to solve the rss problem as well. are you promoting 1 or 3? p.s. on some orchestrations, there could be an external daemon monitoring the machine's free memory and adjusting maxmemory accordingly. in these, the current form of the pr won't have the problem you mentioned.",-1,0,-1,0.8382227420806885,0.798289954662323,0.9329620599746704,-1.0,accept,majority_agreement
1343182145,11193,"reading through this thread again, i'm really struggling to justify the existence of this feature. we still don't believe it's a real workload, just a theoretical one. i think we are placing too much weight into the argument that oom/evictions are logical errors. if you are ""loading"" data that is evict-able, data might be evicted. i also think it's possibly just as likely users will complain about elevated write calls stalling out their client and triggering reconnects. i think we should do this.",-1,-1,-1,0.9680094122886658,0.8388012647628784,0.913044571876526,-1.0,accept,unanimous_agreement
1348011231,11193,", i thought we already agreed, at corresponding [a link], that this is not theoretical race but reproducable scenario. this step forward to backpressure in case of oom and data that soon be released along the pipeline is rather well known pattern in networking. that is, momentary pause and backpressure in order not to overload the receiver. this our best effort to avoid redundant evictions. actually it surprised me that redis only has single limitation on the query-buffer which is `client-query-buffer-limit` (default 1gb per client. and behind it, it simply closes the client). and until that limit, querybuf is being filled up withtout any control.",0,0,0,0.8145906925201416,0.8392015099525452,0.637569785118103,0.0,accept,unanimous_agreement
1350240642,11193,"just because something is ""reproducible"" doesn't imply that it is actually a concern. on that issue you outlined a synthetic workload. i did agree to it then, but jim also brought up the fact that it doesn't really solve the memory issue, it's just shifting around the problem because of how the underlying jemalloc bins are constructed. don't conflate two disjoint problems. backpressure is a legitimate form of a flow control, and i buy into that notion, but i'm saying i don't think you can fairly say that it applies here. getting an oom message is also applying back pressure to the application to try again.",0,0,0,0.8111891746520996,0.5469464063644409,0.940199851989746,0.0,accept,unanimous_agreement
1387154994,11193,removing this pr from 7.2 project to sit stale until the day we'll have a reason to reconsider.,0,0,0,0.7382790446281433,0.9647623896598816,0.7706230282783508,0.0,accept,unanimous_agreement
2016982293,11193,"[a link] thank you for your submission! we really appreciate it. like many open source projects, we ask that you sign our [a link] before we can accept your contribution. **moticless** seems not to be a github user. you need a github account to be able to sign the cla. if you have already a github account, please [a link]. you have signed the cla already but the status is still pending? let us [a link] it.",1,1,1,0.99000883102417,0.993704617023468,0.9937825798988342,1.0,accept,unanimous_agreement
999797150,9963,"suggestion, why not using the script timeout mechanism here: [a link] we can create `scriptrunctx` and set it on redismodulectx, then we will be able to call this api and share the code, wdyt? in addition, i believe we should consider whether or not this can be called from a thread safe context (i believe that we should support it, many of our modules needs to call if from a thread). and we should probably add tests that uses it from a thread safe context if we decide that its supported.",0,0,0,0.958626389503479,0.9889512658119202,0.988636314868927,0.0,accept,unanimous_agreement
999837183,9963,"if feels to me like trying to tie two things that are not really related, and i suspect it'll cause complications in the future. maybe i'm missing something, but if it's just about replicating 10 lines of code that don't contain any very sensitive code, we better go with the cloning approach.",0,0,-1,0.921800434589386,0.8162206411361694,0.8269848227500916,0.0,accept,majority_agreement
999898580,9963,"maybe i'm missing something but if we have a module command that does something heavy, shouldn't it just block the client and do the heavy lifting in a thread? or maybe we're talking about a case where you deliberately want to prevent the server from executing most of the commands while your module command is doing something? is there a concrete example for such a case?",0,0,0,0.8581213355064392,0.9268248081207277,0.9783238172531128,0.0,accept,unanimous_agreement
1000086683,9963,"maybe i'm missing something, but if it's just about replicating 10 lines of code that don't contain any very sensitive code, we better go with the cloning approach. i can see how it can be useful on more places, for example `rm_call` and `scriptcall` has a huge code duplication of basically invoking commands on redis. i agree though that it will probably require more effort to match this code to what we need so we can leave it for the future i guess. the use-case is that when a module need to do a long operation which need to be atomic and then it needs to lock the gil for a long time.",0,0,0,0.9219481348991394,0.9646249413490297,0.9340041875839232,0.0,accept,unanimous_agreement
1005730719,9963,do we wanna add a test using thread safe context? and maybe one that calls this from the rdb_load callback? i guess we do..,0,0,0,0.9866438508033752,0.993323028087616,0.9931074976921082,0.0,accept,unanimous_agreement
1008579065,9963,/core-team please approve,0,0,0,0.981871485710144,0.9787766337394714,0.98884516954422,0.0,accept,unanimous_agreement
1009686071,9963,"it does like we processes event loop file events: [a link] which should include cluster events. i still believe the function should be able to determine what types of ""processing"" is done when it yields, since a failover could happen for example.",0,0,0,0.9696632027626038,0.986929178237915,0.9904386401176452,0.0,accept,unanimous_agreement
1013135901,9963,"do you mean that the module will provide some flags to tell redis what is ok to do inside that yield and what's not? i think this kind of interface is too complex and will be hard to support when future changes are done. also, isn't it already a bug that cluster failover can happen during busy scripts? in which case, we don't need to provide any flags for the module, but rather fix cluster.c or something like that.",0,0,0,0.9610123038291932,0.9350720643997192,0.7301619052886963,0.0,accept,unanimous_agreement
1013800558,9963,"my suggestion for a flag was more to differentiate two different cases. 1. let the module yield to allow cluster events to happen, so it isn't marked as dead by other nodes. the flag would indicate the command is making progress, but isn't done yet. 2. let the module yield to allow a client to potentially kill it. this will also allow cluster events to happen, but the main purpose is to allow us client commands to execute. this would also start throwing errors to clients, which is the behavior i would prefer to avoid. i also think something like extending the client pause blocking mechanism, the one that defers command execution based on what command was sent, for commands that can't be run during the yield context might also work. a newly established connection could send a kill command whenever it wanted, but all other clients would be silently blocked. for cluster failover, we should probably defer the failover till a later point.",0,0,0,0.93631511926651,0.9798266887664796,0.9692107439041138,0.0,accept,unanimous_agreement
1013826200,9963,"afaict all of that is applicable for scripts too. * the fact that we reject commands with busy error rather than postpone them for later. * the fact we can process cluster events (and ping) earlier so that the server isn't marked as dead. * the fact we should avoid triggering failover in the wrong state. so these are all old issues that we should someday improve, and the question is if it should affect the module api. considering the module doesn't know it's hung (if it did, it would have stopped), it just knows the operation can take time, and wants to allow the operation to be monitored and killed. let me make sure i understand, you're suggestion a flag to enable processing of the kill command like so (the new process_commands arg): [code block]",0,0,0,0.9060019850730896,0.9809707999229432,0.9594568610191344,0.0,accept,unanimous_agreement
1015233235,9963,"i added commits to handle what's decided in the core-team meeting * improve documentation around the renamed config * add a flag to rm_yield to let modules yield to redis tasks, but not to client commands. i still didn't implement the logic behind the later, for now pushed the api change to see that we're all good with it.",0,0,0,0.9163272976875304,0.94450181722641,0.9429518580436708,0.0,accept,unanimous_agreement
1015881662,9963,"i'd appreciate a review of my last commit. maybe we should rename `blocked_pause` to `blocked_postponed` since i think it just means that processcommand tried to process it and decided to postpone it. or, if you find bugs in my code, maybe i need to create a dedicated `blocked_yield` flag.",0,1,1,0.5970697402954102,0.7749627232551575,0.7298563718795776,1.0,accept,majority_agreement
1017147005,9963,it seems that `replconfcommand` and `quitcommand` missing `cmd_allow_busy`. [code block],0,0,0,0.9861056208610536,0.9952425956726074,0.994303584098816,0.0,accept,unanimous_agreement
1017171113,9963,good catch.. i guess it was a bad merge conflict resolution.,1,1,1,0.9383355379104614,0.8088885545730591,0.8086017966270447,1.0,accept,unanimous_agreement
1831181220,12817,`rm_unblockclient` is also not thread-safe. `rm_unblockclient()` -> `moduleblockedclienttimedout` -> `updatestatsonunblock()`(not thread-safe) fyi [code block] patch: [code block] command: `blockonkeys.popall k`,0,0,0,0.9855158925056458,0.995511531829834,0.99483984708786,0.0,accept,unanimous_agreement
1848961783,12817,"i lost track (or never had it), with all the comment threads here. can you close the ones that are resolved (and list them in the top comment), so we can focus on what's not resolved? or maybe it's too early, and you need more time to figure things out?",0,0,0,0.649644672870636,0.9178674221038818,0.8061134815216064,0.0,accept,unanimous_agreement
1851899163,12817,introduced: [a link] reason: adding fake client to server.clients_pending_write [code block],0,0,0,0.9783918857574464,0.990528404712677,0.9951730370521544,0.0,accept,unanimous_agreement
1859996236,12817,"`moduleownsgil()` isn't really right, `pthread_id` may not be numeric type or it may be struct, not sure if you've seen the earliest use of `__thread` by me to modify `processingeventswhileblocked`, but `__thread` isn't a c99 standard, and i'm not sure if i should use it or not, but it's a good choice.",0,0,0,0.9241231679916382,0.9533058404922484,0.5965679287910461,0.0,accept,unanimous_agreement
1860068582,12817,is the last comment in the context of the `el_poll_mutex` or a new discussion? i'd rather start discussions in threads so we can focus on each one. p.s. i think maybe we can compromise on _thread and have certain capabilities of redis require that. but the problem with the pthread_id is probably an issue.,0,0,0,0.9716261029243468,0.987677276134491,0.986591637134552,0.0,accept,unanimous_agreement
1864040391,12817,"i've updated the top comment, all the issues have been handled, please take a look.",0,0,0,0.967999279499054,0.926178514957428,0.9542236924171448,0.0,accept,unanimous_agreement
1864953185,12817,"thanks a lot. can you try to add some comment about severity on each of these? i.e. some are rather harmless, and others can cause a crash. also, please copy the statement about when each of them was introduced (version or pr)",1,1,1,0.9556007981300354,0.9422787427902222,0.977786898612976,1.0,accept,unanimous_agreement
1866171873,12817,i've updated top comment.,0,0,0,0.9826416373252868,0.9738476872444152,0.9943806529045104,0.0,accept,unanimous_agreement
1869457140,12817,"it's needed, but right now some other warnings are not related to race conditions, like thread leaks, as this pr needs to be backported, and i'd like them to be handled in other pr.",0,0,0,0.9811031818389891,0.9898157119750975,0.9892142415046692,0.0,accept,unanimous_agreement
1869470594,12817,"considering the size of this pr, i think we may choose to only backport some (safe, or high risk) parts of it, and not the entire thing. still, we can leave these other aspects for a later pr.",0,0,0,0.9804159998893738,0.9863226413726808,0.9834377765655518,0.0,accept,unanimous_agreement
1880618083,12817,don't forget to remove the rm_yield part from the top comment (maybe just add a quick reference to the other pr as a one liner at the bottom of it),0,0,0,0.974775493144989,0.992062509059906,0.9932330250740052,0.0,accept,unanimous_agreement
1882454289,12817,"race condition: [code block] in the commit [a link] i put `rm_blockedclientmeasuretimestart( )` and `rm_blockedclientmeasuretimeend()` in the gil, and it's constantly lock and unlock, otherwise the timeout callback wouldn't be triggered on sleep. can you have a better way to handle it?",0,0,0,0.9807218313217164,0.9822798371315002,0.9929401874542236,0.0,accept,unanimous_agreement
1883589732,12817,"i don't think it's reasonable to ask the module to lock the mutex before starting / stopping the time measurement. maybe the timeout callback can somehow terminate the thread before it messes with the timer? or maybe we should have let the module track the time on it's own, and then just report it to redis at the end? wdyt?",0,0,0,0.937656819820404,0.9083804488182068,0.9486591815948486,0.0,accept,unanimous_agreement
1884452198,12817,"force terminating threads is not good practice, are these tests misleading module developers? and if the module thread is already in `rm_unblockclient()`, will there be unpredictable behavior if we terminate it. but the purpose of this test is to reply to the user in the middle of blocking, not to wait for the thread to end before replying.",0,0,0,0.5884197950363159,0.9377650022506714,0.7841002345085144,0.0,accept,unanimous_agreement
1884477098,12817,"i see that's what you ended up implementing. the replying can be done during.. but reporting how much time was used to process the request can be reported to redis at the end (when you unblock the client, and the total time in that timer is added to the command stats)",0,0,0,0.9832006692886353,0.9894747734069824,0.9898681640625,0.0,accept,unanimous_agreement
1900233105,12817,full ci: [a link],0,0,0,0.9845400452613832,0.8597871661186218,0.9959890246391296,0.0,accept,unanimous_agreement
1910269722,12817,we do not release a new version for this pr right ?,0,0,0,0.8505541086196899,0.9658394455909728,0.9916754961013794,0.0,accept,unanimous_agreement
1910309500,12817,not yet. but we might backport some of these fixes to the next release of 7.2 (see the bottom of the top comment),0,0,0,0.9873194694519044,0.9930440783500672,0.9855415225028992,0.0,accept,unanimous_agreement
976198482,9748,/core-team please approve,0,0,0,0.981871485710144,0.9787766337394714,0.98884516954422,0.0,accept,unanimous_agreement
983395559,9748,-steinberg please don't forget to make a redis-doc pr.,0,0,0,0.9491779804229736,0.9780163764953612,0.9765541553497314,0.0,accept,unanimous_agreement
1120481244,9748,"this pr introduces a breaking change. i encounter it when using the tls configurations with files path and the user overrides the files in place. example: 1. generate certificate files (`tls-key`, `tls-cert`, `tls-ca`). 2. run the server and connect the client using these certificates. 3. override this certificates files with new ones (e.g. certificates rotation) 4. using the connected client to `config set tls-cert` with the same file name (the file overridden), and assuming that once one configuration is changed the redis will reconfigure all tls configuration (`tlsconfigure`). 5. before redis 7, the tls context was re-configured with the new certificate files, but after this pr it will not run the `apply` function as the `prev` and `new` values are identical. and the user will not succeed to connect with the new certificate files although the `config set` returned `ok`. code `config.c:stringconfigset`: `if (new != prev && (new == null || prev == null || strcmp(prev, new))) {` -steinberg wdyt? maybe we should add extra param to configs to enforce applying even if the values are identical. i'm not sure if this is a common usage of tls certificate files, where the user just overrides the exiting files and does not create new files with new names/paths. also in such a configuration, the `rolling back` mechanism not going to work as we don't save the internal state of the server but only the old configuration values.",0,0,0,0.9765605330467224,0.9900856018066406,0.988629937171936,0.0,accept,unanimous_agreement
1120761736,9748,"thanks for pointing that out. i'm ok with the idea of having a flag marking some configs as ""apply always"", but i'd try to avoid it if possible, just to reduce complexity. specifically in this case the problem is that what we're storing isn't the configuration but a ""point to the configuration"" (the file name being the pointer). this is contrary to all other config data in redis and therefore i'd aim for deprecating the mutability of the tls file related configs. and instead we should set the values directly: `config set tls-key-data ""-----begin rsa private key-----....""`. this will resolve these issues and make the key/cert/ca data inline with how redis stores all other configs. in other words store the config itself and not a file name. please note that a failure to apply these configs couldn't have rolled back previously either for the same reason. when implementing this i was aware of this and initially panicked the server. later suggested panicking was extreme so i changed the code to: [code block] as a workaround which seems also like the logical thing to do, i'd just use a different file name, this will also support rolling back in case of failure.",1,1,1,0.6152414679527283,0.7924657464027405,0.9677075743675232,1.0,accept,unanimous_agreement
1120908388,9748,"-steinberg using a different file should be considered better practice that solves both problems. but for backwards compatibility, i'd consider a ""volatile config"" flag that indicates the value is only a pointer so we should always apply and either never attempt rollback or ignore failures.",0,0,0,0.9784084558486938,0.9920755624771118,0.9634190201759338,0.0,accept,unanimous_agreement
1001209245,10004,"i've set this to resolve the issue that describes it, assuming that all the ideas specified there are handled here. please skim over the description and discussion there, and list here (maybe in todo section of the top comment) if there's something not handled yet.",0,0,0,0.9869447350502014,0.9786831140518188,0.9934024214744568,0.0,accept,unanimous_agreement
1001212633,10004,"function flags was missing, added it to todo list.",0,0,0,0.985966145992279,0.9930686950683594,0.994055986404419,0.0,accept,unanimous_agreement
1001974790,10004,"fixed the comments. in the past i argued the same about ""engine"". the term ""function"" is a lost cause (in the context of redis, a ""function"" is a ""script function"". but maybe ""library"" should be called ""functions library"" (or ""functionslib"")? same as we called ""engine"" a ""script engine"" i think its ok to call it just library, but i also thought its ok to just say engine so it make sense we disagree :) that said, i do not mind renaming. how about: * libraryinfo -> functionlibinfo * librariesctx -> functionlibsctx let me know if you are ok with it.",1,1,1,0.9469990730285645,0.9852247834205629,0.9926576018333436,1.0,accept,unanimous_agreement
1002118432,10004,"i'm not sure if just renaming these two is enough. maybe there are other functions (procedures) that should be renamed, comments that should be updated, variable names or command arguments, etc. i suppose that in the context of a function command, the term ""library"" is ok. and same goes for anything in functions.c. if you wanna consider this change, please grep the diff in this pr for the word ""library"". skip functions.c and all the function json files, and see what you're left with.",0,0,0,0.9745614528656006,0.96699720621109,0.9392738938331604,0.0,accept,unanimous_agreement
1002427968,10004,let me know what you think on the last pr (renaming),0,0,0,0.9840587973594666,0.9886639714241028,0.9674260020256042,0.0,accept,unanimous_agreement
1002431273,10004,lgtm (only looked at the commit comment) i assume you used this method to spot them:,0,0,0,0.9867416024208068,0.9808732867240906,0.994566798210144,0.0,accept,unanimous_agreement
1002432792,10004,i actually look everywhere where any function api is used and then find out what need to be renamed. will do another round to make sure i did not miss anything.,0,0,0,0.9765576720237732,0.9392979741096495,0.9907549023628236,0.0,accept,unanimous_agreement
1002456349,10004,/core-team this is ready for merge. please read the top comment and approve,0,1,0,0.9525968432426452,0.5042541027069092,0.9843798875808716,0.0,accept,majority_agreement
1002478268,10004,redis functions flags is discussed on [a link],0,0,0,0.9888030886650084,0.9874547719955444,0.995573878288269,0.0,accept,unanimous_agreement
1003030093,10004,"there are api's on `redis` object that we do allow on `function load` context, for example `redis.log`. do you think we should remove it?",0,0,0,0.9897729754447936,0.99488765001297,0.9922290444374084,0.0,accept,unanimous_agreement
1003033369,10004,we'll soon probably wanna add `redis.version()` so the lib can register different functions / flags depending on the version.,0,0,0,0.9885390400886536,0.9918481111526488,0.9919400811195374,0.0,accept,unanimous_agreement
1003128132,10004,"i think we should opt-in on what we provide and not opt-out, and using a different name might make better distinction about these two run modes.",0,0,0,0.9825777411460876,0.9623597264289856,0.9733341932296752,0.0,accept,unanimous_agreement
1003598885,10004,"i see your point and i agree, so i guess we can introduce a new api, `library`, that will be available only on the load run and we can put there everything that is available to the load run (`library.register_function`, `library.log`, ...). wdyt?",0,0,0,0.9625867009162904,0.8502182960510254,0.9836528301239014,0.0,accept,unanimous_agreement
1004122661,10004,", added `library` api and update top comment, let me know what you think.",0,0,0,0.9843866229057312,0.9094862341880798,0.9553077220916748,0.0,accept,unanimous_agreement
1004240568,10004,"please notice that i added another commit (the last commit) that add `denyoom` flag to `function load` and `function restore`. because we consider functions as data and those commands potentially will add new functions (data), i believe it make sense. let me know what you think.",0,0,0,0.8350120782852173,0.855228841304779,0.9561939239501952,0.0,accept,unanimous_agreement
1006486767,10004,"rename `library -> redis`, updated top comment.",0,0,0,0.9854210019111632,0.9935362935066224,0.9956432580947876,0.0,accept,unanimous_agreement
1238045854,11199,"thanks , i have fixed your comments / replied to them. waiting for more comments from the community.",1,1,1,0.5639821887016296,0.7859916090965271,0.9711973071098328,1.0,accept,unanimous_agreement
1257170703,11199,"i have fixed/replies to your comments. summarising the open questions that remains: 1. whether or not its ok to apply some of the suggested changes on a followup pr and keep this pr safer for cherry pick. related comments: * [a link] * [a link] * [a link] 2. api generalisation where else can this api can/should be used, related comments: * [a link] * [a link] notice that i am not resolving the comments so you can go over them and verify the fixes. please resolve when you approve or let me know if you prefer that i will resolve them myself.",0,0,0,0.9418189525604248,0.8775637149810791,0.6574019193649292,0.0,accept,unanimous_agreement
1279894546,11199,"/core-team please take a look. please note the new ""execution unit"" terminology.",0,0,0,0.982876718044281,0.9777462482452391,0.9835125803947448,0.0,accept,unanimous_agreement
1282695930,11199,conceptually approved in a core-team meeting. please take a quick look to be on the safe side.,0,0,0,0.9312556982040404,0.964314579963684,0.9166961908340454,0.0,accept,unanimous_agreement
1308384343,11199,"pr was conceptually approved in a core-team meeting, with the exception of [a link] it was also approved to be eventually backported to 7.0 in due time.",0,0,0,0.985014021396637,0.9914693236351012,0.9920485615730286,0.0,accept,unanimous_agreement
759273128,8315,"if you want to take another pass, i think everything is fixed now. some open questions i still have: 1. failoverto adds a fixed 5 second to the timeout if it detects a replica is caught up with it, regardless of what value is passed in. this is probably too high, should be configurable somehow. 2. both nodes can end up as masters if the original master gives up after if it has sent the psync failover to the target replica. this seems like an edge case that should probably be handled, but isn't currently in this draft of the pr. 3. related to a previous comment about protocol version. if the primary is on the latest version and the replica isn't, the primary will demote itself but fail to handoff, so there should be no harm. we should also never send failover in that field for any reason. 4. do we want a failover abort option? i think that was originally a concern, i'm not sure it's that necessary. 5. the pause stays in place until the the full-transfer has started/psync has succeeded. this is the most defensive option, but could also unpause the moment we demote the master.",0,0,0,0.9310429096221924,0.9812623858451844,0.7316327691078186,0.0,accept,unanimous_agreement
762467496,8315,"to me the main motivation to implement this as part of redis is to provide a standard and safe way to handle manual failovers. i think this approach can be slightly modified to make it safer and not necessarily less useful, mainly thanks to having the client read-only pause option: * initiate client read-only pause as soon as failover begins. * automatically unpause only on the happy path, i.e. `psync failover` completed successfully. * not sure if `timeout` is really needed, but if we think it is - apply it only while waiting for offsets to sync but not beyond. * if `psync` fails we may retry until successful or until the user aborts (and unpause).",0,0,0,0.7667369842529297,0.947321116924286,0.9271638989448548,0.0,accept,unanimous_agreement
763179586,8315,"looks like allen is busy, so i will work to update the pr. to summarize our conversation: * rename it from failoverto to just failover. it will now look like failover [abort] [force] [timeout x] [to [hostname ip][any one]]. * the timeout only applies to waiting until we have synced up with the replica, at that point the timeout is no longer used. we want to prevent multi-primary when possible. * the previous master will unpause whenever failover ""ends"" which will be either on the replica rejecting or accepting the command. all other scenarios will leave the previous master in a paused state.",0,0,0,0.8956139087677002,0.9689805507659912,0.9546608924865724,0.0,accept,unanimous_agreement
764396907,8315,see top comment again for overview.,0,0,0,0.9775336384773254,0.9869306087493896,0.9947521686553956,0.0,accept,unanimous_agreement
767025874,8315,"ack, will move to optional to. you're right that the most common use case will be people with a single replica.",0,0,0,0.9612298011779784,0.868190348148346,0.9840394854545592,0.0,accept,unanimous_agreement
767154674,8315,look good? do you guys have any thoughts?,0,1,0,0.5241059064865112,0.9176730513572692,0.640362024307251,0.0,accept,majority_agreement
767882636,8315,failover doc: [a link],0,0,0,0.9803786277770996,0.9841153621673584,0.9170238971710204,0.0,accept,unanimous_agreement
1002889590,8315,"hello, i'm looking for a way to trigger atomic failover without data loss. i'm happy to find that this command has been implemented recently. however, it seems like the clients have to be reconfigured manually to use the new master, which requires a restart of all the clients. currently i'm using a single-master redis cluster (i did this by manually assigning slots 0-16383 to a freshly created redis instance, copy the nodes.conf to the relevant redis server, and then restart that redis server with cluster support enabled). then i am able to use clients that support redis cluster and trigger manual failover with the `cluster failover` command. then the clients will automatically be redirected to the new master, which is good. the drawback is that cross-slot transactions are disabled, and i'm planning to update my app to prefix every key with a fixed hashslot to mitigate that. will such a usage scenario get better support in the future? like better documentation and tools for single-master clusters, config option to allow cross-slot transactions in single-master clusters, etc.",1,1,1,0.9210676550865172,0.969126284122467,0.8453480005264282,1.0,accept,unanimous_agreement
745764498,8170,"noted the individual comments, i made more progress today, hoping to update the pr tomorrow with tests.",0,0,0,0.9090194702148438,0.9430804252624512,0.9863260984420776,0.0,accept,unanimous_agreement
750604763,8170,"i think the only thing that's not covered is lazy expiry, did i miss anything about that? (or other things that's not covered yet?)",-1,0,0,0.6727572083473206,0.9781625866889954,0.962698459625244,0.0,accept,majority_agreement
750715299,8170,"yes, i somehow missed that from your original comment.",-1,0,0,0.8275529742240906,0.8214759230613708,0.9856112003326416,0.0,accept,majority_agreement
754307009,8170,"ok, now i think everything is updated.",0,0,0,0.9800018668174744,0.9214216470718384,0.9631218314170836,0.0,accept,unanimous_agreement
754594720,8170,it looks like you pused a one force-push that includes both a rebase and new code. i'm having hard time to know what i already reviewed. (i see 11 commits but all have today's date).,-1,-1,-1,0.9527263641357422,0.5809224247932434,0.892554759979248,-1.0,accept,unanimous_agreement
754601907,8170,"i think i manged to figure it out, the new commits are the last 3",0,0,0,0.9759262204170228,0.6903554797172546,0.8359271287918091,0.0,accept,unanimous_agreement
755960038,8170,/core-team please approve (see top comment),0,0,0,0.9820288419723512,0.9752839207649232,0.9879403114318848,0.0,accept,unanimous_agreement
1004061006,10043,"thanks for your pr.. started looking at it.. i now realize that deleting help.h would mean that new redis-cli will be unable to provide any hints when connected to old servers (in the past it just used to provide the wrong hings). one alternative is to keep the old help.h just for use in old servers but that would probably complicate the code, and also, i suppose it'll mean we'll have to keep the old help.h forever. anyone has any other ideas before we embrace that ""limitation""?",1,1,1,0.7670628428459167,0.8709913492202759,0.9599647521972656,1.0,accept,unanimous_agreement
1004100235,10043,"right - that would mean keeping the existing help.h mechanism as a fallback for when command returns the old format. basically, keep all the code this pr deletes, behind a conditional. we could keep it for a predefined transition period, until the old server version is no longer supported. but sure, it would complicate the code.",0,0,0,0.9257120490074158,0.9583356380462646,0.9753850698471068,0.0,accept,unanimous_agreement
1004160026,10043,ok. i don't like it.. but just wanted to raise it.,-1,-1,-1,0.9753021597862244,0.985165238380432,0.9810887575149536,-1.0,accept,unanimous_agreement
1004922181,10043,"for the record, i guess it's not so bad if a new redis-cli doesn't show hints when running with an old redis server (i would imagine the more common case is an old redis-cli with an new redis-server)",0,0,0,0.9134434461593628,0.8046776652336121,0.8434939384460449,0.0,accept,unanimous_agreement
1004923721,10043,"i'm not sure that's right.. someone managing many redis servers, will maybe update his redis-cli but keep using it for both old and new servers.",-1,-1,0,0.732961118221283,0.7867894768714905,0.8400273323059082,-1.0,accept,majority_agreement
1010713661,10043,"fyi, #10056 was merged.",0,0,0,0.9844760298728944,0.9881752729415894,0.9920985698699952,0.0,accept,unanimous_agreement
1011344731,10043,can redis-cli assume that command info and command docs return the commands in the same order? it would simplify the code.,0,0,0,0.9884408712387084,0.994187355041504,0.9936904907226562,0.0,accept,unanimous_agreement
1011366668,10043,"currently that's the case, and i can't think of any reason why the implementation could be changed in a way that'll break that. i.e. both commands recursively run on the same dict. that said, it seems like an invalid assumption to make (each command has it's own documented semantics, but they have nothing to do with each other, formally)",0,0,0,0.9366959929466248,0.8458880186080933,0.9272878766059875,0.0,accept,unanimous_agreement
1012657262,10043,"command info is not necessary for this purpose. the code now only calls command docs, which has all the relevant information for the help strings.",0,0,0,0.9868990182876588,0.9926344752311708,0.9939822554588318,0.0,accept,unanimous_agreement
1021297424,10043,forgive me for the luck of focus. what's the status here? ready for merge?,-1,1,-1,0.7937738299369812,0.7192831039428711,0.9873766899108888,-1.0,accept,majority_agreement
1021337997,10043,"as far as i'm concerned, it's ready. i don't get the sense that it's had a thorough code review, though. i also don't know if there are any tests that are usually run on redis-cli? on tue, jan 25, 2022 at 5:19 pm oran agra ***@***.***> wrote:",0,0,0,0.7617735266685486,0.5413714647293091,0.8780502676963806,0.0,accept,unanimous_agreement
1021386840,10043,"i'll give it another review.. just wasn't sure there are still changes pending or it's ready from your side. there are some tests in `tests/integration/redis-cli.tcl` but i i'm not sure if the infrastructure can serve for our purpose here. and also, i suppose manual testing is enough for this feature, not sure we need regression tests.",0,0,0,0.8934619426727295,0.9258347153663636,0.9079837203025818,0.0,accept,unanimous_agreement
1021992039,10043,"another thought: presumably in a separate pr, would anyone object to breaking this up into multiple files? 8800 lines is a bit much much for one source file.",0,0,0,0.9324753284454346,0.9768795371055604,0.9449838995933532,0.0,accept,unanimous_agreement
1022019836,10043,"i agree. but i don't see a point in just breaking it up in a pr that does only that.. we should do some major revamp in it some day, maybe even split it into multiple tools, or multiple units bundled in one tool. i don't have a concrete idea. just agree that what we currently have is a mess.",-1,-1,-1,0.8937321305274963,0.9659771919250488,0.7986952662467957,-1.0,accept,unanimous_agreement
1022606382,10043,"okay, i added support for help entries for command group names. i've also done a bit of code cleanup, and i hope i caught all the style fixes. please take another look.",0,0,1,0.8360320925712585,0.7044330835342407,0.8628367781639099,0.0,accept,majority_agreement
1022648338,10043,i don't understand why these tests are timing out. any leads?,0,-1,-1,0.7132134437561035,0.7595998048782349,0.5749611854553223,-1.0,accept,majority_agreement
1025880833,10043,"what's the next move? the options i see are: 1. drop help.h, so that redis-cli is not showing any tips for old servers. 2. keep help.h completely static (representing v6.2 content), and used only when command docs is unsupported. 3. somehow (and for some reason), use both (merge data from command docs with the static one from help.h) my thoughts 1. this pr currently implements 1. 2. i really wanna avoid maintaining this help.h file, so keeping it static with the contents from redis 6.2 seems ok. 3. if for some reason we want to let people skip issuing command docs on startup (as an optimization), and still get up to date tips, we'll need to take option 3, but i don't think it's a must, so i rather not.",-1,0,0,0.4942376017570495,0.8840783834457397,0.882756233215332,0.0,accept,majority_agreement
1026555114,10043,"we discussed this in a core-team meeting, and concluded we don't wanna introduce a regression when new redis-cli talks to an old server. so we'd like to pursue option 2 above. can you handle it?",0,0,0,0.964108943939209,0.9874838590621948,0.9881284832954408,0.0,accept,unanimous_agreement
1026632725,10043,"so, maintain the old behavior as a fallback? including retrieving additional commands from `command`? it's certainly doable. i'll see what i can do. is there a timeline for this?",0,0,0,0.973676860332489,0.9846634864807128,0.97806978225708,0.0,accept,unanimous_agreement
1026687702,10043,"next release candidate will be in about 3 week, would be nice to finish this campaign by then. retrieving additional info from command would only be useful for module commands, right?",1,0,0,0.6400747299194336,0.9547285437583924,0.9022884964942932,0.0,accept,majority_agreement
1026716824,10043,will we ever the static help.h?,0,0,0,0.9795804619789124,0.9852206707000732,0.990458607673645,0.0,accept,unanimous_agreement
1026740923,10043,maybe in a few years (10?) we'll be able to delete it. we can discuss this again in the future and decide.,0,0,0,0.9452313780784608,0.989260196685791,0.9791820049285888,0.0,accept,unanimous_agreement
1026749096,10043,"yes, for module commands.",0,0,0,0.984130561351776,0.989551842212677,0.9898446798324584,0.0,accept,unanimous_agreement
1029533771,10043,"okay, i've reinstated the static help.h file for use with pre-7.0 servers, along with the code that generates the help tables from it. i still can't figure out why some of the tests are timing out. any leads in investigating the problem would be appreciated. [code block]",0,0,1,0.9513203501701356,0.8458502888679504,0.9286897778511048,0.0,accept,majority_agreement
1029849050,10043,i can't figure out why the test hangs on gh actions (it passes locally). maybe you can figure it out?,0,0,0,0.9676698446273804,0.9408802390098572,0.9830804467201232,0.0,accept,unanimous_agreement
1029923749,10043,it's been broken since at least 896dad5,0,0,0,0.9844533801078796,0.8325890898704529,0.9842975735664368,0.0,accept,unanimous_agreement
1030038260,10043,it was actually easy to reproduce locally if using the same `make` line as the one the ci uses. the test hang because redis-cli crashed on sigabrt because realloc was called with a bad address.,0,0,0,0.9550695419311525,0.939858853816986,0.988326609134674,0.0,accept,unanimous_agreement
1030047448,10043,"p.s. `zrealloc` uses `realloc`, but has this in smalloc.c: [code block] so when you call `realloc` from redis-cli.c, it uses libc allocator. but if you debug this with `make valgrind` or any system that doesn't use jemalloc by default, then both use libc, which is why it didn't reproduce locally for you.",0,0,0,0.9869706630706788,0.9945504069328308,0.9930311441421508,0.0,accept,unanimous_agreement
1030049875,10043,"okay, i've learned something. thanks. on fri, feb 4, 2022, 16:38 oran agra ***@***.***> wrote:",1,1,1,0.8926480412483215,0.979453146457672,0.9898932576179504,1.0,accept,unanimous_agreement
1030598276,10043,"maybe we should automatically hide suggestions based on version? i.e. send hello or info at startup, and use the version to hide irrelevant suggestions? or maybe we should remove commands and args that are newer than 6.2 from help.h when generating it? the first option is nicer, but not sure how much effort we wanna invest in old version support (since in versions newer than 7, it'll happen automatically since we use command docs)",0,0,0,0.9522245526313782,0.9872307181358336,0.8329480886459351,0.0,accept,unanimous_agreement
761061836,8324,"for the 3 bugfixes you found (zdiffstore etc, mentioned above), i think it would be a good idea to issue a separate pr, so we can cherry pick that fix to 6.0.",0,0,0,0.9643086791038512,0.9802483916282654,0.9853447079658508,0.0,accept,unanimous_agreement
761763828,8324,"/core-team please approve the new command output fields, see example in the issue, and the new module api",0,0,0,0.9859206080436708,0.9830495715141296,0.987092137336731,0.0,accept,unanimous_agreement
765541062,8324,"last commit handles the above comments by + add the ""startfrom"" arg to the keyword spec(see top comment)",0,0,0,0.9860585331916808,0.9902386665344238,0.9954102635383606,0.0,accept,unanimous_agreement
766143117,8324,"/core-team note the changes: 1. a new argument for the `keyword` spec, named `startfrom` 1. a new keyspec flag named `incomplete`. see the top comment about migrate command.",0,0,0,0.9876213669776917,0.9937546253204346,0.988527774810791,0.0,accept,unanimous_agreement
768077667,8324,"i don't have a clear use case for the read/write flags for the keyspecs, but i think we should make that distinction, i'm sure it'll come in handy, even inside redis one day. this is in part related to the lookupkeyread vs lookupkeywrite saga, and in part lesson learned from redis on flash. is is good to know which key is gonna be only read from, which one will be just overwritten, and which one will be modified (both read and write). maintaining this info in the command table is not a huge overhead, and since for each command we always know what each key argument is gonna be used for, there's never any doubt as to what to fill in that field.",0,0,1,0.5984741449356079,0.7775324583053589,0.7634314894676208,0.0,accept,majority_agreement
768103411,8324,"sure, i guess my concern is we're optimistically building out this functionality without a clear understanding of how it will be used. the key location use case, which is well understand for cluster mode and acls, doesn't really need it. the one way decision is exposing this to clients. even if we have use cases inside redis, it may not be relevant to clients. the api is solid though, we could always introduce new specs/deprecate old ones. the risk is small is small in the end, i'm just not convinced we should release in for 6.2.",0,0,0,0.8607763648033142,0.9274534583091736,0.6485888361930847,0.0,accept,unanimous_agreement
768932060,8324,"and myself got to discuss this a bit more. in a nutshell, we conclude that: * we can go through another round of improvements to the keyspec proposal above to make it much more flexible and support more command patterns, without increasing complexity much * considering all the different modules out there, we won't get 100% coverage unless we come up with full command grammar. this will require full parsing - much more complex * there is still some uncertainty about the value of this, we need to hear clearer voices from the client ecosystem about how useful this is going to be calling and other redis client library developers to provide their perspective. tl;dr the idea is to extend `command info` significantly to describe more/most/all command patterns so clients could extract key names from command arguments autonomously, and we need to iterate the design and get some validation of the value in doing that.",0,0,0,0.7192763686180115,0.933904230594635,0.9444659948349,0.0,accept,unanimous_agreement
768959882,8324,lettuce doesn't use `command info` for key/argument patterns at all. we actually use it only to determine which commands are available (command names). the key/argument pattern is so complicated that it's easier for us to just declare methods statically instead of assembling commands from `command info`.,0,0,0,0.9368427395820618,0.9898552894592284,0.9843221306800842,0.0,accept,unanimous_agreement
768980849,8324,"thanks. 1. so that means that each time a new command is added you have to implement a to extract the keyname arguments? 2. forgive me for my ignorance (and laziness), does lettuce supports running arbitrary commands (like execute_command in redis-py)? if that's the case, then you want to let users use commands that lettuce doesn't support yet (or module commands), so using command getkeys or the new keyspecs idea can solve that. 3. would you consider using the new keyspecs when available? do they look usable to you? it can simplify your work, but you'll need to keep the old code in case the redis server that's used doesn't support them.",1,1,1,0.9419569373130798,0.8450429439544678,0.8991866707801819,1.0,accept,unanimous_agreement
769035237,8324,"exactly. this more due to the fact that with java you kind of expect a method declaration to assist users. aside from that, we have a method to dispatch plain commands (`dispatch(""set"", ""key"", ""value"")`-style). based on that, the user decides what arguments come into a command and we don't have the immediate need to know which argument is a key or a keyword. we have a parser for the `command` response that returns a value object to the caller. we would certainly add the keyspec there for the sake of having complete support for the response but we don't have any use for it.",0,0,0,0.943044364452362,0.9871104955673218,0.9764719605445862,0.0,accept,unanimous_agreement
769038148,8324,so with `dispatch` (which would be needed to use new command or module commands) how do you know which cluster node to forward the command? (using either command getkeys or relying on the `-moved` would be an overhead).,0,0,0,0.9887646436691284,0.9944785833358764,0.992170751094818,0.0,accept,unanimous_agreement
769040347,8324,"excuse my oversimplified representation of our api. the actual code to use `dispatch` is: [code block] `commandargs` captures which argument is a key and which one is a value, mostly to apply the correct encoding. that also allows us to reason about the command argument ordering.",0,0,0,0.6157931089401245,0.746564507484436,0.8299556374549866,0.0,accept,unanimous_agreement
909971769,8324,"we can match the old output of command to the new one and see that the legacy spec is ok. i did i.e. write some test that implements what we expect a client to implement based on the specs, then somehow compare the result of that with getkeys on either a hard coded list of commands, or some fuzzer? not sure a fuzzer will be beneficial (especially for commands that have a keyword spec) wdym by the getkeys suggestion? to actually write the logic that retrieves the keys when given argv + command's output? and then tests it against a predefined group of argv arrays and compare to command getkeys?",0,0,0,0.9716710448265076,0.990222692489624,0.9881012439727784,0.0,accept,unanimous_agreement
911943762,8324,"/core-team please approve again. quite a lot have changed since last time. i think the main difference is that we've split the specs into two steps (start_search and find_keys), and changed the module apis. the top comment was updated so please have a look at it. i suggest to focus on reviewing the module api, and the output of command command. note that the changes to command command are gonna be reverted soon (that's why we don't document it yet), and instead will be probably put into a new command**s** command (the old command will only be kept for backwards compatibility, and the one one will have info on sub-commands). also note that the command table is gonna be changed soon to a json file, so you can ignore the format there.",0,0,0,0.7607077360153198,0.8994554877281189,0.9173358678817748,0.0,accept,unanimous_agreement
916086925,8324,approving the concept and apis.,0,0,0,0.9768447279930116,0.9779940247535706,0.8029172420501709,0.0,accept,unanimous_agreement
918924275,8324,running daily ci before merging: [a link],0,0,0,0.9866312146186828,0.9860447645187378,0.9958332777023317,0.0,accept,unanimous_agreement
919040031,8324,valgrind found some issues in the module api: [a link] and some test failure on alpine linux: [a link],0,0,0,0.9618598818778992,0.984119951725006,0.9931447505950928,0.0,accept,unanimous_agreement
1897638878,12913,"i like this idea, but one concern is that cluster with 16384 slots, in the worst case scenario, we would need to iterate through all the slots to obtain complete information about ht[0] and ht[1]. perhaps we need to use `rehashingstarted` and `rehashingcompleted` to optimize this process.",1,0,0,0.6113770604133606,0.5135714411735535,0.5723434090614319,0.0,accept,majority_agreement
1898080094,12913,"yes, let's do that (keep track of it rather than calculate it on the spot)",0,0,0,0.9845761060714722,0.9886474013328552,0.9917091727256776,0.0,accept,unanimous_agreement
1951636796,12913,"now that ""kvstore"" is merged, i have to rewrite a lot of things. `mem_overhead_hashtable_rehashing` and `mem_overhead_hashtable_lut` are now maintained in `kvstoredictrehashingstarted` and `kvstoredictrehashingcompleted`, and therefore it is inconvenient to tell whether the rehashing dict is from dbs or from pubsub dicts. but i guess including the overhead of pubsub is somehow ok? sometimes pubsub also takes a lot of memory and we need to check it. and since `databases_rehashing_dict_count` may not be so useful and now we need to iterate all dbs to get the exact count, i temporarily dropped it. or we can maintain the overhead inside kvstore structure and when `info` is called, we gather the overhead from all dbs' kvstore. and since in this case we have to iterate all dbs, we can keep the `databases_rehashing_dict_count`.",0,0,0,0.7953108549118042,0.941457986831665,0.9529159665107728,0.0,accept,unanimous_agreement
1954557025,12913,"discussed and approved this in a core-team meeting. one metric in info, and all 3 in memory stats. i haven't reviewed the recent changes, please ping me when this is ready.",0,0,0,0.9343878626823424,0.8768221139907837,0.7423425912857056,0.0,accept,unanimous_agreement
1955986676,12913,"i realize we need to update the reply schema, see [a link]",0,0,0,0.9834798574447632,0.971098244190216,0.992768168449402,0.0,accept,unanimous_agreement
1958530760,12913,"just realized that in test ""redis can resize empty dict"" in other.tcl, it is assumed that `memory stats` shows only the overhead of non-empty dbs. but now all dbs' overhead is displayed and so we need to change the test.",0,0,0,0.9877963662147522,0.9930374026298524,0.98689204454422,0.0,accept,unanimous_agreement
1959069841,12913,"the test should read `overhead.hashtable.main` of db9 (the db whose dict expanded and then emptied). in the test, it use to be that only db9 has a key (`r set a b`) and so `memory stats` shows only db9's overhead. in this case we only need to find the first `overhead.hashtable.main` field and read its value. however in this pr `getmemoryoverheaddata` is modified and `memory stats` shows not only non-empty dbs but all dbs, ([a link] so we need to specify it is the db9's overhead that we need to read. the test would also pass if it reads other db's overhead, but that's meaningless.",0,0,0,0.970014750957489,0.9917165637016296,0.9915900826454164,0.0,accept,unanimous_agreement
1959115603,12913,"ohh, you mean that so far it just looked for any `overhead.hashtable.main`, and you now modified it to be `db.9 overhead.hashtable.main`. ok, so that's solved then. let's wait to see if has anything else to argue and then merge it.",0,0,0,0.9724884033203124,0.8659857511520386,0.9603219628334044,0.0,accept,unanimous_agreement
1963857561,12913,any more problems?,0,0,0,0.6583727598190308,0.9502967000007628,0.9399369359016418,0.0,accept,unanimous_agreement
1970701995,12913,"my ocd is kicking... i feel it's best to add ""db"" to the names of these metrics to avoid the misunderstanding that they refer to all hash data types: info memory * `mem_overhead_hashtable_rehashing` -> `mem_overhead_db_hashtable_rehashing` memory stats * `overhead.hashtable.lut` -> `overhead.db.hashtable.lut` * `overhead.hashtable.rehashing` -> `overhead.db.hashtable.rehashing` wdyt? about the db prefix error, could you change `database.dict.rehashing.count` to `db.dict.rehashing.count` and modify the regular expression in memory-stats.json to `""^db\\.\\d+$""`, then try again?",-1,-1,-1,0.983143985271454,0.7786734104156494,0.9783798456192015,-1.0,accept,unanimous_agreement
1973716956,12913,just retrospectively acknowledging that the updating wording seems better to me as well.,0,0,0,0.9613272547721864,0.9085711240768432,0.984752357006073,0.0,accept,unanimous_agreement
1382246450,11708,/core-team hey team. this is a new module api that allows module commands to register themselves to the built-in acl categories.,0,1,0,0.967069685459137,0.9625652432441713,0.9142888188362122,0.0,accept,majority_agreement
1399128863,11708,"hi team, i have pushed the code with the necessary changes. please take a look and review them. thanks!",1,1,1,0.978469967842102,0.9921417236328124,0.9909812808036804,1.0,accept,unanimous_agreement
1399895153,11708,"we need to consider how this impacts users on upgrade. i.e. a command is added to a category implicitly exposes it (without user's intention), or blocks it (breaking an app). i suppose this potential for breakage is not about upgrading redis (to include this api), but rather about upgrading the module. and not necessarily when the module starts using this api, but any time it changes the categories (i.e. even 5 years from now, when a module upgrade adds a command to a category). and in fact it's arguably no different than cases were redis [a link] categories. i guess it ultimately depends on the module developers, not redis. and like many other breaking changes, we just have a choice between keeping something bad (inconsistent, confusing or hard to use) forever, or fixing it and accepting the uncomfortable transition. still, i'd like to hear opinions and suggestions on how we can mitigate this.",0,0,0,0.92222261428833,0.9673107862472534,0.9793304800987244,0.0,accept,unanimous_agreement
1400677483,11708,"i think the current choice is the right tradeoff, but also interested in other options. given the security implications, i feel pretty strongly that we shouldn't just start opt'ing in modules into acl categories on an engine upgrade, it makes more sense on a module upgrade. i think the changes we made to acl categories before were all very tactical and low risk generally, and they still made me feel uncomfortable. i also would like to give modules the flexibility to choose what categories they want to be a part of.",-1,0,0,0.6783077716827393,0.6167851090431213,0.7588096857070923,0.0,accept,majority_agreement
1422189542,11708,"this pr was conceptually approved in a core-team meeting. can be merged after the last batch of comments are addressed we are ok with blocking the registration of command, configs, and data type outside the onload (must be mentioned in the top comment as a breaking change). one other concern that was raised is if the api would be convenient. considering that all modules are gonna want to use this, but they must remain compatible with older versions of redis, they need to do some filtering of the categories from the flag string they pass. the alternative is to add this feature to either a new api or the existing `rm_setcommandinfo`, so the module can conditionally add this info without filtering of the hard coded flags string. we would like to explore these alternative (implement each in some branch) and see which one feels better. can you handle all of that in the coming couple of weeks?",0,0,0,0.95926034450531,0.9769679307937622,0.9524595141410828,0.0,accept,unanimous_agreement
1422271248,11708,"thanks, that sounds good, yes i can handle these changes in couple of weeks. will keep you posted on the pr. will implement the alternative and put up another pr to see which one makes more sense.",1,1,1,0.955348014831543,0.9815411567687988,0.9820199608802797,1.0,accept,unanimous_agreement
1466983268,11708,"hi team, i have updated this pr and have addressed the comments. i have also updated the pr with the implementation of blocking the registration of command, registering the configs, and creating the data type outside the onload function. please review the changes and let me know if there are any questions.",0,0,1,0.7642789483070374,0.6170287728309631,0.8229885697364807,0.0,accept,majority_agreement
1467292929,11708,"for the conversation about redismodule_setcommandinfo(). i feel like there are three options: 1. we leave the api as is. module developers will need to implement a check for redis version and determine flags as necessary. 2. we extend redismodule_setcommandinfo(). we will need a new version, and module developers will still need to check for redis version for setting the fields. 3. we implement a third api. redismodule_setcommandcategories(), which a module developer will need to check the version to know if they are allowed to call. \1 and 2\ seem equally bad, as we basically are required to have a redis version check for the api. 3\ seems theoretically better, as we could check if that command exists, and only call it if it does. implementation wise they are all pretty similar now, as we just need to go and set all the bits and flags and call ""recompute"" like we already do. in the fullness of time, i still think 1 is the clearest and most sensible approach. i would still be inclined to suggest we merge what is currently implemented. have you thought much more about this.",0,0,0,0.7123733758926392,0.9684396982192992,0.971199870109558,0.0,accept,unanimous_agreement
1467644767,11708,"iirc the concern wasn't about detecting the redis version, but rather about the string manipulation the module would need to do (either concatenate strings, or filter them), which would make the boilerplate code of registering commands rather ugly. so if that's the case, and we take the categories to another string (either setcommandinfo or setcommandcategories), it resolves that problem. i agree that in the fullness of time this concern is gonna be gone, but in the short term (next 4 years), all modules that wanna use it are gonna suffer from this complication. am i missing something?",-1,-1,0,0.5925139784812927,0.5782397985458374,0.753943681716919,-1.0,accept,majority_agreement
1469316264,11708,"core meeting update. we decided that we should probably implement the new api, rm_setcommandcategories(), since that was probably the most module developer friendly thing to do. other than that, the new apis are approved.",0,0,0,0.9677648544311525,0.9766469597816468,0.8991761803627014,0.0,accept,unanimous_agreement
1476818941,11708,"hi team, i have pushed the changes to this pr and implemented the new api, rm_setcommandcategories(). please review the changes and let me know if there are any questions.",0,1,1,0.8309348225593567,0.6805931329727173,0.5300027132034302,1.0,accept,majority_agreement
1477128527,11708,i have also updated the top comment with the updated implementation.,0,0,0,0.9843615293502808,0.985564649105072,0.9950222969055176,0.0,accept,unanimous_agreement
1477138688,11708,"all updates, ptal and validate the new name looks good at the very least.",0,1,0,0.7153812646865845,0.8078145980834961,0.5297369956970215,0.0,accept,majority_agreement
1552921391,12192,"generally lgtm. i think the -1 in the slot number is acceptable, i suppose that for many clients will find it easier than supporting a new reply, and i hope that for some it'll even be implicitly supported in the current code, but let's validate that with client library maintainers. not sure regarding timing (fitting this into 7.2), don't want to rush it and then regret.",0,-1,0,0.8625478148460388,0.6839600801467896,0.7144522666931152,0.0,accept,majority_agreement
1553336010,12192,"i have a strong preference to keep moved. it still seems easier long term for clients to just understand one error. we currently don't force pubsub commands to primaries, but maybe we should, i think that's a different conversation though. not sure what to say about that.",-1,0,-1,0.6487305760383606,0.74592125415802,0.6866418123245239,-1.0,accept,majority_agreement
1553336287,12192,how is this a breaking change?,0,0,0,0.5742731094360352,0.880012571811676,0.9662765264511108,0.0,accept,unanimous_agreement
1554041223,12192,"`readonly` and `readwrite` cannot be executed in standalone mode before this, i'm not sure if it will really have an impact, but to be safe i labeled breaking change.",0,0,0,0.9661961793899536,0.9065448045730592,0.959377646446228,0.0,accept,unanimous_agreement
1554072948,12192,"i vote for moved, which will not bring new understanding costs.",0,0,0,0.9026285409927368,0.961049497127533,0.9563001990318298,0.0,accept,unanimous_agreement
1554788867,12192,usually this is just a major decision. i don't think we would expect clients to be relying on this functionality for anything? i'm going to remove the tag.,0,0,0,0.9482997059822084,0.9441378116607666,0.947140336036682,0.0,accept,unanimous_agreement
1559734069,12192,"we discussed this in a core-team meeting, we feel we'd like to completely design this feature, before implementing and committing to parts of it. two concerns that were raised are: 1. maybe moved response will need to carry more info (like protocol, tls, or others) 2. we think we'd like all multi-instance deployments (even a master+replica set) to look like a cluster, that means from topology discovery concerns, we might wanna present them as such to clients. 3. the above discussion about instances in containers or behind nat, and `master-announce-*`, etc. (including tls), all of that should be designed before we take the first step. further discussions are needed..",0,0,0,0.8597396016120911,0.9851442575454712,0.9641149640083312,0.0,accept,unanimous_agreement
1560421771,12192,"it's ok we can design this feature completely and raise all concerns, this is a good way in scheme design stage. and now this pr can work well when users deploy redis on simple hosts, i think we have reached a consensus on this point. and the implementation now is forward compatible, . about the concerns: 1. extend the ability of move is a good idea, but i think this is out of this pr. move is a general redirection reply that affects not only standalone but also cluster, we can do it in 8.0 2. i don't get this point, clients can use `info replication` and `role` to get the master-replicas topology, and get the replica-subreplicas topology recursively. 3. this is a practical problem, but the current `*-announce-*` implement are not perfect either, sometimes it can be confusing. * for example when we config the announced ip:port, they will override the real ip:port for communication between redis noeds, but users may cannot access the announced ip:port. in this case they need use `cluster-announce-hostname`, but imho the `cluster-announce-hostname` and `cluster-announce-ip` are confusing, users cannot distinguish which one is for server side and which one is for client side. moreover, `cluster-announce-hostname` may need `cluster-announce-hostname-port` and `cluster-announce-hostname-tls-port` to work. ![a link] * another situation is redis nodes and client in different subnets as described in #7460, if users config `cluster-announce-ip:port` then the redis noes cannot reach each other via the announced ip:port ![a link] * there are also many issues with the current announced ip:port mechanism, and we need to solve the problem before applying it here. i don't want this pr to become too complicated, i suggest we put these things aside for now and focus on satisfying the users of simple host first. all in all, my opinion is we can design completely but implement step by step.",1,0,1,0.7159501910209656,0.5298730134963989,0.944372534751892,1.0,accept,majority_agreement
1561542375,12192,"first, it is necessary to clarify the meaning of ""move"" in the context mentioned. does it represent 301 or 302? perhaps more people believe that ""move"" represents 301, but in the case of redis-cluster, it has always been possible to have a -> move -> b, which makes it look more like 302. i think perhaps it should be called ""replacer,"" which would indicate that the new address permanently replaces all the functionalities of the current node, rather than just being a redirection. regarding the seamless switch problem in redis, i propose adding a ""retired"" status to the master node. if the current node is not the master node or there are no slave nodes available, an error will be returned.. when the master node is in the ""retired"" status, it can still accept read and write commands without the client being aware of this change. in this state, the master node handles read commands as usual, but forwards write commands to the slave node(s) (while keeping track of keys that the slave node(s) have not yet synchronized). once the slave node(s) reach an identical state with the master node, the master node responds to any command with ""replacer"". when the client receives the ""replacer"" status, it switches to the slave node, which provides all the functionalities of the original master node.",0,0,0,0.9780451059341432,0.9908520579338074,0.9863258004188538,0.0,accept,unanimous_agreement
1561942045,12192,"i will say, i think the incremental delivery we made this happen in makes me wish we had thought this through more holistically :d. so, is our ideal cluster setup: these configs are set as a block to indicate there is a different way to reach this node for replication and cluster. by default we assume a flat topology for all nodes, so this would also apply to clients, however we definitely assume the topology is flat for the nodes in the cluster. [code block] we also need a second set of configs, which are used when the route from the client to the cluster may not be the same as the route between nodes in the cluster. [code block] i think in the fullness of time we want all of this configuration for cluster mode disabled as well, so i'm not clear why we wouldn't just copy the interface we are currently using. do we want clients to be doing the `replication info`? i think we don't, i think we want them to call something like `cluster shards` to learn about the topology. it's an important call out here that cluster mode enabled doesn't have sub-replicas, so what is the right way to expose that. i see a few approaches: 1. we have all nodes know the topology, through gossip or some other means. 2. we push the clients to call multiple discovery commands or force it to a specific node. i think we want to align on 1 to be our long term solution. i think this is very much an engineering mindset, but it's not very coherent from project mindset. for performance improvements, that incremental approach makes more sense since, but when releasing a coherent feature, we should strive to build something that is consistent over a longer time frame.",1,0,1,0.6991373300552368,0.5872367024421692,0.9884828925132751,1.0,accept,majority_agreement
1562187636,12192,"this makes sense, but there is a problem how to identify whether this connection is from a client or a node, some admin service may need get the announce topology not the announce client topology. i know some virtual network tech can do that, but there is currently no universal standardized technology to achieve this. maybe we need some ways like a new command `client come-from`, but it's a bit ugly, and command can lie. yes, i agree these configs can be used for more than just cluster, maybe in some day we can remove the `cluster` prefix. i didn't understand the words `copy the interface we are currently using`, you mean adding `master-announce-ip:port`? i think it's unnecessary, master-replica is different with cluster-nodes, master doesn't need connect to replica, the `replicaof` must use the nat ip:port to connect master, so the `-moved` must reply with the nat master ip:port. but now like sentinel is using these to get the topology, and in redis perspective sentinel is just a normal client. this feature is forward compatible, if we add some new configs or change internal mechanism, it can still work well for users, that's why i think we can do step by step.",-1,-1,0,0.6292191743850708,0.7329964637756348,0.8329277038574219,-1.0,accept,majority_agreement
1569378813,12192,"ping /core-team , need your feedbacks",0,0,0,0.9640421271324158,0.9267640709877014,0.9934480786323548,0.0,accept,unanimous_agreement
1598101952,12192,"i think it makes sense to keep cluster. while talking with our aws customers, people seem to resonate calling any collection of nodes a cluster. the configs only make sense for more than 1 node. i would really like to just use the current cluster announce configs, and only add a single new config which describes an unsharded cluster (in addition to the other configs i mentioned). i want to merge our two deployment configurations, cluster-enabled and disabled, as much as possible. from an end-user perspective the result is basically the same, but from a management and maintenance perspective, i think it will be good for us long term. i agree with this, but i just think it's short sighted. i don't want end users configuring stuff that stops being relevant quickly.",0,0,0,0.7737685441970825,0.9599236845970154,0.8116852641105652,0.0,accept,unanimous_agreement
1598156100,12192,"interesting, on the contrary, our users at alibaba cloud prefer to clearly distinguish between standalone and cluster because these two usage modes are very different. i have experience with aws elasticache and have consulted with some users. i find it misleading to call a standalone mode with a primary node and multiple secondary nodes a ""cluster"". i do not like this design. i don't want to mix up standalone and cluster. if you want to merge the configuration options for these two different modes, i still believe that removing the ""cluster"" prefix is a good approach because the announced ip port doesn't need to distinguish between deployment modes. yes, that's why i don't want to add too much configuration. the current approach is already sufficient for users who deploy directly on physical hosts and for those who deploy in nat mode using replica-announce-ip. in my opinion, before we have figured out the correct and optimal use of announced ip:port, it's better to maintain a relatively pure state and this is the correct way to avoid short-sighted.",-1,-1,0,0.9674909710884094,0.9639919996261596,0.5505797266960144,-1.0,accept,majority_agreement
1600240618,12192,"it's a little hard for me to follow the discussion. iirc we argued that we should design that topic fully before making any changes, and that one of the concerns is topology discovery commands, but i don't see a proposal around that topic. did we conclude something to change that? regarding the redirect configurations, i'm also lost in the discussion, is the current implementation in the pr the most up to date proposal (which we can approve, reject or discuss it's shortcomings)? i.e. is the debate about whether or not we should rename / add alias to the cluster-* configs, or simply reuse them for non cluster, or are there any other issues around nat deployments that are not solved by the current set of configs in the pr?",-1,-1,-1,0.677206814289093,0.6244271397590637,0.9291611313819884,-1.0,accept,unanimous_agreement
1600498216,12192,"1. about topology discovery, i think the role command and the replication section in the info command are sufficient, sentinel also uses these two commands to obtain the topology. 2. regarding the name of the configuration item, i changed to ""replica-enable-redirect"" because i want to retain enough flexibility. currently, this pr does not redirect pubsub commands, but this may change in the future. if we use the name ""replica-redirect-read-write"", it will be difficult to change it later. 3. in the matter of nat deployments, i think there is no problem. currently, for standalone(non-cluster) mode, if nat deployment is used, the ""replica-announce-ip"" must be configured, and the ip specified in the replicaof command must also be the master's nat ip(equivalent to ""master-announce-ip""), otherwise the replica cannot connect to the master. so we don't need to add the ""master-announce-ip"" config, the replica can already return ""-moved -1 master-nat-ip:port"", everything can work well. 4. my argument with madelyn is reuse or rename the cluster-* configs iiuc. actually, i don't want to bring cluster-* configs into the discussion of this pr, using ""replica-announce-ip"" can work well for standalone(non-cluster) mode.",0,0,0,0.9267102479934692,0.9837512373924256,0.6400774717330933,0.0,accept,unanimous_agreement
1600523364,12192,"ok, i'm not certain about the topology discovery, i think someone argued that client libraries are using cluster commands and that we should think about exposing that, but i admit i don't know anything about that topic. regarding nat, i'm confused, you said there's no problem, but also suggested to add `master-announce-ip`? maybe add it to this pr? regarding the `cluster-*` configs, maybe adding an alias can make it nice? again, if it does, let's add them to the pr, so we'll have something to approve or complain about, maybe in several threads, which will be easier to track.",0,0,0,0.7337285280227661,0.7389194369316101,0.653675377368927,0.0,accept,unanimous_agreement
1600535202,12192,"maybe add it to this pr? no no no, i mean replica can reply with master's nat ip without master-announce-ip, since the ip used in replicaof command is already master's nat ip. we don't need add master-announce-ip. i prefer just keep the current configs, i don't want things to become too complicated, alias may make it more difficult for users to understand",0,0,0,0.8996503353118896,0.9729770421981812,0.9859484434127808,0.0,accept,unanimous_agreement
1603439779,12192,"i think this gets to the heart of my main point, and why i think this entire pr is only focused on the short term. i want all client configurations to execute basically the same set of commands and handle topology the same way. i don't want clients to implement two different pathways (info replication and cluster shards). we are at the point where we can try to unify them, and i don't want us to take the short route. there are two decisions we need to make which are important: 1. how clients see the commands. i think this should be `cluster *`, i feel extremely strongly about this. i think any decision that is not this is short sighted. i think we already have consensus about the moved stuff. 2. the configuration that is used. today, non-clustered nodes aren't gossiping with each other, so the configuration is the inverse of the cluster mode flags. instead of nodes being configured to advertise their configuration, they are configured to advertise their master configuration. we can't alias the configs because they don't do the same thing, releasing these configs is a one way door. i don't feel as strongly about two, my argument is more that i don't think it's really aligned with our end state of trying to merge non-clustered vs clustered.",1,1,1,0.505254328250885,0.8612654209136963,0.5401409864425659,1.0,accept,unanimous_agreement
1603950761,12192,"no offend, but you seem to be promoting aws design and forcing everyone to accept it, which makes me uncomfortable. in this pr, i just want to focus on solving the redirect method in standalone mode, i strongly oppose imposing cluster commands on standalone mode in this pr. if you insist on doing so, you can implement it in so-called cluster v2.",-1,-1,-1,0.8833105564117432,0.961921453475952,0.9663546681404114,-1.0,accept,unanimous_agreement
1604010079,12192,"i think that in cases redis isn't using the cluster subsystem / architecture, we shouldn't use ""cluster"" interface. e.g. in retrospect some of these `cluster-*` configs should have been without the `cluster` prefix, and we can fix that with an alias. same goes with the cluster commands. we should imagine what we want in the end, and if the interface applies to both we can expose a new one and keep backwards compatibility. i agree that it can be confusing in the short term, but maybe better in the long term. i suppose it's similar to the changes when we renamed ""lua"" to ""script"" recently.",0,0,0,0.9343258142471312,0.8907803893089294,0.9290759563446044,0.0,accept,unanimous_agreement
1605075134,12192,"can you elaborate? i'm arguing that in the fullness of time everyone should be running on the clusterbus system, so in that world it would make sense. changing the api later adds a bunch of unnecessary churn in the meantime. i'm okay with changing to another topology api, but that also seems rather pointless. i don't think this has any relevance. the naming change you are proposing is purely semantic. i never called them anything besides scripts, it was mostly organizational and allowed us to generalize some configs. the issue here is that the configs are structurally different, they can't just be solved by aliasing afaik.",-1,0,0,0.7143145799636841,0.6226734519004822,0.7415207028388977,0.0,accept,majority_agreement
1605084788,12192,"i'm sorry i'm making you uncomfortable, that was not my intention. i just don't want to just solve a tactical problem. in the past we've talked about trying to move everything to be more similar to cluster mode enabled, and i thought we were all more in agreement on that. everything we release is something we have to maintain, so i really want to make sure we understand the vision that we're building towards.",-1,-1,-1,0.9871978163719176,0.987644612789154,0.9569743871688844,-1.0,accept,unanimous_agreement
1605917447,12192,"i don't think we're aiming there. we **are** aiming to get rid of sentinel, and allow for unsharded (possibly multi-db) redis-cluster. but i think we still want to support standalone. i.e. a plain set of one master + replicas and someone managing them from outside without all the complexity of cluster. i think that when someone's using redis this way, it should become his responsibility (the deployment scripts) to manage all the configurations correctly, but from the client's perspective it should look the same.",0,0,0,0.9535947442054749,0.962244749069214,0.9465009570121764,0.0,accept,unanimous_agreement
1606153079,12192,"we are aiming to get rid of sentinel, and allow for unsharded (possibly multi-db) redis-cluster. ok, assuming the ""we"" is you and zhao, i think that is the misalignment. i don't really see the benefit of removing sentinel by pushing a bunch of configuration user side (which is what in practice you mean be scripts). i want all configurations to be passed through something like the clusterbus, so that nodes are talking with each other and management is simpler.",0,0,0,0.909980833530426,0.9448105096817015,0.9788148999214172,0.0,accept,unanimous_agreement
1606722093,12192,"by ""we"" i meant that it was something all of us agreed on long ago (not specifically this thread). i.e. in order to retire sentinel, we wanna let cluster handle the sentinel use cases that are currently unsupported (un-sharded, with voting replicas or any other way we can support a cluster of a single shard. and since it's un-sharded, it might as well also allow multi-db). in that case no script will be required, it'll be a proper cluster (cluster bus and all). the main gain from this imho is that we won't need to keep maintaining both architectures (cluster and sentinel) at the same time. anyway, completely detached from the above paragraph, i think we should still support the standalone mode, where there's no cluster bus, and instances don't talk to each other to make decisions, instead everything is controlled from outside by the orchestration scripts users write (without the cluster mode subsystem being active). i'm arguing that from operations (deployment / configuration) point of view these two architectures are completely different, but the client / app interfaces should be the same.",0,0,0,0.95396625995636,0.9853585958480836,0.7524954676628113,0.0,accept,unanimous_agreement
1607016077,12192,"fwiw, i thought it was surprising when i learned that replicas don't automatically take over in case of master failure. the different modes were confusing to me as a newcomer. in a good distributed database, everything should just work. cluster is the only mode where failover is automatic and clients automatically and quickly learn about it without reconnecting. i hope we can make it usable for all, by minimizing the negative aspects such as cluster bus network overhead, memory overhead, etc., which i think we can do with unsharded cluster, voting replicas, the one-dict-per-slot feature (eliminates the linear memory overhead) and whatever else. deprecate multiple dbs in all modes or support it in all modes. user-written orchestration scripts are risky. there's always a risk you don't get things right. i don't think we should recommend it for failover handling and such. single standalone nodes can be useful by themselves though. but whenever two nodes talk to each other, why not enable the cluster bus?",0,0,0,0.394651710987091,0.7097418308258057,0.4611959755420685,0.0,accept,unanimous_agreement
1611158355,12192,"i strongly agree with this. standalone clients rely on the user providing the server address, and they can assume all connections always go to the same endpoint. redis cluster clients must deal with multiple, changing endpoints - which naturally makes everything more complex. the main concern i have with this pr is that we're inadvertently pushing this complexity into standalone redis clients. why should we do it when we already have redis cluster clients that have done that? we should leave standalone clients as they are and drive more adoption of redis cluster clients by treating the redis cluster spec as the universal protocol for clients to talk to redis, particularly any deployment with multiple instances.",0,0,0,0.641155481338501,0.9640257358551024,0.7840186953544617,0.0,accept,unanimous_agreement
1611184468,12192,"finally waiting for your reply. but i don't agree with you. standalone mode has been widely used, and i believe it will continue to be used for a long time. i'm not sure how you came to this conclusion, but the standalone mode also uses different endpoints for the master and replicas. moreover, as far as i know, many users also achieve read-write separation in standalone mode by executing read commands on replicas. i don't understand, we didn't introduce any complexity to standalone, if users don't want to receive -moved they can disable the config `replica-enable-redirect`. if they enable it, it means that they want to obtain the ability of automatic switch-over. we can promote the cluster mode, but it's not a reason to give up on optimizing standalone mode. from my point of view, suppressing b in order to promote a is a very wrong approach.",0,-1,0,0.8331925272941589,0.5599439740180969,0.7167752981185913,0.0,accept,majority_agreement
1611278968,12192,"the following are my opinions and responses about the matters we discussed during our core-team meeting yesterday: 1. firstly, i believe we should continue to support standalone mode for a long time, and there may be four deployment modes in the future: standalone, sharded-cluster, unsharded-cluster, and sentinel. 2. it's very necessary to enable standalone mode to have the ability of redirection, clients can automatically failover according to `-moved` without causing errors in users business. and this will not break anything, including sharded-cluster, unsharded-cluster, and sentinel. i don't understand why this optimization is not being made. 3. as for `-moved -1 ip:port`, the `-1` indicates that redis server is currently no split deployment according to slots. therefore, this interpretation is reasonable for standalone, sharded-cluster, and unsharded-cluster, without any ambiguity. 4. regarding clients, as far as i know, clients also have their own working modes. they have explicit standalone mode (which is usually not explicitly written in the code) and cluster mode. their work processes are different, and they have different paths when processing the `-moved`. this will not cause any ambiguity. for examples: * [a link] * [a link] 5. speaking of how clients handle `-moved` in more detail, you may argue that when cluster mode receives `-moved`, it needs to execute `cluster slots` or other commands to update the local routing table. however, there are two meanings behind `-moved` in cluster mode: * one is ""slot migration occurred"" * another one is ""the master-replica role has changed"". actually, it is unnecessary to re-pull the routing table when the master-replica role changes. the client just needs to replace the current ip with the new ip in -moved. only when the slot migration occurs, it is necessary to re-pull the table to avoid continuous redirection of subsequent commands. here i think you can get the point that if the client works in standalone mode, there is no need to pull the table because there will be no slot migration events. 6. as for sentinel mode, this is even easier to solve. after receiving `-moved`, if clients want to be more real-time, just automatically redirect to the new address. if clients want to keep using the previous way, then just send `sentinel get-master-addr-by-name` to sentinel again to get the new master address. `-moved` enhances the real-time performance of failover processing and does not break the current working mode of sentinel clients.",0,0,0,0.6441107988357544,0.9764834642410278,0.777732789516449,0.0,accept,unanimous_agreement
1611769052,12192,"i agree with that, and i also think `unsharded-cluster` and `sentinel` should eventually merge, leaving us with a single server-side implementation, even if we continue to expose the sentinel api for some time for backward compatibility. i'm not sure we mean the same thing by `standalone`. to me, this means a single `redis-server` instance. it may be replicated for various reasons, but we don't consider this deployment a distributed system. it's not meant to provide h/a, for example, and we expect users who want that to use `unsharded-cluster` instead and not try to roll their own home-grown scripting solution (as pointed out). because from my point of view, this brings to standalone-mode clients complexities that we already solve in cluster-mode clients. if we solve that problem for cluster-mode and support unsharded-cluster, why not pick a cluster-mode client if we need redirects, etc.? we need to figure out two things here: 1) does this introduce significant complexity to clients, as i fear? maybe we need to get some perspective about this from some of the client maintainers. 2) in a future world where we have an `unsharded-cluster` option, based on cluster v2 or even more advanced work that removes existing cluster or cluster bus complexities, what is the value of using `standalone+redirects` and not `unsharded cluster`? for clients that already handle `-moved`, i agree this is simple and clear. but as stated above, my concern is that now we're requiring *all* clients to handle `-moved` which has a lot of potentially sharp edges: * does the redirect apply to *all* connections or just the one we received the reply for? * if we have a connection pool, do we need to drop all connections and reconnect? * what happens if the `-moved` target fails to resolve / connect / authenticate? do we have to remember the old address and go back to it again, hoping we'll get better luck or at least `-moved` to a better endpoint? otherwise, it's a dead end. i know that different clients handle this differently, and i think we should have done a better job creating a redis cluster client spec that describes exactly how clients need to handle the different cases. but one common thing to cluster-mode clients is they maintain some view of the server-side topology - what nodes exist, etc. they may update it incrementally based on `-moved` or use `-moved` as a trigger to refresh everything, but this is inherent to having cluster support. standalone clients don't have any of this as a common baseline. this also means that even if we define `-moved` as a valid reply in standalone mode, we may discover that adoption is limited. i agree, and that's exactly the point i've tried to convey. sentinel clients, like cluster clients, and unlike standalone clients, are already aware of topology and are part of a distributed system.",0,0,0,0.929612934589386,0.9807097911834716,0.9603012800216676,0.0,accept,unanimous_agreement
1612048951,12192,"you have a point that `unsharded-cluster` also may need a moved-redirect without slot. from a client pov, perhaps `standalone-with-redirect` and `unsharded-cluster` are identical? if that is so, i have no objection to a `standalone-with-redirect` mode, i.e. if there is no extra complexity for clients to care about. i hope it will be possible to scale an unsharded cluster to a sharded cluster in runtime, or at least switch between these modes using manual failovers, so a good client should handle both. that is a very good point! :bulb: i haven't thought about that before nor seen it implemented. a client can easy tell the difference: if the moved-targed was a replica before, it means there was a failover. let's explain this in the cluster spec and i'll consider implementing this logic in the clients i'm involved in (hiredis-cluster in c and some client in erlang). these are also exactly the two events cluster clients may want to subscribe to, so they can eagerly update their slot mapping before they get a single moved. (i implemented one of them in #10358.) btw, i think it's good that clients are allow some amount of freedom in how they handle these things.",0,0,0,0.9112229347229004,0.7638034820556641,0.962864100933075,0.0,accept,unanimous_agreement
1612077205,12192,"these types of optimizations are risky to document and suggest clients make, since it limits our ability to make changes server side. if in the future we support nodes which might act as a primary to some slots and a replica to others, think an all primary cluster where if a node fails one of the other primaries takes the slots, we run into a situation where your assertion no longer holds. there are also extreme cases where a replica migration and slot migration might occur, so the clients stale data might think something was a failover that was really a migration. this can't happen in the standalone case.",0,0,0,0.6974173188209534,0.9178901314735411,0.7696653008460999,0.0,accept,unanimous_agreement
1612395943,12192,"edit ----- unsharded-cluster will try its best to be compatible with standalone, so from the perspective of the client, it will be accessed in standalone mode. old response: --------- as a jedis reviewer, i evaluated that if the replica redirect function is implemented in jedis standalone mode, the work will not be too much (1 day). as a contributor of redis, my opinion on this function is: redis has been developed for 14 years, and the standalone mode is simple and out-of-the-box. and most clients have (or default) to support the standalone mode, in the foreseeable future, it will not be replaced. even if we support `unsharded-cluster`, the access protocol is also in `cluster` mode, that is, the open source client must support `unsharded-cluster` on the cluster code, not in standalone mode, so i think optimizing the standalone experience (whether from redis or client), are meaningful and will not be deprecated in the future.",0,0,0,0.5899578928947449,0.9174610376358032,0.9431151151657104,0.0,accept,unanimous_agreement
1612458256,12192,"yes, you are right! that is a beautiful wish, and i also hope we can achieve it. however, unfortunately, it is not possible, or rather, it is unlikely for a long time. even if we have implemented unsharded-cluster and also use the same client (cluster mode?) to access both unsharded-cluster and sharded-cluster, there are still significant differences for users' businesses. when scale from unsharded-cluster to sharded-cluster, many problems will arise, such as the inability to execute select commands, inability to execute sort by, inability to execute cross-slot data commands, and so on.",1,1,1,0.7979254126548767,0.9917237162590028,0.9802682399749756,1.0,accept,unanimous_agreement
1612464057,12192,"wouldn't cluster mode have these issues as well? these problems are not unique to standalone mode, and cluster mode will also encounter them. moreover, these problems have already been solved in cluster mode and can be easily applied to standalone mode. as said, this is not a difficult task.",0,0,0,0.95473313331604,0.989423930644989,0.9799845814704896,0.0,accept,unanimous_agreement
1614010722,12192,"it looks like we all agree that standalone should be long-term supported, i think we can merge it, /core-team any other questions?",0,0,0,0.9522485733032228,0.9789233803749084,0.9085161685943604,0.0,accept,unanimous_agreement
1617454544,12192,"we agree about standalone mode being supported in the future, but i think merging this pr (as is or with modifications) requires consensus on other topics where we still don't have it. i think we should at least be aligned about the answers to these questions: 1. should the standalone mode support high availability, or can we assume we have a non-sharded cluster for that? 2. if high availability in standalone mode is supported, what does that look like on the protocol side? arbitrary extensions, a subset of redis cluster spec, or a full redis cluster spec? 3. do we expect all standalone clients to support high availability (unlike in the current state where clients need to support sentinel or cluster mode explicitly)? if we do, and we adopt a full or partial cluster spec for standalone high availability, doesn't this mean we effectively want to move all clients to cluster mode?",0,0,0,0.9466689825057985,0.988143026828766,0.957894504070282,0.0,accept,unanimous_agreement
1617692368,12192,"i couldn't fully understand what you were saying, i'm trying to provide some answers based on my understanding: firstly, of course, standalone should support high availability, which is unrelated to the redis mode (standalone or cluster) and even redis. all databases should support high availability. regarding redis, it's not true that returning `-moved` in cluster mode means high availability. the `-moved` in cluster mode is essentially to indicate the change of the routing table, which includes failover. cluster supported self-failover from the beginning, which i think is a wise choice. however, cluster also allows disabling automatic failover by setting `cluster-replica-no-failover`. in this case, is it not high availability if manual failover is performed through an external system? as far as i know, many users build their own high availability systems, even without using sentinel and cluster's auto failover. these self-built high availability systems ensure their running redis, whether standalone or cluster. i guess aws is doing the same. in addition, `-moved` is only a supplement to the high availability of the server-side master-replica role switch, or it is an optimization of redis high availability switch. the `-moved` reply in cluster can help clients better handle server-side switch. standalone switch is still happening without `-moved`, and having `-moved` doesn't mean it becomes cluster. i still need to emphasize one point: `-moved` has two meanings, ""slot migration occurred"" and ""the master-replica role has changed"". if stick to its literal meaning, `-moved` should only be used to describe slot migration, and master-replica role switch should be described by other words, such as `-redirect`, even the switch in the cluster mode should use `-redirect`. but i don't recommend doing so, as it would break existing systems and has no meaning.",0,0,0,0.8949598073959351,0.972730040550232,0.9113782644271852,0.0,accept,unanimous_agreement
1627674167,12192,"sorry about that. please refer to specific unclear questions so i can try to explain myself more clearly. we can question that. why? today, standalone redis does not support high availability. you need to use either sentinel, which is compatible with standalone redis, or redis cluster, which is incompatible. but if we had a non-sharded cluster, why would anyone who wants high availability choose standalone? it's still highly available because the redis cluster specification clearly defines how clients are expected to learn about and deal with failovers. redis standalone doesn't have this capability today. yes, we can add it - but that's my point: why should we do it if we already have it in redis cluster, and we're in consensus about creating a non-sharded cluster in the future? but if we had a much better redis cluster supporting non-sharded mode, why would those users still build their high-availability systems? if we agree that there are valid use cases for redirecting in standalone mode, using a different, dedicated reply makes sense.",-1,-1,-1,0.9868934154510498,0.9852350950241088,0.9796889424324036,-1.0,accept,unanimous_agreement
1628086269,12192,"tbh, i'm afraid i don't totally agree with your definition of high availability, but i can follow your opinion and discuss it further. from these words, in my understanding you are suggesting that high availability means that the server can provide switch information to allow the client to smoothly complete the switch, right? and then in this case, we can assume that high availability is unrelated to whether the server can detect and perform failover on its own, i.e. it is unrelated to whether cluster is configured with `cluster-replica-no-failover`, whether standalone uses sentinel, and whether failover coordinator is used in flotilla (imho, fc in flotilla is like sentinel in standalone. if unsharded-cluster is part of flotilla, how does it perform failover?). so the key point in high availability is whether redis server can return switch information to the client, e.g. `-moved`. we can continue the discussion based on this pov. my opinion is that every form of redis needs to support high availability, including standalone, cluster, and unsharded-cluster. users have the right to choose, and iirc we have already reached a consensus that standalone needs to be supported for a long time, even if unsharded-cluster appears in the future. i still don't understand why we can't make standalone support high availability. are we planning to abandon users who use standalone mode? i also need your opinion on this issue. however, please note that unsharded-cluster is only a concept, it has not started development and there are still many details that have not been touched upon, and it should be noted that there is a big difference between concepts and implementation. and it is uncertain when it can be implemented, considering this, i also hope that we can support high availability for standalone users as soon as possible.",-1,0,-1,0.5800647735595703,0.8847461342811584,0.8867369890213013,-1.0,accept,majority_agreement
1628235397,12192,"we are certainly not gonna abandon standalone. the key point in my opinion remains that although there are several ways to manage instances, the clients should have just one interface, and that's what we need to fully design before we made any changes.",0,0,0,0.8976605534553528,0.969685435295105,0.9596993327140808,0.0,accept,unanimous_agreement
1628265773,12192,"can you elaborate on it? we agree standalone is long term supported, then high availability is necessary. i don't see any conflict with ""the clients should have just one interface"".",0,0,0,0.9781411290168762,0.9755620360374452,0.9931661486625672,0.0,accept,unanimous_agreement
1628592567,12192,"a highly available redis deployment provides all the moving parts required to handle failover end-to-end. the ability to provide clients with the information necessary to handle failover is a key element in a highly available deployment - i agree with that. but server-side support is only one part. the other part is that clients need to properly support that. currently, some clients do (those that support sentinel or cluster), and some clients don't (the rest). in a world with unsharded-cluster and standalone server modes, why would we need to work out a way to deploy highly available standalone-mode redis? yes, someone can build their own system based on standalone redis and handle failovers. but why should they do that? and why should we ask clients that only support cluster or sentinel to support yet another kind of deployment? after all, if someone builds their own h/a system based on standalone redis and custom components, they could also use a load balancer or dns or other network tricks to handle those redirects. yes, they can choose: 1. run standalone redis which, like today, doesn't support high availability *out of the box*. 2. run cluster mode redis, which supports high availability and shading - but is not compatible with standalone redis. 3. run a future non-sharded cluster redis, which supports high availability but no sharding and is 100% compatible with standalone redis. you're proposing option ""4"". but why? and how exactly is it supposed to work? the way i understand it, it's a home-grown system and, as such, it can either be built not to require any client support (vips, dns, etc.) or communicate to clients using the cluster protocol. i agree! and based on this discussion, i think it's important to focus our efforts there so it's no longer just a concept.",0,0,0,0.9393077492713928,0.968481183052063,0.9345313906669616,0.0,accept,unanimous_agreement
1628675533,12192,"i disagree with this answer. if the cluster can't do failovers, it's not highly available. if all replicas are configured with `cluster-replica-no-failover`, or if there are no replicas at all, the cluster is not highly available by itself. (edit: i see now that the question does include an external system that performs manual failover, so the answer does make sense.) i think we need to make a difference between ""support ha"" and ""be ha"". standalone is not ha by itself, but it can be ha with help of sentinel or other sentinel-like tools. in the same way, maybe a cluster with `cluster-replica-no-failover` can be ha with help of some kind of daemon, k8s operator or such. but it's not ha by itself. why do we want to deprecate sentinel? i believe the reason is that it's not very elegant to require an external tool. a system that can be ha by itself is more elegant and user-friently. we can't forbid external sentinel-like tools but we don't need to actively support them.",-1,0,0,0.814462423324585,0.7981305122375488,0.8130649328231812,0.0,accept,majority_agreement
1628715636,12192,"we wanna deprecate sentinel so that we don't have to keep maintaining that code (side by side with cluster). ideally, the unsharded-cluster can fill that spot, maybe together with some adapter layer to provide sentinel interface to clients.",0,0,0,0.9834945797920228,0.991526424884796,0.9886178374290466,0.0,accept,unanimous_agreement
1628819815,12192,"what users can choose should be: 1. run standalone redis which, optimized, **support** high availability. 2. run cluster mode redis, which supports high availability and shading - but is not compatible with standalone redis. 3. run a future non-sharded cluster redis, which supports high availability but no sharding and is 100% compatible with standalone redis. and about point 3, i don't think unsharded cluster can be 100% compatible with standalone, like if a server in standalone mode does not support redirection, then the client in standalone mode will not be able to complete failover. as a result, accessing a server in unsharded cluster mode with a client in standalone mode cannot guarantee high availability. vip and dns cannot support high availability in real-time. since dns cannot guarantee real-time updates, and there may be existing connections pointing to the backend after the vip is updated, these will result in those old connections accessing the replica (master before failover). this is the problem that this pr aims to solve, allowing those old connections to be automatically switched to the new master (replica before failover), i.e. automatically routed to the new master's dns, establishing new connections on the vip will also access the new master.",0,0,0,0.951637327671051,0.9858863353729248,0.9823927879333496,0.0,accept,unanimous_agreement
1630579289,12192,"why do you think there's no way to get unsharded cluster to be 100% compatible with standalone redis? i think this is the key here. if we do manage to achieve that, users will be able to connect to it using standalone clients for maximum compatibility but no failover support, or using cluster clients if they also wish to have failover support. i was only providing this as an example of how someone who builds their own high availability can address that, without client support.",0,0,0,0.9551435112953186,0.9756599068641664,0.987413763999939,0.0,accept,unanimous_agreement
1631813459,12192,"do you mean that we can use the client's cluster mode or standalone mode to access unsharded-cluster? this may have the following details: 1. clients in cluster mode usually need to call `cluster slots` or `cluster nodes` to obtain and resolve the routing table. but i think unsharded-cluster does not need it, because it holds 16384 slots and does not need routing tables to inform. 2. clients in cluster mode usually restrict the execution of cross-slot commands (most clients check the key by themselves and report an error in advance, e.g. [a link], but standalone does not, and neither does unsharded-cluster. 3. clients in cluster mode usually restrict `select` and `swapdb` commands, but neither are standalone and unsharded-cluster. so i think it is the right way to use the client's standalone mode to access unsharded-cluster (the only thing we need to do is to support the `moved` protocol for standalone, so that high availability can be achieved)",0,0,0,0.9794554710388184,0.99258691072464,0.9930902123451232,0.0,accept,unanimous_agreement
1632265277,12192,"there is an alternative to redirects that doesn't need any modification to clients: the server can close the connection after failover. if the client reconnects, it can be nat-routed to the new master. if failover is triggered manually, cloing clients can also be done manually using client kill. too brutal? regarding unsharded cluster, i opened a ticket to discuss it: #12408",0,-1,-1,0.5671194195747375,0.9633722305297852,0.9463051557540894,-1.0,accept,majority_agreement
1635182255,12192,"why? in my opinion, this is an irresponsible approach for users. clients using standalone mode also need to support high availability, which is a very necessary feature.",-1,-1,-1,0.8800919651985168,0.9223621487617492,0.9462301135063172,-1.0,accept,unanimous_agreement
2016980659,12192,"[a link] thank you for your submission! we really appreciate it. like many open source projects, we ask that you sign our [a link] before we can accept your contribution. you have signed the cla already but the status is still pending? let us [a link] it.",1,1,1,0.9900075197219848,0.9931225180625916,0.9939326643943788,1.0,accept,unanimous_agreement
762373301,8327,"/core-team hey, this should be ready for review. i looked through the command code, but would appreciate another pair of eyes at least since that is not the code i'm more comfortable with.",0,1,0,0.5107496976852417,0.964984118938446,0.8323334455490112,0.0,accept,majority_agreement
762720282,8327,"imho `multi/exec` and lua script can do it better, but if users really wanna it, i prefer some clear and specific commands like getex/pgetex/getdel as mentioned in the original issue, it's not easy to understand and use getex with so much flags, especially these flags have many conflict.",0,0,0,0.9287254810333252,0.9159036874771118,0.9334936141967772,0.0,accept,unanimous_agreement
762801080,8327,"indeed it can be achieved by multi exec, but this will make it more convenient for users, and afaik it was requested a lot. the problem with splitting it to many commands is that whenever we wanna add a new option, we need to add it to all of them, or add another variant of the command. redis did abandon setex, setnx, setxx a long time ago in favor of set with various arguments. and we recently abandoned getset (to avoid adding an ex argument to it) in favor of set with get argument, and did the same to many many other commands (z*range*, georadius*, *pup*push). so as long as the getex command is always a write command and always refers to one key, and always returns the value, i think we should stick with one command rather than several.",0,0,0,0.9559583067893982,0.9713191986083984,0.97846257686615,0.0,accept,unanimous_agreement
763561854,8327,"please avoid pushing a rebase and an actual change (editing old commits) in the same force-push, it's very hard for me to find your changes and review them.",-1,0,0,0.9008012413978576,0.5885481238365173,0.7477999329566956,0.0,accept,majority_agreement
763801688,8327,"thank you. can you please update the pr top comment, so that we can use it as a commit comment. keepttl needs to be removed, but maybe a few other updates too. also maybe note some non-obvious things about the code (like why `setexcommand` no longer needs to be handled in feedappendonlyfile). if you can, please also make a pr [a link]",1,1,1,0.9258864521980286,0.5857359766960144,0.9353309273719788,1.0,accept,unanimous_agreement
763802694,8327,"/core-team please approve a new getex command, and new arguments to the set command (see top comment)",0,0,0,0.9838451147079468,0.9835071563720704,0.9924851059913636,0.0,accept,unanimous_agreement
763813642,8327,thank you so much for the review. i have updated the pr top comment with the new details. i will follow up with doc change soon and make a pr once it is ready.,1,1,1,0.9794713854789734,0.976709485054016,0.9870247840881348,1.0,accept,unanimous_agreement
764456206,8327,"just a little suggestion, could we rename the `getex` command for example `getx`? like `xml` means e**x**tensible **m**arkup **l**anguage. i know `getex` means **get extended**, but users may be confused, it looks so much like `setex`.",0,0,0,0.9618736505508424,0.9831400513648988,0.97404807806015,0.0,accept,unanimous_agreement
764481198,8327,"i agree getex is not very good, but i'm also not sure getx is much better. maybe someone has additional suggestions? ?",0,0,0,0.7384478449821472,0.849759042263031,0.7024327516555786,0.0,accept,unanimous_agreement
764852956,8327,"i think i like getx slightly more than getex. the other one i thought about was getopt, for get options, but thought it was slightly clunky.",1,0,0,0.7514463067054749,0.6293731927871704,0.5615512728691101,0.0,accept,majority_agreement
764943677,8327,"i do not have a strong opinion, but may be `getx` can help clear some confusion due to `setex`. i can revise the pr once there is an agreement.",0,0,0,0.8375697731971741,0.96717369556427,0.9750449657440186,0.0,accept,unanimous_agreement
766349921,8327,"i was in favor of getx but i wanna change my mind and go 180° backwards. * getx may be a good name for an ""extended"" command, but it's not consistent with setex, so we may one day wanna add setx too (it looks odd to get getx and setex) * on the other hand, ex may stand for ""expiry"" in set and it indeed looks odd to get a del argument there. then thinking of the info commandstats, i think that del is inherently a different action than changing expiry, and i would rather see getdel distinctively there rather than be mixed with getex. what these two command have in common is that they're a write command that doesn't take a new value, but that doesn't mean they have to be one command. we do have a get argument for set, but that doesn't really changes what the command does to db. but a del argument might be wrong. p.s. using an old exat argument it is also possible to use getex as a getdel, but that's a side effect i would like to ignore. bottom line, i now vote for splitting this command into two: getex and getdel. /core-team please share your thoughts or approve before we ask for changes to be implemented in the pr.",-1,0,0,0.4994348585605621,0.9129838347434998,0.6622967720031738,0.0,accept,majority_agreement
766600370,8327,"i like your proposal better. i suppose we've been on a trend to have fewer commands, but it's probably a good pattern to split commands by what they are intended to do. two minor comments if we go down this path: 1. getex should behave more similarly to setex, the syntax should probably be [code block] so that in the future we can extend setex with the same functionality. 2. instead of getdel, i would propose just ""pop"", since that is fairly consistent for ""remove and return"" across redis. this sounds good in my head, but it may be a bad idea.",1,1,1,0.8995684385299683,0.9694005846977234,0.9603881239891052,1.0,accept,unanimous_agreement
766605386,8327,"i don't agree with these proposals. 1. we never wanna extend setex with more functionality, it's a deprecated command which is replaced by adding these functionalities to the set command. making getex look like setex would be a mistake, it should look like set. this is quite clear that putting the ` ` as a mandatory positional argument would conflict with the persist option. 2. i think pop is confusing (hints this is a list or a stack), i think getdel reflects what it does better and is more consistent with the rest of the string commands.",-1,0,-1,0.5620771646499634,0.6468913555145264,0.771857738494873,-1.0,accept,majority_agreement
766608476,8327,"1. than i would keep it as getx, to not confuse anyone with setex. 2. eh, i don't like getdel, but i don't feel strongly about it. edit: the other proposal was take.",-1,0,0,0.707909345626831,0.5204910039901733,0.6992632746696472,0.0,accept,majority_agreement
766612542,8327,"i think the getex refers to ""get with expiry"", it's similar to ""get ex"" and imho doesn't need to be syntax compatible with setex. what i don't like about getx (referring to extended) is that we add a new convention to redis commands. tomorrow we'll have brpopx, and others. but in fact, after removing the del argument, this is a get with expiry. i rather avoid the getx, and i think that once we removed the del argument, getex is a good name (just don't try to match it to the bad syntax of setex). i still don't like pop and take, these are fancy names, but getdel it clearer and easier to understand it refers to a string object.",1,0,0,0.6273400187492371,0.5234496593475342,0.633002519607544,0.0,accept,majority_agreement
766622715,8327,"why do we want to limit only to expire? that is what get's us into these constrained situations in the first place. right now we are just adding flags optimistically, i honestly think the persist flag is probably worthless, and ideally we would probably want to more deeply re-think how redis commands are structured so we don't have all these weird name collisions. i guess i really don't feel that strongly: i have a weak preference for getx + getdel then.",-1,-1,-1,0.9582342505455016,0.634894847869873,0.7948201298713684,-1.0,accept,unanimous_agreement
766676621,8327,i'm aligned with 's opinion on this.,0,0,0,0.9604505896568298,0.8907517194747925,0.9782028794288636,0.0,accept,unanimous_agreement
766776489,8327,i also agree with .,0,0,0,0.9677851796150208,0.9359686970710754,0.966431736946106,0.0,accept,unanimous_agreement
766791220,8327,"since there's a lot of text here, please be a bit more explicit. i assume you mean this post: [a link] i.e: [code block] maybe it would help if you say why you don't like the alternatives (mixing get as an argument for the first command, or naming them take, pop, getx)",0,0,0,0.9301277995109558,0.9730669856071472,0.9815452694892884,0.0,accept,unanimous_agreement
767161397,8327,"okay the long version then :) * `getx` seems confusing, creates yet another command notation which i don't think we want at this point. * `pop` implies an operation on a collection, i think it's confusing. * `take` is a bit awkward for me as it doesn't seem related in any way to `get`. * `getex` is not perfect, it's a bit ambiguous as it is more easy to interpret as ""expire"" rather than ""extended"", but i can't come up with something better. at some point i did toy with `getmut` (mutable get) but: * it would make more sense if we also give up `getdel` and use a `del` argument, which is not good introspection-wise * is also awkward * steers this discussion further from a conclusion",1,1,1,0.8523476719856262,0.877426028251648,0.9900930523872375,1.0,accept,unanimous_agreement
767309401,8327,"ok, next. who wants to make a redis-doc pr?",0,0,0,0.9805418252944946,0.981300175189972,0.991305410861969,0.0,accept,unanimous_agreement
767315064,8327,i will follow up with a doc soon.,0,0,0,0.9715097546577454,0.971588969230652,0.9800323247909546,0.0,accept,unanimous_agreement
767737351,8327,document pr - [a link],0,0,0,0.9855526089668274,0.9880472421646118,0.9954376816749572,0.0,accept,unanimous_agreement
1341342115,11595,this trick can perhaps be used for the main keyspace db later if we move the key to robj as you suggested in #10802. (open addressing makes scan complicated. this is simpler.),0,0,0,0.9789703488349916,0.9943879246711732,0.9941890239715576,0.0,accept,unanimous_agreement
1341348246,11595,"reviewers: the first 6 commits are from ""make dictentry opaque"" and have already been reviewed. it's enough to look at ""store keys without dictentry in dict for sets"" (and any commits added in the future).",0,0,0,0.9877739548683168,0.9918233752250672,0.9932667016983032,0.0,accept,unanimous_agreement
1341499526,11595,"this looks like a good idea for improving memory footprint of sets. at the same time, as part of [a link] we've been thinking about making memory improvements for the main dictionary that could also benefit sets. our main idea was to embed entry metadata and keys into the byte array inside of the dict entry (similar to rax entry). this would allow optional values for sets and embedded keys without pointers, achieving similar (perhaps exactly same) memory improvements as you did here. one way these two changes could co-exist, is if we focus only on the main dictionary in [a link] making a copy of the dict type, and keeping legacy layout (with your improvement) for sets, while iterating on top of the entry with embedded content for the main dictionary. this might be a reasonable approach, given that embedding all parts of the dict entry has more risks and would take more time to get done. if embedded entry design proves successful, we could migrate sets to it later too.",0,0,1,0.5965940356254578,0.928699553012848,0.7160543203353882,0.0,accept,majority_agreement
1341922562,11595,i don't think the two ideas are necessarily exclusive. this approach still looks has to pay 24 bytes when we need a collision resolution. having a way to reduce that to 16 bytes by omitting the key pointer would still save some extra bytes.,0,0,0,0.9748616814613342,0.9376648664474488,0.9461228251457214,0.0,accept,unanimous_agreement
1345483735,11595,"i'm not sure i'm aware of the related discussions, for forgive me if i'm causing a mess. i think that previously i thought this optimization will use a special dictentry with no value (key and next), and save 8 byte. it's nice that we save the next entry too, but: 1. maybe there's no reason for a full 24 byte dictentry in case of collision? maybe we'll store the key directly in the table when there's no collision, and a pointer to a struct with key and next when there is a collition. 2. maybe we can avoid the next pointer on the main dict and hash/zset too, by using that msb and a struct of 16 bytes? i.e. this pr does two unrelated optimizations in a certain case, and leaves cases that can't use both, completely optimized. considering the refactoring of #11465 it seems we can easily achieve either separately too.",-1,-1,-1,0.6693941950798035,0.567313551902771,0.8314105868339539,-1.0,accept,unanimous_agreement
1346198980,11595,"of course we can do your suggested optimizations too. they are almost orthogonal to this pr. the reason i didn't implement ""omit next pointer"" yet is that dictentry metadata complicates things and there are plans to get rid of the metadata (#10589). the possible dictentry optimizations i've considered so far: 1. omit the entire dictentry when possible (this pr) 2. omit next pointer in dictentry 3. omit value in dictentry 4. put multiple key-value pairs in the same dictentry allocation (aka bulk chaining) do you want me to put multiple optimizations in this pr or to make separate prs?",0,0,0,0.9442020058631896,0.9893065094947816,0.9895308017730712,0.0,accept,unanimous_agreement
1346292411,11595,"you're right, let's not mix things, but let's see what's logical to combine (added numbers to your post) 1. is only relevant to sets, not for the global dict or any other type 2. a generic dict optimization, whenever there are no collisions 3. only relevant for sets again same as 1 4. a generic dict optimization, same as 2 (for when we **do** have collisions) so 1 and 3 are both relevant when we don't have a value, and also when we don't depend on the metadata feature. they're both about sets, and i could argue that 1 is an evolution of 3. so maybe handle these two here. and then a separate pr for each of the other two?",0,0,0,0.9544198513031006,0.95320063829422,0.9788967967033386,0.0,accept,unanimous_agreement
1346500761,11595,"ok, i can try adding a dictentry without value (with key and next) in this pr. i guess we should come up with a scheme for pointers, also for the other things we want in the future. how about this? lsb dictentry* actually points to --- ----------------------------- xx1 key 000 normal dictentry {key, value, next, (metadata)} 010 dictentry_no_value {key, next} 100 reserved for dictentry_no_next {key, value, (metadata)} 110 reserved for future use, maybe bulk entry, maybe something else",0,0,0,0.9660475254058838,0.9851816296577454,0.9718518257141112,0.0,accept,unanimous_agreement
1346687943,11595,"note that in 32bit you only have 2 lsbs. maybe we'll have to have some compromises, or let each instance of the dict designate the bits for different purposes. i.e. some dicts will need one optimization and know which bucket uses it, and for others that optimization is irrelevant and can use that bit can be used to distinguish the bucket type for another optimization.",0,0,0,0.985023021697998,0.9838007092475892,0.9889304041862488,0.0,accept,unanimous_agreement
1346822668,11595,"i believe we have 3 lsbs on 23bit too, as long as the allocation is at least 8 bytes. malloc returns memory ""suitably aligned for any built-in type"" according to `man malloc`. something similar is written in posix and the c standards too. built-in types include double and int64_t iiuc. when jemalloc is used, lg-quantum = 3 also for 32-bit. it's not even possible to set lg-quantum lower than 3.",0,0,0,0.9801769256591796,0.9915354251861572,0.9907978773117064,0.0,accept,unanimous_agreement
1346839378,11595,"i guess you're right. there is one case where we don't have 8 bytes alignment: if we don't have_malloc_size, we `return (char*)ptr+prefix_size` in ztrymalloc_usable, where `prefix_size` is `sizeof(size_t)` which is 4 on 32-bit. so zmalloc is not fully malloc compatible... can we change prefix_size to a minimum of 8?",0,0,0,0.9758036136627196,0.974587380886078,0.9794116616249084,0.0,accept,unanimous_agreement
1347039943,11595,"you're right (twice), but also sds pointers can easily get non-8 byte aligned (due to the sds header, that can even by one byte), i.e. considering that we use that bit to distinguish between dictentry and sds, we need both to avoid using that bit. am i missing anything? in addition to that, if that dict thing is a generic feature, it also collides with the ability to store other (non pointer) values (like long), but i guess we can put that aside and enable that optimizations only on dicts that only save pointers.",0,0,0,0.9703595638275146,0.9714494943618774,0.9824572205543518,0.0,accept,unanimous_agreement
1347320439,11595,"yes, so it seems. :hand_over_mouth: the sds header is either 1, 3, 5 or 9 bytes so all sds pointers are odd. this optimization actually relies on the least significant bit being 1 (i.e. an odd value) and doesn't encode it further. this is what distinguishes it when it is used instead of a dictentry. if a 'no_value' dict is fed a key that is not an odd pointer, it falls back to using a dictentry, so it's fine to use `long` where some are odd and some are even and get the optimization half of the time. but you could say this trick is biased towards odd numbers and sds keys. if we want a dict even numbers or normally aligned pointers (e.g. robj) as keys, the caller can flip the least significant bit to enable this optimization manually. or we can control it using a bit in the dicttype. that's not something we need at the moment though.",0,0,1,0.9607438445091248,0.8308597207069397,0.6252094507217407,0.0,accept,majority_agreement
1348140524,11595,"ohh, so you actually rely on the fact sds headers give you odd pointers (and it'll break if we some day change sds). i certainly missed that. actually i don't think i bothered to look at the code yet, just the description (which probably should have had that detail). well, i don't like it very much, if we can't find another way, maybe indeed dicttype should be able to control that. need to think if we have better options.",-1,-1,-1,0.927133858203888,0.9214255213737488,0.9757642149925232,-1.0,accept,unanimous_agreement
1348176460,11595,"the top comment start like this: ""if a key has lsb=1 (e.g. sds strings)"". well, dict type can have flag that makes turns the bias towards even keys and then we set the bit internally and clear it again before it's returned to the dict user. we don't have this need now though, so i didn't implement it. if we want to use any bits in sds keys, this bit is actually the only thing we can rely on right now.",0,0,0,0.9800950288772584,0.9850560426712036,0.9891836643218994,0.0,accept,unanimous_agreement
1348771673,11595,"imho, i don’t think i like the implementation that makes me feel less readable. why not extend dicttype to support operations with different entries (create, set value, get value, or other)?",-1,-1,-1,0.95438152551651,0.9251757264137268,0.9556359648704528,-1.0,accept,unanimous_agreement
1348870100,11595,i don't understand exactly. can you explain more or give an example?,-1,0,0,0.5859649777412415,0.712527334690094,0.700844407081604,0.0,accept,majority_agreement
1348878226,11595,sorry i missed that. it would be better to also explain that sds strings always have the lsb set and why. so it's clear we're counting on that.,-1,-1,-1,0.9884210824966432,0.9891136884689332,0.9839121699333192,-1.0,accept,unanimous_agreement
1348883785,11595,"yes, you're right it wasn't very clearly explained. :) if you like me to continue this at all, then how about adding two bits to dicttype: `keys_are_odd:1` and `keys_are_even:1` and we can assert on that when adding keys and only use the key-as-dicttype optimization if one of these are set. no fallback. is it better?",1,1,1,0.975706934928894,0.9943108558654784,0.9940422177314758,1.0,accept,unanimous_agreement
1349092632,11595,"it's hard to dismiss such a big memory saving. but on the other hand this campaign starts to feel like the code is gonna be ugly and complicated. i was hoping someone comes up with a winning design. let me try to recap, and see if it's not that dirty after all. out of the 4 different optimizations we listed above, the only one that doesn't store a dictentry pointer is this one, which stores an sds. the other 3 will store a dict entry, which in both 32bit and 64bit has 8 byte alignment, unless we don't have_malloc_size (which i'm willing to overlook for a moment, we can skip this optimization for that config). so we can actually rely on the bit scheme you listed here [a link] and if we want it more generic, we can take the `keys_are_odd` flag you proposed earlier (i suggest not to implement it yet but document that idea next to the current limitation). maybe with some effort and some comments it'll not end up too bad. wdyt?",-1,-1,-1,0.9474623203277588,0.8776695132255554,0.940169394016266,-1.0,accept,unanimous_agreement
1350845151,11595,"that's what i'd like to think. with some great reviewing we can get some nice result. :wink: of course, with pointer tricks, we do need casting to/from uintptr_t and flipping bits, etc. which adds complexity but we can isolate that stuff in functions. i thought about defining dictentry as a `union { entry with next; entry without next; etc. }` but it doesn't help much since we need to cast the pointer anyway and can as well cast to different structs depending on bits. i believe the reason for the sun/sparc definition of have_malloc_size is precisely to make it 8 byte aligned in 32-bit builds. that architecture seems to require that though, which x86 doesn't, but what if we enable this on all 32-bit builds? perhaps the dict memory saving covers the alignment penalty. we should probably measure this. [code block] ---- in #9464, i added a createentry callback in dicttype, which replaces the metadata feature and lets the caller allocate the entry and embed whatever they want in the entry, such as key and value. is this what you have in mind? i don't particularily like this though, since it makes the dict api more complex. this pr tries to keep the dict api simple and keep the dict entry opaque, which i think is a good thing. the caller doesn't need to worry about the internals of the dict entry.",1,1,1,0.9501245617866516,0.961638569831848,0.9906296133995056,1.0,accept,unanimous_agreement
1350937391,11595,"regarding prefix_size, i don't think the dict optimizations (specifically for deployment who don't use sets) can overcome the overhead of increasing it, but i'm not sure i care much about the case when we're both in 32bit **and** don't have_malloc_size. either one of these on it's own will be ok, and when they're combined people will have to pay that price. bottom line, so i'd accept the change you suggested.",0,0,0,0.8584200143814087,0.9441039562225342,0.8830417394638062,0.0,accept,unanimous_agreement
1351327304,11595,"yes, it is exactly what i imagined it to be, it is more elegant and abstract.",0,0,0,0.8831995129585266,0.900736927986145,0.5197442770004272,0.0,accept,unanimous_agreement
1363092753,11595,"i have implemented dictentry without 'next' pointer on top of this pr in a separate branch [a link]. (maybe just look at [a link] if we decide to go this way, we can as well include it in this pr since it's almost the same trick. wdyt? if we want the caller to put stuff inside the dict entry, we can either use metadata (already available) or let the caller use `sizeof(dictentry)` and allocate the entry, in which case we can't make it opaque. i don't like either of these options very much. imo it's a better abstraction to isolate the optimizations inside dict and make the entry opaque. the point of putting stuff in the same allocation as the dict entry is to avoid one cache miss and one pointer, right? key-as-entry can accomplish that too by skipping the dict entry for no-value dicts (except for collisions but that's a minority). (if we put key in robj and use the whole robj as the dict key, we can make the main db dict a no-value dict. we just need a special keycompare function.) some ascii diagrams regarding main db dict [code block]",-1,1,-1,0.9801076650619508,0.771489679813385,0.9791839122772216,-1.0,accept,majority_agreement
1364686640,11595,"i skimmed through both this pr (the delta from the previous one), and the last commit in the branch you referred to. i didn't do an in-depth review of each line yet. to be honest, the part i liked the least, was `dictmemusage`. i think the complexity here is acceptable (considering it's all inside dict.c, and consider the memory savings it provides). the other two things that we should validate before proceeding imho are: 1. be confident that this direction doesn't block other things we plan to do soon. 2. do some benchmark and make sure this doesn't have any negative impact (on either hashes, or sets)",0,0,0,0.6209251284599304,0.6602901220321655,0.5031260251998901,0.0,accept,unanimous_agreement
1366160372,11595,"thanks for looking. well, i'd had to come up with something. :) do you have a better idea? it's an estimate but in many cases we sample keys and values, so the total size is an estimate anyway. i just hope my statistical calculations make sense. sure, what other things do you have in mind? of course. i'm hoping the tag action:run-benchmark tag can help with this. assuming none of the above is a blocker, there are still some more things to solve until this is ready. * the defrag code still uses dictsetkey which asserts for sets. (it's possible to implement dictsetkey for keys without entry but it'd fallback to hashing the key. alternatively, we can let dictscandefrag take a struct of `{defrag-alloc, defrag-alloc-key, defrag-alloc-value}` callbacks and let the dict code handle defragging the keys and values as well. any preference?) * in the no-next branch, the tests crash in various places. i'm trying to debug it.",1,1,1,0.9892237782478333,0.994940996170044,0.9961140155792236,1.0,accept,unanimous_agreement
1366412032,11595,"i'm not certain, i thought you know that better than me. feel free to jump in. i'm not certain the tag by itself is enough, we can start with that, but maybe we'll need some specific workflows to hit specific scenarios? please ping filipe if needed. i'm not certain i follow you, but i think we want to avoid re-hashing keys, so the other (slightly uglier) option is preferred.",0,0,0,0.7660074830055237,0.6513280272483826,0.8160852193832397,0.0,accept,unanimous_agreement
1378351402,11595,"we discussed this pr in a core-team meeting and we would like to proceed. we realized that we might wanna change some things later (give up one optimization in favor of another), but since dict will be abstracted, it will be easy to do even after this is merged, or even in a later version (no compatibility issues) please update on the status and next steps.",0,0,1,0.9265297055244446,0.9059779047966005,0.8248330354690552,0.0,accept,majority_agreement
1378911357,11595,this pr is up to date and working. i think it can be merged. i've updated the top comment slightly. (we also discussed omitting the 'next' pointer in all dicts. i have an implementation in a separate branch but it's not stable.),0,0,0,0.9366836547851562,0.8969179391860962,0.9455347061157228,0.0,accept,unanimous_agreement
1378925665,11595,"ok. i'll need to go through a detailed review. but also i remember we were missing some things. one was benchmark, not sure what else is any..",0,0,0,0.840368926525116,0.8470742702484131,0.9075619578361512,0.0,accept,unanimous_agreement
1378951308,11595,benchmark has run now. it looks ok to me. ![a link] see [a link],1,1,1,0.9652148485183716,0.8868579268455505,0.996292531490326,1.0,accept,unanimous_agreement
1380533813,11595,daily ci (for valgrind) [a link],0,0,0,0.985714077949524,0.9879250526428224,0.9921568632125854,0.0,accept,unanimous_agreement
1380650231,11595,"looks like there some assertion in the module tests with 32bit build, ptal",0,1,0,0.9870253205299376,0.5003845691680908,0.9917789101600648,0.0,accept,majority_agreement
1380703148,11595,### automated performance analysis summary this comment was automatically generated given there is performance data available. using platform named: intel64-ubuntu20.04-biredis to do the comparison. in summary: - detected a total of 54 stable tests between versions. ### comparison between unstable and zuiderkwast:key-as-dictentry. time period from a month ago. (environment used: oss-standalone) | test case |baseline unstable (median obs. +- std.dev)|comparison zuiderkwast:key-as-dictentry (median obs. +- std.dev)|% change (higher-better)| note | |--------------------------------------------------------------------------|------------------------------------------|----------------------------------------------------------------|------------------------|--------------------| |memtier_benchmark-10mkeys-load-hash-5-fields-with-1000b-values | 120948 +- 0.3% (3 datapoints) | 120048 +- nan% (1 datapoints) |-0.7% |-- no change -- | |memtier_benchmark-10mkeys-load-hash-5-fields-with-1000b-values-pipeline-10| 173252 +- 0.1% (3 datapoints) | 171867 +- nan% (1 datapoints) |-0.8% |-- no change -- | |memtier_benchmark-10mkeys-load-hash-5-fields-with-100b-values | 170524 +- 0.2% (3 datapoints) | 168662 +- nan% (1 datapoints) |-1.1% |-- no change -- | |memtier_benchmark-10mkeys-load-hash-5-fields-with-100b-values-pipeline-10 | 481923 +- 0.4% (3 datapoints) | 464881 +- nan% (1 datapoints) |-3.5% |potential regression| |memtier_benchmark-10mkeys-load-hash-5-fields-with-10b-values | 202098 +- 0.4% (3 datapoints) | 203361 +- nan% (1 datapoints) |0.6% |-- no change -- | |memtier_benchmark-10mkeys-load-hash-5-fields-with-10b-values-pipeline-10 | 598786 +- 0.1% (3 datapoints) | 605342 +- nan% (1 datapoints) |1.1% |-- no change -- | |memtier_benchmark-1mkeys-100b-expire-use-case | 259532 +- 0.1% (3 datapoints) | 260207 +- nan% (1 datapoints) |0.3% |-- no change -- | |memtier_benchmark-1mkeys-10b-expire-use-case | 259175 +- 0.3% (3 datapoints) | 259582 +- nan% (1 datapoints) |0.2% |-- no change -- | |memtier_benchmark-1mkeys-1kib-expire-use-case | 258416 +- 0.7% (3 datapoints) | 260832 +- nan% (1 datapoints) |0.9% |-- no change -- | |memtier_benchmark-1mkeys-4kib-expire-use-case | 259649 +- 0.4% (3 datapoints) | 259729 +- nan% (1 datapoints) |0.0% |-- no change -- | |memtier_benchmark-1mkeys-hash-hget-hgetall-hkeys-hvals-with-100b-values | 274890 +- 0.4% (3 datapoints) | 276307 +- nan% (1 datapoints) |0.5% |-- no change -- | |memtier_benchmark-1mkeys-list-lpop-rpop-with-100b-values | 276134 +- 0.6% (3 datapoints) | 276369 +- nan% (1 datapoints) |0.1% |-- no change -- | |memtier_benchmark-1mkeys-list-lpop-rpop-with-10b-values | 277877 +- 0.3% (3 datapoints) | 279045 +- nan% (1 datapoints) |0.4% |-- no change -- | |memtier_benchmark-1mkeys-list-lpop-rpop-with-1kib-values | 271764 +- 0.2% (3 datapoints) | 270485 +- nan% (1 datapoints) |-0.5% |-- no change -- | |memtier_benchmark-1mkeys-load-hash-5-fields-with-1000b-values | 136116 +- 0.3% (3 datapoints) | 131817 +- nan% (1 datapoints) |-3.2% |potential regression| |memtier_benchmark-1mkeys-load-hash-5-fields-with-1000b-values-pipeline-10 | 180910 +- 1.0% (3 datapoints) | 179835 +- nan% (1 datapoints) |-0.6% |-- no change -- | |memtier_benchmark-1mkeys-load-list-with-100b-values | 200173 +- 0.9% (3 datapoints) | 201852 +- nan% (1 datapoints) |0.8% |-- no change -- | |memtier_benchmark-1mkeys-load-list-with-10b-values | 233292 +- 0.2% (3 datapoints) | 235273 +- nan% (1 datapoints) |0.8% |-- no change -- | |memtier_benchmark-1mkeys-load-list-with-1kib-values | 143119 +- 0.3% (3 datapoints) | 144046 +- nan% (1 datapoints) |0.6% |-- no change -- | |memtier_benchmark-1mkeys-load-stream-1-fields-with-100b-values | 189935 +- 0.9% (3 datapoints) | 189733 +- nan% (1 datapoints) |-0.1% |-- no change -- | |memtier_benchmark-1mkeys-load-stream-1-fields-with-100b-values-pipeline-10| 541997 +- 0.4% (3 datapoints) | 543571 +- nan% (1 datapoints) |0.3% |-- no change -- | |memtier_benchmark-1mkeys-load-stream-5-fields-with-100b-values | 151407 +- 0.7% (3 datapoints) | 150564 +- nan% (1 datapoints) |-0.6% |-- no change -- | |memtier_benchmark-1mkeys-load-stream-5-fields-with-100b-values-pipeline-10| 337467 +- 1.3% (3 datapoints) | 344005 +- nan% (1 datapoints) |1.9% |-- no change -- | |memtier_benchmark-1mkeys-load-string-with-100b-values | 235254 +- 0.6% (3 datapoints) | 236943 +- nan% (1 datapoints) |0.7% |-- no change -- | |memtier_benchmark-1mkeys-load-string-with-100b-values-pipeline-10 | 805071 +- 1.6% (3 datapoints) | 821425 +- nan% (1 datapoints) |2.0% |-- no change -- | |memtier_benchmark-1mkeys-load-string-with-10b-values | 252614 +- 0.2% (3 datapoints) | 253519 +- nan% (1 datapoints) |0.4% |-- no change -- | |memtier_benchmark-1mkeys-load-string-with-10b-values-pipeline-10 | 1043960 +- 0.1% (3 datapoints) | 1037922 +- nan% (1 datapoints) |-0.6% |-- no change -- | |memtier_benchmark-1mkeys-load-string-with-1kib-values | 215486 +- 0.1% (3 datapoints) | 216643 +- nan% (1 datapoints) |0.5% |-- no change -- | |memtier_benchmark-1mkeys-load-zset-with-10-elements-double-score | 133177 +- 0.4% (3 datapoints) | 131313 +- nan% (1 datapoints) |-1.4% |-- no change -- | |memtier_benchmark-1mkeys-load-zset-with-10-elements-int-score | 168669 +- 0.5% (3 datapoints) | 167078 +- nan% (1 datapoints) |-0.9% |-- no change -- | |memtier_benchmark-1mkeys-string-get-100b | 270030 +- 0.5% (3 datapoints) | 271417 +- nan% (1 datapoints) |0.5% |-- no change -- | |memtier_benchmark-1mkeys-string-get-100b-pipeline-10 | 1427914 +- 0.3% (3 datapoints) | 1430684 +- nan% (1 datapoints) |0.2% |-- no change -- | |memtier_benchmark-1mkeys-string-get-10b | 271456 +- 0.5% (3 datapoints) | 271334 +- nan% (1 datapoints) |-0.0% |-- no change -- | |memtier_benchmark-1mkeys-string-get-10b-pipeline-10 | 1427805 +- 0.7% (3 datapoints) | 1434849 +- nan% (1 datapoints) |0.5% |-- no change -- | |memtier_benchmark-1mkeys-string-get-1kib | 271239 +- 0.5% (3 datapoints) | 270881 +- nan% (1 datapoints) |-0.1% |-- no change -- | |memtier_benchmark-1mkeys-string-get-1kib-pipeline-10 | 1422791 +- 0.2% (3 datapoints) | 1436788 +- nan% (1 datapoints) |1.0% |-- no change -- | |memtier_benchmark-1key-list-10-elements-lrange-all-elements | 252518 +- 0.2% (3 datapoints) | 254024 +- nan% (1 datapoints) |0.6% |-- no change -- | |memtier_benchmark-1key-list-100-elements-lrange-all-elements | 155628 +- 0.4% (3 datapoints) | 155317 +- nan% (1 datapoints) |-0.2% |-- no change -- | |memtier_benchmark-1key-list-1k-elements-lrange-all-elements | 28348 +- 0.2% (3 datapoints) | 28447 +- nan% (1 datapoints) |0.4% |-- no change -- | |memtier_benchmark-1key-set-10-elements-smembers | 208332 +- 0.8% (3 datapoints) | 205808 +- nan% (1 datapoints) |-1.2% |-- no change -- | |memtier_benchmark-1key-set-10-elements-smembers-pipeline-10 | 700134 +- 3.7% (3 datapoints) | 716401 +- nan% (1 datapoints) |2.3% |-- no change -- | |memtier_benchmark-1key-set-10-elements-smismember | 267170 +- 0.4% (3 datapoints) | 271167 +- nan% (1 datapoints) |1.5% |-- no change -- | |memtier_benchmark-1key-set-100-elements-smembers | 120970 +- 0.1% (3 datapoints) | 122871 +- nan% (1 datapoints) |1.6% |-- no change -- | |memtier_benchmark-1key-set-100-elements-smismember | 239807 +- 0.6% (3 datapoints) | 242402 +- nan% (1 datapoints) |1.1% |-- no change -- | |memtier_benchmark-1key-set-1k-elements-smembers | 20122 +- 0.5% (3 datapoints) | 19573 +- nan% (1 datapoints) |-2.7% |-- no change -- | |memtier_benchmark-1key-zset-10-elements-zrange-all-elements | 124358 +- 0.2% (3 datapoints) | 124843 +- nan% (1 datapoints) |0.4% |-- no change -- | |memtier_benchmark-1key-zset-100-elements-zrange-all-elements | 29350 +- 0.1% (3 datapoints) | 29749 +- nan% (1 datapoints) |1.4% |-- no change -- | |memtier_benchmark-1key-zset-1k-elements-zrange-all-elements | 5396 +- 0.7% (3 datapoints) | 5384 +- nan% (1 datapoints) |-0.2% |-- no change -- | |memtier_benchmark-1key-zset-1m-elements-zrevrange-5-elements | 257408 +- 1.3% (3 datapoints) | 261668 +- nan% (1 datapoints) |1.7% |-- no change -- | |memtier_benchmark-2keys-set-10-100-elements-sdiff | 27577 +- 0.3% (3 datapoints) | 27512 +- nan% (1 datapoints) |-0.2% |-- no change -- | |memtier_benchmark-2keys-set-10-100-elements-sinter | 134830 +- 1.1% (3 datapoints) | 134634 +- nan% (1 datapoints) |-0.1% |-- no change -- | |memtier_benchmark-2keys-set-10-100-elements-sunion | 26448 +- 0.2% (3 datapoints) | 26523 +- nan% (1 datapoints) |0.3% |-- no change -- | |memtier_benchmark-2keys-stream-5-entries-xread-all-entries | 118346 +- 0.1% (3 datapoints) | 118037 +- nan% (1 datapoints) |-0.3% |-- no change -- | |memtier_benchmark-2keys-stream-5-entries-xread-all-entries-pipeline-10 | 189946 +- 0.8% (3 datapoints) | 192854 +- nan% (1 datapoints) |1.5% |-- no change -- |,0,0,0,0.9319674372673036,0.992412269115448,0.8982936143875122,0.0,accept,unanimous_agreement
1380712388,11595,and the above report was generated with the reporting tool. even tough it seems we have stable numbers there are 2 variations on large hashes. test case | baseline unstable (median obs. +- std.dev) | comparison zuiderkwast:key-as-dictentry (median obs. +- std.dev) | % change (higher-better) | note -- | -- | -- | -- | -- memtier_benchmark-1mkeys-load-hash-5-fields-with-1000b-values | 136116 +- 0.3% (3 datapoints) | 131817 +- nan% (1 datapoints) | -3.2% | potential regression memtier_benchmark-1mkeys-load-hash-5-fields-with-1000b-values | 136116 +- 0.3% (3 datapoints) | 131817 +- nan% (1 datapoints) | -3.2% | potential regression notice the use machine is on intel lab and fully devoted to this. there should be small error on the numbers -- notice the variance of data of same version is minimal. please use biredis as platform to look for more stable data when going over the dashboard: [a link],0,0,0,0.9179272055625916,0.9736777544021606,0.9902618527412416,0.0,accept,unanimous_agreement
1382418419,11595,i don't understand why we got assertion on entryhasvalue in the 32-bit build [a link] that dict has values so all entries in it should have values. i've added another assert after zmalloc to check that we get 8 bytes alignment. maybe bad alignment why it was accidentally tagged wrong in the lsb bits. i have no other ideas why it could happen.,-1,0,0,0.5989912748336792,0.5564101934432983,0.6143503785133362,0.0,accept,majority_agreement
1382715854,11595,"maybe for small allocations it can do that? i.e. allocations of less than 8 bytes obviously don't have to be 8 byte aligned. i suppose that's true for allocations of 12 bytes? although that would be odd that the only test that fails on this is a module config test (specifically the ones in which dictreplace ends up doing a replacement) note that in this case it's 32bit jemalloc, and we don't seem to have any coverage for 32it glibc malloc. maybe run some specific tests on 32 bit malloc (both jemalloc and glibc) to learn more what it does for a few allocation sizes (4 and 12)? i see this is in dictreplace when trying to free the previous value, which is strange since i see the same assertion after allocating the entry in dictinsertintobucket doesn't fail.",0,0,0,0.9819132685661316,0.966708242893219,0.9672436118125916,0.0,accept,unanimous_agreement
1386926036,11595,"i assume the benchmarks you shared cover the first 3 bullets (right?), what about the last one? maybe for that last one we need to run some local / manual benchmark?",0,0,0,0.9861039519309998,0.9919407963752748,0.9899023771286012,0.0,accept,unanimous_agreement
1387096085,11595,"i had some time so i looked into it. first, i verified that both glibc and jemalloc, in a 32bit build, always return addresses that are 8 bytes aligned (even for small allocations). secondly, i found the problem: [code block] we call dictfreeval on a stack allocated dictentry (not a heap one). i suppose that since dictentry is opaque, this is the only place we're at risk of doing such a thing.",0,0,0,0.9678415656089784,0.9839702248573304,0.9235154986381532,0.0,accept,unanimous_agreement
1396233006,11595,## benchmark redis-server --save '' warmup: populate with random elements in the range 0..200k. the set soon reaches a more or less constant size near 200k. redis-benchmark -p 10 --threads 2 -n 10000000 -r 200000 sadd myset __rand_int__ benchmark 1: a relatively constant set. run the warmup again. most elements already exist. redis-benchmark -p 10 --threads 2 -n 10000000 -r 200000 sadd myset __rand_int__ benchmark 2: a set that keeps growing and growing. redis-cli flushdb redis-benchmark -p 10 --threads 2 -n 10000000 -r 100000000 sadd myset __rand_int__ ### key-as-dictentry (this pr) 1. [code block] 2. [code block] ### unstable 1. [code block] 2. [code block] ### compare results thoughput | | this pr | unstable | diff | |---|------------|------------|-------------| | 1 | 1109631.62 | 1051414.12 | +5.54% | | 2 | 887862.94 | 784006.25 | +13.2% | latency p50 | | this pr | unstable | diff | |---|------------|------------|-------------| | 1 | 0.399 | 0.415 | -3.85% | | 2 | 0.503 | 0.567 | -11.3% |,0,0,0,0.9019301533699036,0.9853709936141968,0.9677024483680724,0.0,accept,unanimous_agreement
1396577966,11595,"great (maybe mention it in the top comment), can you have a look at the possible regressions filipe posted, see if you can reproduce them or dismiss them. anything else missing before we can merge this?",1,1,0,0.8349254727363586,0.883266031742096,0.6558342576026917,1.0,accept,majority_agreement
1396698804,11595,and will these as 2 new tests to the spec :),1,1,0,0.7078301906585693,0.989476442337036,0.9907694458961488,1.0,accept,majority_agreement
1396897280,11595,"so the only thing that left is the two possible regressions, right? it could be due to some assertions added, maybe we can drop them..",0,0,0,0.980776846408844,0.9834887385368348,0.989406168460846,0.0,accept,unanimous_agreement
1397096758,11595,"i tried memtier_benchmark-1mkeys-load-hash-5-fields-with-1000b-values locally. i got the exact same numbers for p50 latency and 1% regression in throughput. memtier_benchmark-1mkeys-load-hash-5-fields-with-1000b-values `memtier_benchmark --test-time 180 ""--pipeline"" ""10"" ""--data-size"" ""1000"" --command ""hset __key__ field1 __data__ field2 __data__ field3 __data__ field4 __data__ field5 __data__"" --command-key-pattern=""p"" --key-minimum=1 --key-maximum 1000000 -c 50 -t 4 --hide-histogram` the command was taken from [a link]. unstable: [code block] this pr: [code block] looking at the graphs on the grafana site (screenshot below), it seems this branch (the short brown line with fewer data points) follow the same pattern as unstable (the longer yellow line). ![a link] thus, i would guess it's not a real regression, just coincidence. wdyt?",0,0,0,0.9734607338905334,0.9895642995834352,0.9903621673583984,0.0,accept,unanimous_agreement
1397134402,11595,"could that have been some environmental issue? in any case, if we can't reproduce it locally, i'm guessing it's nothing. anything else?",0,0,0,0.9098854660987854,0.9383800625801086,0.9685558676719666,0.0,accept,unanimous_agreement
1397141793,11595,"do you want to start the daily again just to be sure it's stable? other than that, i believe it's ready to merge. if you have a lot of time, perhaps you want to find the problem in the dict-no-next branch too? :-) i have rebased it on top of this branch' last changes. when that branch is working well, it can bring a lot of benefit since it saves memory in all dicts.",1,1,1,0.93627268075943,0.9894575476646424,0.9900736808776855,1.0,accept,unanimous_agreement
1397187277,11595,fully ci [a link],0,0,0,0.9865716695785522,0.985122561454773,0.9920907020568848,0.0,accept,unanimous_agreement
1398330389,11595,"and confirmed that locally ( on a stable cascade lake system there is no variance ) [code block] sorry for the delay on this but wanted to check manually. all set performance wise :) ps: as stated, we're adding 2 extra benchmarks to track set ingestion over time to [a link]",1,1,1,0.9796671867370604,0.9857516884803772,0.9878382682800292,1.0,accept,unanimous_agreement
63564549,2143,"sir, this is one hell of a pull request. i will definitely read the code and test it tomorrow while flying 35000ft above the ground. ![a link]",-1,-1,1,0.8752763271331787,0.9881622195243835,0.5718380212783813,-1.0,accept,majority_agreement
63770677,2143,impressive work. i read the code all in once and it was quite clear and concise. i saw no immediate bugs. also great you have extensive test code in there (nearly half the new code is tests). i'm sure we can get this into unstable soon. :airplane:,1,1,1,0.9884612560272216,0.989030420780182,0.9955295920372008,1.0,accept,unanimous_agreement
63849239,2143,"thanks for the review! i incorporated your fixes (plus a few more cleanups) into the existing commit. the diff is below since it's difficult to see changes between rebased forced pushes. i don't see any warnings against clang or gcc-4.9.2 anymore, but feel free to try more compilers and tests. the more problems we find now the fewer cleanup commits we have to apply later. :) [code block]",1,1,1,0.9879867434501648,0.9950807094573976,0.9965500831604004,1.0,accept,unanimous_agreement
64084254,2143,"updated with below diff thanks to sunheehnus at [a link] the fix also caught an invalid check in a test case. now that we're merging quicklist nodes more properly, we use fewer nodes in a test (26 -> 25) and that's a good thing. [code block]",1,0,1,0.8183448910713196,0.5373104214668274,0.9510422348976136,1.0,accept,majority_agreement
64102223,2143,"love the feature and that there are even reviews here. thank you. tomorrow i'll be back in sicily from bracelona, and monday i'll review the code as well.",1,1,1,0.9904720783233644,0.9946893453598022,0.9967159032821656,1.0,accept,unanimous_agreement
64106518,2143,updated: [code block] thanks for the ongoing reviews!,1,1,1,0.9520191550254822,0.9284403324127196,0.985663652420044,1.0,accept,unanimous_agreement
64198291,2143,finish my review. you did impressive work. learn a lot from you. :) :+1: :+1: :+1:,1,1,1,0.988820493221283,0.99587482213974,0.997269093990326,1.0,accept,unanimous_agreement
64204019,2143,"wow, , your work is great as well.",1,1,1,0.9890246987342834,0.994577407836914,0.9956327080726624,1.0,accept,unanimous_agreement
64204246,2143,"thank you, after matt changes the code according to the reviews i'll do mine as well... waiting because there is already too many good things ongoing here :-)",1,1,1,0.9905624985694884,0.9915807843208312,0.9958072900772096,1.0,accept,unanimous_agreement
64265674,2143,"update: `3 files changed, 124 insertions(+), 43 deletions(-)` new changes: - imported ziplists now get appended element-by-element instead of just added as a single block of elements (see `quicklistappendvaluesfromziplist()` and wrapper for it `quicklistcreatefromziplist()`). - fixed incorrect iterator updating during a delete; turned that section of code inside out - incorporated suggested fixes from [a link] - delete range fixes when given crazy arguments + tests for the crazy arguments - use proper string value instead of casting a long long to a char\* incorrectly - during rotate, update ziplist index pointer during when appropriate - added tests to trigger some previously untraversed code paths (see the number pushes in test `rotate 500 val 5000` — now triggers the longval segfault and the head-ziplist-moves-itself segfault (before they were fixed)). - improved test cases to accumulate errors when `ql_verify` fails instead of just printing a failure message in the middle of 10,000 line test output. - changed rdb ziplist import to use `quicklistcreatefromziplist()` [code block]",0,0,0,0.9758795499801636,0.98404061794281,0.98076069355011,0.0,accept,unanimous_agreement
64265858,2143,great :-) my review starts tomorrow morning.,1,1,1,0.990649402141571,0.9960185885429382,0.9970608353614808,1.0,accept,unanimous_agreement
64422310,2143,"finally... initial feedback, based on an initial review :-) 1. great work, very appreciated: this can make a difference for many users, and may also turn the list type into a more interesting type for redis, because of the memory benefits not easily applicable to zsets, for example, that have certain use cases overlaps with lists. thank you. 2. impressive amount of tests: +1. 3. t_list.c was simplified by the changes because of the single encoding in many parts, which is great. 4. we should allow list-max-ziplist-value option to be processed, just ignored. 5. the way lists are stored into rdbs now is a non trivial speed and size regression for rdbs for data sets composed of many small lists. i think we should introduce a new rdb type for quicklists where each ziplist is saved and loaded as a blob. ziplists are internally architecture agonistic so this is pretty simple to do. the result is huge speedup in loading/saving times for big lists compared to pre-quicklist redis. i can take care of implementing that. 6. there are zmalloc() checks around the code, these can safely be removed, since the redis memory management way is: abort on oom since recovery is futile. 7. remove all the “static”: no symbols on redis crashes stack traces otherwise. 8. this is not a problem, but i noted that quicklistdelrange() is a lot smarter it needs to be for ltrim semantics. in theory complexity could be reduced. 9. we may remove the listtype\* wrappers, there is only a single type. however the current code is more adapt to change in the future. tradeoffs... better to take the wrappers probably. 10. quicklists will probably stress the allocator more than lists on long-lasting workloads that remove/add elements to lists. i bet users will complain about this soon or later. however because of the memory savings is a neat win: if you use 100mb x 1.4, it is a lot better than using 1gb x 1.02. we should just remember to ignore complains sometimes... ;-) 11. maybe we should get rid also of the number of items parameter? from all your graphs, there is a soft spot at around ~80 items per ziplist where we have all the advantages. but also would be interesting to see the speed charts. 12. in your blog post i didn’t saw much about speed, did you tested a few workloads maybe? ok that’s the initial impressions. i’ll review more carefully and test the code more tomorrow to provide more feedbacks. since this is a lot of new code, it may be wise to merge just into unstable and wait for it to hit a stable release naturally (redis 3.2). because of this, also, that’s the right moment to add the new rdb format for quicklists as well... however it is important to be able to read old rdb formats, so some code (already present in your quicklist implementation) to handle the old list formats should be retained. more comments asap, i really appreciated this effort, and also thank you for the reviews of matt’s code.",1,1,1,0.9928370118141174,0.9956427812576294,0.996754229068756,1.0,accept,unanimous_agreement
64661045,2143,"today i checked the implementation better, and ran a few fuzzy tests for a long time, trying to break it from different point of views. i also did some performance profiling. tldr: 1. it does not break easily... i was not able to crash it a single time using complex access patters, inserts, deletions, popping, with many clients at the same time in long lists, with op set designed to stress merge operations. 2. it is faaaast. much faster tha common lists. lrange is impressive but the real killer for me was `del`. now del is proportional to the number of ziplists... so it's o(n/big_constant). the only really enhancement that we need is to serialize it as ziplists, because of the stuff expressed above, but there is actually another thing that i forgot to mention, which is migrate. quicklists + rdb serialization as ziplists = moving big lists in a trivial amount of time. another tiny feature request is to see the actual number of ziplist nodes in ""debug object"" output. i'm really enthusiastic about this pr, it is a complex thing, well executed, with big impact.",1,1,1,0.5337569117546082,0.4558659791946411,0.8853292465209961,1.0,accept,unanimous_agreement
64667492,2143,"wow, if is more fast than common lists, this will be great. i'm learning a lot from all of you, very awesome. :wave:",1,1,1,0.9931182861328124,0.9958035349845886,0.9969915151596068,1.0,accept,unanimous_agreement
65842924,2143,"warning: big follow-up reply below! yeah, as far as i know, there aren't any other options we accept but ignore. we could just keep a list of options to ignore to loop over if we remove things in the future. that is possible, but for loading saved ziplists, they should be re-added element-by-element instead of just restored in-place. re-adding ziplists element-by-element cleans up any internal node fragmentation from users inserting/deleting elements in the middle of the quicklist and breaking the internal maximum fill levels. yeah, there are, but there are also zmalloc return value checks in other places in the redis code, so i just kept the pattern alive (e.g. adlist.c). it's written to be a generic data structure, so if people redefine zmalloc to just malloc, it should still do the right thing. at some point i'm going to make all the redis data structures a reusable external library. :) that's a new one to me. i'd prefer to leave some static signal because compilers can generate better code when they know certain functions don't leave the file, and it's made to be reusable-outside-of-redis. we could label functions as redis_static then define redis_static to nothing for redis builds. we get a lot of crash reports with no symbols anyway (packagers manually stripping the binary?). everything is made to be generic and reusable, even if redis doesn't need some of the operations. if there's a shortcut to make ltrim usage quicker, we should certainly add it. :) yeah, that's just a design decision. i left them there for redundant type checking, but we could remove them completely. difficult to say. i could see it happening either way. it probably depends on usage patterns (append-only workload with heavy reads? best memory usage). there are more optimizations we could make to reduce fragmentation (pre-allocating ziplists up to 4k or 8k), but they may not be useful for everybody. removing the number of items _is_ possible now, but for another reason. the number of _items_ is actually a really bad measurement for ziplist performance. if we set a fixed limit of 80 items and people are inserting 5 mb images into a list, that's _really_ bad. if people are inserting only integers, that's also bad because we could still store more integers in one ziplist without impacting performance. commit [a link] adds the option to use dynamic ziplist length based on total ziplist _size_ instead of a maximum length. using a maximum size gives us _optimal_ pointer overhead reduction while also giving us _optimal_ speed due to ziplist reallocations. (the slowdown in large ziplists is because of memory reallocation, so forcing ziplists to remain small gives us better bounds on ziplist performance. also see [a link] so, we could modify the option in three ways: completely remove the option and just set a global default of 4k or 8k ziplist sizes, change the option to only allow selecting 4k, 8k, or 16k ziplist sizes, or leave it as-is where we can select ziplist sizes or ziplist length. it just depends on if we think letting people tune their sizes has any benefit. (we should probably rename the option too?) setting ziplist limits based on size instead of length _also_ protects us from users inserting large elements into a ziplist and causing the ziplist to grow too big. now, if someone inserts a large element, that element gets its own ziplist assigned and no other elements will insert to that ziplist. we automatically get protected from bad performance due to big elements. very observant! the first post had no performance testing. insert/delete performance testing is at [a link] i haven't done performance testing of just reads yet, but those should be very close to the linked list version. that certainly works! [larger sidenote: we should probably end up using a slightly better workflow. we currently have `unstable -> stable`, but we really need a few more steps [[this can also help fix the ""too many redis point releases"" or ""major new features in minor version numbers"" problem if we push proper maintenance releases (2.8.18.1, 2.8.18.2, etc) versus others]]. git workflows for public projects are solved problems we don't have to re-invent. see things like [a link] — i have an entire local 'pu' branch (not public yet, but soon) that has about 40 commits from public prs waiting for official approval. we have too many proposed things that nobody gets around to testing/using, and since nobody tests or uses them, they never get approved, so it's a cycle. ""unstable"" is actually _too_ stable! :)] pluggable, key/value metadata rdb format, right? :) great to hear! it sure broke a lot while being written. most of the code tests run in a loop covering all the different edge cases for ziplist sizes too which helps a lot. yay! deleting in the middle of large lists is probably the biggest speed-up compared to linked list. oh, i hadn't even considered that. it'll help a lot with large lists in clusters during slot moving. good point. i'll look into adding it (the quicklist node count is just quicklist->len; we could also dump the individual ziplist info (byte size and element count) in debug output too). thanks for all the notes! so, what's remaining: - rdb format upgrade - save ql as series of zls - restore ql by appending zl elements (`quicklistappendvaluesfromziplist()`) - add ql-specific debug info - move existing list `list-max-ziplist-value` option to new ""ignore"" section of config parsing - maybe rename `list-max-ziplist-entries` (or remove it entirely and just set default ziplist size to 4k or 8k) - investigate if `quicklistdelrange()` can take a faster ltrim shortcut. - maybe rename static functions to redis_static and set redis_static="""" during build",0,1,-1,0.5882205963134766,0.4883290827274322,0.8796719312667847,,review,no_majority_disagreement
66252605,2143,"quick reply to move forward asap: 1. ignored options: we had it in the past at some point, for a similarly popular option. instead of having a list, given that's a single option, for now just removing the option entirely from config set/get and processing it in redis.conf as startup emitting a warning may do. we'll deprecate it entirely at some point in the future. 2. you write ""re-adding ziplists element-by-element cleans up any internal node fragmentation from users inserting/deleting elements in the middle of the quicklist and breaking the internal maximum fill levels."". i don't think this is a good idea for a simple reason: if you have an issue with fragmentation while the system is running, then you have a general problem. if you don't have a general problem, then you should not care about restoring the same setup when the list is reloaded. otherwise semantically is like if restarts are needed from time to time. which of the two is true? :-) 3. malloc return value check: the old checks are there because those libs were imported form old code that did not ignored the malloc return value (generic libs indeed). we may do this: remove checks inside the redis core anyway. retain them when possible for code implemented already in form of a quasi-standalone-library in order to make reuse simpler. 4. symbols: stack traces in issues or emitted by running tests that i was no longer able to reproduce saved me a lot of times. the hypothetical speed gain, we don't even now if there is. given that fixing issues with little efforts is critical to the project, removing the statics is fundamental. not blocking to get merged since i can do it myself easily, but would appreciate if prs follow the existing code style to avoid useless friction for useful code to get inserted. 5. about the workflow, i agree, and don't agree. larger discussion in the ml, but my fear is that our current core team of two is so tiny that with a more sane development model may end reducing our output. tl;dr: i want to merge this code, there are just small things left. the only blocking one is rdb format / fragmentation on load issue. if this gets fixed i'll merge it asap. about rdb, if you want to act in order for this work be merged: 1. you can break the rdb format compatibility, upgrading the number. however if you start your work in this direction, please ping me, i want to break rdb format for another issue, better that we break it a single time. 2. please make the new code backward compatible with old rdb files (but not the contrary..., not needed). 3. if you really really think the defragmentation stuff is a good idea, please follow up here and let's implement this as a single pass operation that can be performed after loading the object: often it may not be needed _at all_, or we can just have thresholds that modify nodes when they are too far from optimal sizes. 4. i can help implementing this if needed (i don't think you need any help but you may be already up to something else). let's work together to let this code enter the code asap so that users can stress test it.",0,0,0,0.9712809920310974,0.9881766438484192,0.9858046174049376,0.0,accept,unanimous_agreement
66360123,2143,"(just replying to this one thing right now) example of ziplists in a quicklist: [code block] if someone inserts at position 40, we now have: [code block] the ""fragmentation"" is just the second node now has 8 elements instead of the maximum allowed. if you re-create the quicklist element-by-element, all internal nodes will return to their maximum fill. the only way to ""fix"" it during runtime is to always re-create the entire list after any mid-list insert or delete, which isn't reasonable. (if multiple adjacent nodes end up below the allowed fill level, they will get merged. it's only individual inserts or deletes that create not-maximum-fill nodes.) so, there isn't actually a problem here. but, if we re-add elements individually at rdb restore time, then we aren't persisting the 'fragmentation' across restarts.",0,0,0,0.9502777457237244,0.9727120995521544,0.9240695238113404,0.0,accept,unanimous_agreement
66372340,2143,"persisting fragmentation is not a problem per se: is what happens if the system keeps running instead of being restarted, which is the optimum, so we should not try to optimized for the wrong case. instead the difference in loading time between how it is now, and how i think it should be, is 40x (test it, it's just a bet, but i think is more than 1 order of magnitude for sure). so what you want more, 40x faster any-size lists loading, or persisting fragmentation which is what happens anyway in the optimal case?",0,0,0,0.957078456878662,0.9582279324531556,0.9773226976394652,0.0,accept,unanimous_agreement
66374118,2143,persisting fragmentation isn't the problem — _restoring_ fragmentation is what we're trying to avoid. :) restoring lists element-by-element (instead of entire ziplists at once) could be fast enough (*untested) since it's just sequential append operations.,1,1,1,0.5808622241020203,0.910316526889801,0.9714222550392152,1.0,accept,unanimous_agreement
66374585,2143,"yep but it is what happens when the system is not restated, which is what we hope in the first instance, so i mean, an optimization that you can't control as a developer of the feature, that may not happen at all, is imho futile (unless it has zero drawbacks). the list fragmentation is going to be, anyway, bound fortunately. and here we have a tradeoff, an optimization you can't control, vs speed in critical moments (restarts / bgsaves).",0,0,0,0.9227490425109864,0.9116671681404114,0.5387686491012573,0.0,accept,unanimous_agreement
66527543,2143,"freshly rebased quicklist branch is pushed here. all the tests still pass, though i did have to update one test: [a link] done! let me know what else you want to change with this rdb version increase (6->7). i'll get it added to this branch. done! the rdb loader can still restore old lists from linked lists or individual ziplists. not done! if we want to test it, it's a one line change. we would just change `quicklistappendziplist()` to `quicklistappendvaluesfromziplist()` in the rdb loading code. i figured it out mostly okay. one of the worst parts was figuring out tests were failing because this thing had to be updated too (there weren't notes anywhere. i added big all-caps notes for future editors.): [code block] ### remaining i'll work on the rest of these today/tomorrow: - add ql-specific debug info - ignore existing list list-max-ziplist-value option - rename list-max-ziplist-entries, ignore old option - investigate if quicklistdelrange() can take a faster ltrim shortcut. - remove static - though, all i see in the redis codebase now are 'static' declarations everywhere. :) - `~/repos/redis/src% grep ^static *.c |wc -l` = `292` :dancer:",0,1,0,0.7131174206733704,0.8227264881134033,0.5452856421470642,0.0,accept,majority_agreement
66528988,2143,"cool, thank you. this will get merged asap.",1,1,1,0.9858304858207704,0.9877732396125792,0.9939393997192384,1.0,accept,unanimous_agreement
66577725,2143,"i've found a few more improvements to make, so let's hold off until attempting to merge this until next week. i want to run a few more tests to compare performance differences too. :)",1,1,1,0.983639657497406,0.99177086353302,0.9939807653427124,1.0,accept,unanimous_agreement
67613852,2143,"ping! i'll blocking merging unstable into testing until this is ready, we can't miss this from next testing release.",0,1,1,0.8181313276290894,0.8905983567237854,0.5812026262283325,1.0,accept,majority_agreement
67656244,2143,update imminent. adding a few more notes to the accompanying benchmarticle. will push within an hour hopefully. :gift:,1,1,1,0.9481143951416016,0.8593037128448486,0.9952551126480104,1.0,accept,unanimous_agreement
67668408,2143,"things to review: - i enabled a [a link] in lzf decompression - the new `redis-server test quicklist` test runs over 200,000 test iterations and takes between 20 seconds and 45 seconds depending on your test machine. - ~~i haven't valgrind'd the new changes yet~~ - have run valgrind now (takes 45 minutes for the test to complete under valgrind!) — `==16396== all heap blocks were freed -- no leaks are possible` - i think i [a link] to sds, but i'm really bad at knowing what i can add there. :) - we allow [a link], but don't print warnings about old options yet.",0,0,-1,0.7824828624725342,0.5079604387283325,0.712304413318634,0.0,accept,majority_agreement
67955281,2143,"hello matt, thank you for all the work put on this! i've yet to finish my review but i found two things to report asap. maybe i'll comment more later. 1. there are memory leaks apparently, if you run `make test` on your latest commit branch on osx, the `leaks` utility will detect a number of lost blocks. you can get more info by running the whole test over valgrind, like this: [code block] if you don't want to run the full test to start debugging, just run selected units with --single. there are leaks in the following units: type/set, unit/scripting, integration/aof. i did not debugging at all so no clues, sorry. however the leaks are reproducible at every run, you'll track it in no time. 1. lzf macro for asm is probably a bad idea, from the lzf changelog of a version that is newer than the one we run here: _\- finally disable rep movsb - it's a big loss on modern intel cpus, and only a small win on amd cpus._. i suggest reverting the lzf change and opening a new issue where we understand if we want to switch from 3.5 to 3.6, where lzf claims some speed improvement. i hope to have more in the next hours, however since i'll take a few days away from work i may be silent for a few days. thanks.",1,1,1,0.9809866547584534,0.9920946955680848,0.9929184317588806,1.0,accept,unanimous_agreement
67956581,2143,"those could have been because it got rebased against unstable when unstable had the bad merge with set operations. (also related to that: the ci didn't detect all those errors _and_ the ci doesn't alert anybody when there are errors. is there an easy fix? maybe have the ci email the redis-dev list when a valgrind test fails on a public branch? travisci could do that (and more) automatically too.) updated: re-based to current unstable. removed lzf asm thing too. though, what is the ""lzf changelog?"" i didn't see a reference to where the implementation came from (version 3.5? version 3.6? from where?) so i couldn't check versions myself.",0,0,0,0.9440321922302246,0.9717005491256714,0.979476511478424,0.0,accept,unanimous_agreement
67957886,2143,"not sure what is the problem with unstable, i guess spopcommand() was broken and later fixed into unstable but your code is still rebased with a broken version? so this would mean -> we have no problems with quicklist at all, hopefully. about the ci, it should definitely detect it, and it can be configured to send emails, currently is disabled because it is a bit annoying... since there are many false positives when you run the test _so_ many times. however i can improve recidiv in order to send only if there is a given pattern (valgrind for example). however normally i go to check the ci manually very often, but since i'm in my home town, this is what happened: i'm here without access to the box, and the kind person that helps us to keep our house clean apparently powered off the computer for error... :-) no longer ci until i return back first days of january. about lzf: 1. changelog of lzf is at their official site: just download the latest tar.gz and read the changelog file. website is here: [a link] 2. about our version, i believe we are just one version behind: [code block] 3.5 mentions my bug report, so i'm pretty sure i updated as soon as the fix was released. however 3.6 uses less memory and more speed, so may be worth it assuming it's stable.",-1,-1,0,0.6501421332359314,0.9739296436309814,0.7238547801971436,-1.0,accept,majority_agreement
67960461,2143,not anymore! i've updated this branch to include the new version. all my tests pass with no crashes or memory leaks. ymmv. :) :christmas_tree:,1,1,1,0.9846643805503844,0.9958385229110718,0.9966936111450196,1.0,accept,unanimous_agreement
68068376,2143,"thanks for the update of lzf, the only thing to check is if the previous version was actually not modified compared to the original sources, i totally don't recall but that's easy to check. about the more interesting problem of sdsnative(), actually i see what you are doing here (cit.). i think there is an even better solution that avoids the double-copy and reallocation, i'm sure you thought about it but avoided to implement it to don't put too much stuff into the same pr. so basically i'm working at a branch (originated from the quicklist branch) that refactors rdb.c in order to have functions that return plain zmalloc allocated objects instead of robjects. it was not a huge change at all and should improve loading times. at the same time i'm reviewing more code from quicklist. in the first days of january at max everything will get merged: in order to merge i've also to do my rdb changes otherwise i need to re-increment the rdb number again, or risk that users have rdb files with same version but different format. thanks again! :christmas_tree:",1,1,1,0.7743602991104126,0.8746801614761353,0.9592724442481996,1.0,accept,unanimous_agreement
68996956,2143,"hello matt, in order to merge i'm doing my changes to rdb here: [a link] i see that we diverged a bit, you mostly changed spacing and did a force update: trivial to rebase my changes upon yours, but please for future change add commits instead of force-updating so that it will be simpler to merge. thx!",1,1,1,0.6400917768478394,0.98312908411026,0.9940074682235718,1.0,accept,unanimous_agreement
69151560,2143,"merged! applying my commits from the other branch to master as well, and rdb v7 is done...",0,0,1,0.8924500346183777,0.6612339019775391,0.803004264831543,0.0,accept,majority_agreement
69353843,2143,"sorry for the confusion! thanks for getting this added (_and_ thanks for the new k/v rdb format)! that's the problem with distributed revision control (and github prs)... the code lives in the author's repo and can be updated at any time. if i _did_ only add new commits after this pr was first created, it would have 100 cleanup commits sitting here. :) the _real_ problem is git has a bad default interface. the **default** should be `git pull -r` to automatically fix any upstream changes. without `git pull -r`, git thinks everything changed on top of existing changes, and it can't deal with it. so, if `git pull` freaks out, run `git merge --abort` then `git pull -r`. or, worst case, `git checkout unstable; git branch -d [messed up branch]; git checkout [clean branch]` (assuming the branch is only the pr branch without any local changes on it). :shipit:",-1,-1,1,0.9174094200134276,0.8996875882148743,0.971356213092804,-1.0,accept,majority_agreement
163983411,2143,:+1: which version can i use this feature?,0,0,0,0.8673242330551147,0.9106714129447936,0.9205015301704408,0.0,accept,unanimous_agreement
163992222,2143,it's currently in the `testing` branch and will thus end up in the upcoming 3.2 release.,0,0,0,0.9891113638877868,0.9949775338172911,0.9935352802276612,0.0,accept,unanimous_agreement
1641921039,12416,"i rather you avoid amend and force-push, it's harder to do incremental review this way. the pr is gonna be squash-merged eventually anyway.",0,0,0,0.8972392082214355,0.9843489527702332,0.95079106092453,0.0,accept,unanimous_agreement
1764113224,12416,"sorry, was busy in other working. while going through the latest comments. i see you guys are prefer to have separated apis. personal i agree with it as from user perspective i'd like have explicit api name to help understanding the exact operation. i reverted to the earlier version i pushed to have separated apis. please let's initiate reviewing from this version.",-1,-1,-1,0.9827213883399964,0.9903690218925476,0.9658557772636414,-1.0,accept,unanimous_agreement
1820109032,12416,is the code mature enough to be candidate to redis 8.0?,0,0,0,0.9858156442642212,0.9947288632392884,0.994331955909729,0.0,accept,unanimous_agreement
1820397244,12416,"yes, i hope so. can you update the top comment to describe the api correctly? then we can check that everyone agrees about this api. later, when the pr is merged, the top comment will be used as the commit message.",0,0,1,0.93408465385437,0.8281244039535522,0.6614258885383606,0.0,accept,majority_agreement
1863728757,12416,sorry for the bother. let's follow the pr #12874 instead to follow company's contributing to open source policy. i shall close this pr.,-1,-1,-1,0.9903008937835692,0.9915722608566284,0.993274450302124,-1.0,accept,unanimous_agreement
925487387,9511,"hi , got some time to take a look at this?",0,0,0,0.9746391177177428,0.9386505484580994,0.9030730724334716,0.0,accept,unanimous_agreement
925546511,9511,already on my list.. will get to it when i'm done with other things.,0,0,0,0.9623079895973206,0.9504188895225524,0.8909516930580139,0.0,accept,unanimous_agreement
925552824,9511,"got it. on thu, sep 23, 2021 at 14:49 oran agra ***@***.***> wrote: -- best regards, andy pan",1,1,1,0.9623055458068848,0.8499670624732971,0.9897095561027528,1.0,accept,unanimous_agreement
932694327,9511,ping,0,0,0,0.9714881777763368,0.9308286309242249,0.4991936683654785,0.0,accept,unanimous_agreement
932871515,9511,"sorry, it's still on my list (haven't forgotten it), but i have lots of other things to handle first.",-1,-1,-1,0.9838401675224304,0.9920030832290648,0.9886110424995422,-1.0,accept,unanimous_agreement
1735009019,12611,seems that this fix is much better than #12601. but you still need to fix the other places with `defined(mac_os_x_version_10_6)`,0,0,0,0.9624075889587402,0.9916324019432068,0.9553476572036744,0.0,accept,unanimous_agreement
1735236279,12611,please also handle these ([a link],0,0,0,0.9864131212234496,0.9894515872001648,0.9954119324684144,0.0,accept,unanimous_agreement
1735265390,12611,there are three more in debug.c,0,0,0,0.9828234910964966,0.988752543926239,0.9945735335350036,0.0,accept,unanimous_agreement
1735296938,12611,please temporarily apply this commit([a link] to this and i will help you to run ci to verify it.,0,0,0,0.983336627483368,0.9614832997322084,0.994934618473053,0.0,accept,unanimous_agreement
1735313113,12611,ci only for macos with commit ([a link] [a link],0,0,0,0.98722106218338,0.9850183129310608,0.9953556060791016,0.0,accept,unanimous_agreement
1735721819,12611,it seems that the ci has failed. could you please confirm what the issue is?,0,0,0,0.883087694644928,0.92895245552063,0.976284921169281,0.0,accept,unanimous_agreement
1735724232,12611,"it doesn't relate to this pr, please ignore it.",-1,0,-1,0.8635168075561523,0.8974944949150085,0.9072266817092896,-1.0,accept,majority_agreement
1738767563,12611,please also fix `#if (defined __apple__ && defined(mac_os_x_version_10_7))` in `config.h`.,0,0,0,0.9881209135055542,0.9954956769943236,0.9958831071853638,0.0,accept,unanimous_agreement
1746240754,12611,"hi guys, when this pull request can be merged? i can not install redis-memory-server on mac os sonoma now.",0,0,0,0.9841681122779846,0.955124020576477,0.99132639169693,0.0,accept,unanimous_agreement
1751808369,12611,-binbin any news on the review?,0,0,0,0.9851090908050536,0.9858054518699646,0.9892818927764891,0.0,accept,unanimous_agreement
1751917224,12611,need this pr to be merged asap :(,-1,-1,-1,0.979781985282898,0.9865374565124512,0.9944103360176086,-1.0,accept,unanimous_agreement
1752520056,12611,please let us know when you release a new version. we are looking forward to it.,1,0,1,0.9373627305030824,0.9793134331703186,0.9925161004066468,1.0,accept,majority_agreement
1754602765,12611,"we didn't have any plans for an immediate release, is it blocking, feel free to provide context.",0,0,0,0.8273780941963196,0.9808650612831116,0.9770312905311584,0.0,accept,unanimous_agreement
1755744087,12611,"users of the [a link] npm package are having [a link] installing redis stable (7.2), which is the default. we are able to specify an old version as a workaround, but some users are unhappy with that. we are also considering applying the [a link] programmatically, but this is risky.",0,0,-1,0.5833304524421692,0.6148953437805176,0.5396423935890198,0.0,accept,majority_agreement
1757798470,12611,why does an old version (which?) work? shouldn't it suffer from the same problem? we plan on publishing a set of releases next week.,0,0,0,0.9538978338241576,0.9311622381210328,0.9727115631103516,0.0,accept,unanimous_agreement
1757883955,12611,"on my machine (os x 13.6), all of redis 6.0, 6.2, and 7.0 build successfully, whereas 7.2 fails with this error: [code block]",0,0,0,0.9731238484382628,0.9923527836799622,0.9935272336006165,0.0,accept,unanimous_agreement
1759064103,12611,i don't understand why 7.2 would be different than 7.0 in that respect. they both have the same code and conditions around fstat. do you understand that?,0,0,0,0.9137632250785828,0.8262032866477966,0.8247414827346802,0.0,accept,unanimous_agreement
1759076404,12611,"did you compile these versions(6.0, 6.2, 7.0) after upgrading to 13.6? they all compile with the same error on my local macos.",0,0,0,0.9889497756958008,0.9829578995704652,0.9924169778823853,0.0,accept,unanimous_agreement
1759629090,12611,"i see what's going on. it looks like we added `-werror=deprecated-declarations` in 7.2 ([a link]. on my machine, 7.0 and earlier would emit `'fstat64' is deprecated` as a warning, but the build would still complete; with 7.2, it became an error. so the issue i'm having is not the same as the original issue, but the fix is the same. i believe the original reporters may have had m1 macs (i don't); maybe that explains the difference in behavior. i know some users reported that the old version workaround worked, while others said it didn't, so maybe the difference is machine-specific.",0,0,0,0.8894450664520264,0.9582324028015136,0.9346399903297424,0.0,accept,unanimous_agreement
1760699670,12611,"thanks, you are right. test on i7 macos, these (5.0, 6.0, 7.0) are compiled successfully, but with warnings.",1,1,1,0.9290475249290466,0.9726486206054688,0.9802498817443848,1.0,accept,unanimous_agreement
1762066928,12611,"fyi, i have an m1 pro with macos 13.6 and i get the same error with `redis-memory-server`.",0,0,0,0.9830310344696044,0.8696860671043396,0.9857515692710876,0.0,accept,unanimous_agreement
1762855745,12611,"yes, we hope you will release an updated release as soon as possible so that we can use it in the `redis-memory-server` package",0,0,0,0.832790732383728,0.8961686491966248,0.9878165125846864,0.0,accept,unanimous_agreement
1768915921,12611,"fyi, i have an m1 with macos 14.0 and i get the same error with redis-memory-server.",0,0,0,0.9747587442398072,0.5208365321159363,0.9864485859870912,0.0,accept,unanimous_agreement
1770527821,12611,a new version of redis with this fixed was released yesterday. so i assume you're using an older version.,0,0,0,0.987409234046936,0.9895038604736328,0.9900751709938048,0.0,accept,unanimous_agreement
1771197884,12611,sorry. i forgot to remove the cache.,-1,-1,-1,0.9878793358802797,0.9922257661819458,0.9934899806976318,-1.0,accept,unanimous_agreement
1783826447,12611,do backport [a link] not call out these type of fixes? i was going to ask when it was going to be released in a 6.2.x release and assumed it wasn't until i looked at the comparison here: [a link],0,0,0,0.9829391241073608,0.9803722500801086,0.9924626350402832,0.0,accept,unanimous_agreement
1783831935,12611,"the reason i didn't list these in the 6.x and 7.0 release notes is that it was stated above that in these branches it doesn't fail the build, only in 7.2 it fails the build.",0,0,0,0.9648346304893494,0.9929530024528505,0.9874000549316406,0.0,accept,unanimous_agreement
1783862210,12611,i believe builds were failing for all unpatched versions on m1 macs.,-1,0,0,0.8836638927459717,0.901881456375122,0.979167103767395,0.0,accept,majority_agreement
1783887916,12611,agreed with that this was failing for us and others on m1/m2 macs on all versions.,0,0,0,0.84468013048172,0.9499945640563964,0.9417989253997804,0.0,accept,unanimous_agreement
1783888287,12611,ok. sorry.. so anyway i did include the fix in all of them.,-1,-1,-1,0.9891780614852904,0.9919231534004213,0.9927345514297484,-1.0,accept,unanimous_agreement
967733725,9774,this is a new pr that replaces old pr: [a link] there were some comments left on the old pr that are now addressed in this new pr.,0,0,0,0.9882624745368958,0.993728756904602,0.9945361614227296,0.0,accept,unanimous_agreement
980825077,9774,"/core-team please review, there are two core team decisions. the first is the config to limit maximum amount of outbound data, the second is the command to describe the information for the cluster links.",0,0,0,0.9840511679649352,0.9929057359695436,0.991293966770172,0.0,accept,unanimous_agreement
987680690,9774,"the decision was that we will set the default to 0, implying infinite today, and we'll evaluate how it works. we'll document that you can do something about it.",0,0,0,0.9764910340309144,0.9796615839004515,0.9935224652290344,0.0,accept,unanimous_agreement
987686811,9774,谢谢,0,0,0,0.9021308422088624,0.8466947674751282,0.9076678156852722,0.0,accept,unanimous_agreement
993061410,9774,accompanying doc pr: [a link] thanks.,1,1,1,0.665028989315033,0.9769663214683532,0.7845324277877808,1.0,accept,unanimous_agreement
993206538,9774,the top comment of the pr is outdated too.,0,0,0,0.7612549662590027,0.9034372568130492,0.908087968826294,0.0,accept,unanimous_agreement
995400056,9774,"sorry to ask more from you, but we moved to a new command definition system: [code block] you can transfer the contents of the doc pr over to this cr now. can you approve the major changes if you have time, so we can get this merged?",-1,-1,-1,0.9879212379455566,0.98853999376297,0.980455219745636,-1.0,accept,unanimous_agreement
996041072,9774,"fyi, i'd prefer to avoid rebase, amend, and force-push, it's harder to review what changed. since we're merging most prs with a squash-merge, it doesn't matter how many incremental commits they contain. in some cases we have prs that we plan to merge without squash (if they contain several topics, or some mass refactory in a separate commit), and in these cases, when we rebase, we must do a separate force-push (not mixed with any actual change), so we can let gh show us the diff that each force-push introduced. anyway, long story short, does the last push contain any actual changes? or just a rebase?",0,0,0,0.8536133766174316,0.9173357486724854,0.95302814245224,0.0,accept,unanimous_agreement
996053465,9774,this is just a rebase to handle the merge conflicts because there is the new command definitions.,0,0,0,0.9839081168174744,0.9919551014900208,0.9926787614822388,0.0,accept,unanimous_agreement
996057719,9774,"the new cluster links command will be implicitly supported in async-loading when this pr and #9878 are both individually applied, right? (no need to make any special changes in one for the benefit of the other) p.s. don't forget to make a redis-doc pr.",0,0,0,0.970675766468048,0.978766143321991,0.9943810105323792,0.0,accept,unanimous_agreement
996098789,9774,"my apologies. i'm clumsy with github prs. i simply did not know that `we're merging most prs with a squash-merge`. to answer your question, my last force-push contained: 1. rebase 2. added a new `cluster-links.json` file under `src/commands/` folder. 3. added a line to `commands.c` for `cluster links`, manually. later i realize that `commands.c` is supposed to be auto-generated. so my next revision will remove my manual change to `commands.c` and replace it with an auto-generated change.",-1,-1,-1,0.9853096604347228,0.9901106953620912,0.9545960426330566,-1.0,accept,unanimous_agreement
996102580,9774,"yes, running `cluster links` commands during async loading should be fine. i gave it the same flags that `cluster nodes` have. the accompanying redis-doc pr is here: [a link] i dont see a commit on the `redis-doc` repo that removes all existing json files. i dont want to start making `redis-doc/commands/*.json` and `redis/src/commands/*json` to diverge, so im stilll keeping the `cluster-links.json` file in my `redis-doc` pr.",0,0,0,0.9613831043243408,0.9577684998512268,0.9910287261009216,0.0,accept,unanimous_agreement
996122857,9774,"for now we didn't yet get rid of commands.json in redis-doc, so we need to be very careful and make sure we do the same changes in both places, so we we eventually delete it, no data is lost",0,0,0,0.9248592853546144,0.9833251237869264,0.978777289390564,0.0,accept,unanimous_agreement
996166354,9774,can you verify that commands.c and the json file were implemented correctly? they look right to me but i haven't looked that much. besides that i think this is ready to merge along with the doc pr.,0,0,0,0.9535203576087952,0.7563931941986084,0.9803202152252196,0.0,accept,unanimous_agreement
996808520,9774,`disconnect link when send buffer limit reached` test failed in my daily ci. [a link] [a link],0,0,0,0.957161545753479,0.9875630140304564,0.9911680221557616,0.0,accept,unanimous_agreement
1002470005,9774,can you please look into these failures. see also [a link] [code block],0,0,0,0.986650049686432,0.9847853183746338,0.9937084913253784,0.0,accept,unanimous_agreement
1002617971,9774,"that issue specifically is caused because the cluster tests do hard resets in between tests, and the nodes may not have re-established all links, especially for slower tests. i was looking into the other failures a bit last night, and don't have any other suggested fixes at the moment.",0,0,0,0.8856481909751892,0.95794016122818,0.982266366481781,0.0,accept,unanimous_agreement
1003667959,9774,"some failure in the freebsd ci in `disconnect link when send buffer limit reached` [a link] this may be an issue with the ci itself (it't not stable), but also, maybe there's some timing issue in the test.",0,0,0,0.9508987069129944,0.9888678789138794,0.9823306798934937,0.0,accept,unanimous_agreement
1003668059,9774,"i'll ping on our aws slack, i'm not sure he's noticed this.",-1,0,0,0.6051821112632751,0.8182570338249207,0.9758435487747192,0.0,accept,majority_agreement
1003668206,9774,a few days ago he told me he's on vacation 8-),1,1,1,0.949166178703308,0.9651385545730592,0.8085334300994873,1.0,accept,unanimous_agreement
1003668724,9774,"i probably should have known that :d. i was able to slowly validate it was this commit that is causing both the crashes on sanitizer and the buffer disconnect issues. i have no idea why though, it might just be we're dumping a bunch of memory into the test which doesn't seem to be done elsewhere in the cluster test.",1,0,1,0.6706029176712036,0.9061295986175536,0.9895500540733336,1.0,accept,majority_agreement
1004236189,9774,i'm back. i will sync up with maddy and investigate and fix what is going on. sorry and thanks.,-1,-1,-1,0.9839605689048768,0.9910672307014464,0.9567083120346068,-1.0,accept,unanimous_agreement
1008262857,9774,"any news? i must admit i didn't notice it fail lately, but got this one now: [a link] [code block] it could be just a random failure of the slow freebsd ci (we see broken pipes in random places). note that in the past we saw: [code block] as well as other types of errors, on **non**-freebsd ci. * a hung triggering a sigsegv kill by the test framework on a sanitizer run. * a the following failure on centos+tls: [code block]",0,0,0,0.8891274333000183,0.9510847330093384,0.7167442440986633,0.0,accept,unanimous_agreement
1012652019,9774,"hey sorry for the late response. but i have been trying to reproduce this test failure, but couldn't. this is the command i run (i copied it from [a link] [code block] i've run it numerous times locally. i've run it numerous times on a freebsd 64 bit ec2. all with the latest `unstable` branch. never able to reproduce the failure. from the error message, it looks like redis crashed during my test. the crash must be flaky and non-deterministic. any ideas that could help reproduce it? another question - how do i preserve redis logs from these test runs?",-1,-1,-1,0.9899232983589172,0.9906299710273744,0.9846040606498718,-1.0,accept,unanimous_agreement
1012679427,9774,"it's bound to happen every time on my local freebsd vm, i've only allocated 1 core and 256m ram to it, and since i don't know about clusters, there's no way to solve this problem directly, maybe i can package the logs and send them to you. i use the following to reproduce. [code block]",0,0,0,0.980360209941864,0.9532511234283448,0.9832062125205994,0.0,accept,unanimous_agreement
1012802321,9774,"when memory is exhausted, this test is bound to fail, and here are some logs after enabling `loglevel debug`. [code block] [code block]",0,0,0,0.9885309934616088,0.9917789101600648,0.9923977255821228,0.0,accept,unanimous_agreement
1012814027,9774,"huh, looks like file descriptors are exhausted? , you mentioned you reproduced this on a freebsd vm. could you please share what exactly is your setup? which virtualmachine? in the meanwhile, i will try to simulate your setup with a ec2 to reproduce. i.e. i will provision a ec2 with freebsd, 1 core and 256mb memory. also, if you could share the full redis logs for the failed test, that'd be great. (for example you could attach a file here, or use something like [a link] thanks a lot!",1,1,1,0.97944974899292,0.9915669560432434,0.993086874485016,1.0,accept,unanimous_agreement
1012834938,9774,"justpaste.it can't upload files, so i created a repository to hold the logs([a link] * system: `freebsd 12 64bit` * vm: vmware * ulimit [code block] * change loglevel [code block]",0,0,0,0.9819833040237428,0.9790849685668944,0.9936705827713012,0.0,accept,unanimous_agreement
1015076735,9774,"ok. i am able to reproduce the test failure with 1 core, 512mb ram 64bit freebsd. the failure is redis getting oom killed by kernel due to out of swap. in the test, i'm allowing cluster link buffers to grow up to 32mb. that proved to be too much for the freebsd test environment used by the daily runs. i'm currently experimenting with a new cluster link buffer limit in my test. the challenge is that in order to trigger the condition under test, which is a cluster bus link being freed once over the limit, i need to fill up the cluster link buffer. but to fill up the link buffer, i need to first fill up the tcp write buffer in the kernel level, which varies from os to os. i will post a fix pr as soon as i can get a good stable buffer limit.",0,0,0,0.8379815816879272,0.86419677734375,0.7958820462226868,0.0,accept,unanimous_agreement
1015092611,9774,"note that in the ""other"" test suite, we have the `--large-memory` which enables us to run certain tests only on systems with a lot of memory (currently only manually). also, what about the test failures in the sanitizer runs, like this one: [a link] not sure if this one is related: [a link]",0,0,0,0.9829208254814148,0.9915122389793396,0.9911726117134094,0.0,accept,unanimous_agreement
1015978344,9774,"this has a different root cause. in the test, i'm assuming as soon as i send a large publish command to fill up a cluster link, the link will be freed. but in reality the link will only get freed in the next `clustercron` run whenever that happens. my test is not accounting for this race condition. i will fix it too. this link has two failures. `test-sanitizer-address (clang) ` and `test-freebsd` `test-sanitizer-address (clang) ` is not related to my test. `test-freebsd` is related to my test and it should be the same oom root cause.",0,0,0,0.9113047122955322,0.7400888204574585,0.9461174011230468,0.0,accept,unanimous_agreement
1016128938,9774,"i didn't look at the test, but i wanna mention that there are other tests in which we struggled to fill output buffers and had to deal with the os socket buffers swallowing our bytes. i think these are in: tests/unit/client-eviction.tcl tests/unit/maxmemory.tcl tests/integration/replication-buffer.tcl iirc the best approach we had was to pause the destination process, and then gradually fill more and more data until we see it starts piling up, then stop.",0,0,0,0.976677656173706,0.9110330939292908,0.9425392746925354,0.0,accept,unanimous_agreement
1017951216,9774,"pr to fix flaky test ""disconnect link when send buffer limit reached"" - [a link] sorry for all the inconvenience. and thanks for the help and advice.",-1,-1,-1,0.4943675100803375,0.9868581295013428,0.9883204698562622,-1.0,accept,unanimous_agreement
1018239089,9774,"test failed : [a link] [code block] don't know about this test but might be related with timing. i increase ""cluster-node-timeout"" for something else on my branch, i see this test fails here. just fyi.",-1,-1,0,0.7871606945991516,0.7269847393035889,0.7990455627441406,-1.0,accept,majority_agreement
1018725555,9774,thanks for reporting the failure. sorry for the inconvenience. i updated my pr([a link] to fix this test as well.,-1,-1,-1,0.9898696541786194,0.9872136116027832,0.9879536628723145,-1.0,accept,unanimous_agreement
2413102986,13592,### ce performance automation : step 1 of 2 (build) done. this comment was automatically generated given a benchmark was triggered. started building at 2024-10-29 07:48:16.872643 and took 64 seconds. you can check each build/benchmark progress in grafana: - git hash: 832a7ad84f913fa0e0aadc91e6953f68f839f41e - git branch: moticless:info-keysizes - commit date and time: n/a - commit summary: n/a - test filters: - command priority lower limit: 0 - command priority upper limit: 10000 - test name regex: .* - command group regex: .* you can check a comparison in detail via the [a link],0,0,0,0.9246423840522766,0.970316469669342,0.6521506905555725,0.0,accept,unanimous_agreement
2413104933,13592,### ce performance automation : step 2 of 2 (benchmark) running... this comment was automatically generated given a benchmark was triggered. started benchmark suite at 2024-12-19 08:07:30.023779 and took 7866.556751 seconds up until now. status: [################################------------------------------------------------] 39.48% completed. in total will run 271 benchmarks. - 164 pending. - 107 completed: - 0 successful. - 107 failed. you can check a the status in detail via the [a link],0,0,0,0.8748902678489685,0.9599227905273438,0.9234582781791688,0.0,accept,unanimous_agreement
2413111949,13592,"### automated performance analysis summary this comment was automatically generated given there is performance data available. using platform named: intel64-ubuntu22.04-redis-clx1 to do the comparison. in summary: - detected a total of 130 stable tests between versions. - detected a total of 5 regressions bellow the regression water line 10.0. - median/common-case regression was -17.9% and ranged from [-84.2%,-13.6%]. you can check a comparison in detail via the [a link] ### comparison between unstable and moticless:info-keysizes. time period from 5 months ago. (environment used: oss-standalone) #### regressions table | test case |baseline redis/redis unstable (median obs. +- std.dev)|comparison redis/redis moticless:info-keysizes (median obs. +- std.dev)|% change (higher-better)| note | |-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------:|-----------------------------------------------------------------------|------------------------|----------| |[a link] | 19127| 3019 +- 0.2% (9 datapoints) |-84.2% |regression| |[a link] | 1225| 292 +- 0.3% (10 datapoints) |-76.2% |regression| |[a link]| 4292| 3644 +- 3.4% (9 datapoints) |-15.1% |regression| |[a link] | 4237| 3660 +- 3.1% (10 datapoints) |-13.6% |regression| |[a link] | 5676| 4660 +- 2.5% (9 datapoints) |-17.9% |regression| regressions test regexp names: memtier_benchmark-1key-100m-bits-bitmap-bitcount|memtier_benchmark-1key-1billion-bits-bitmap-bitcount|memtier_benchmark-1key-list-10k-elements-linsert-lrem-integer|memtier_benchmark-1key-list-10k-elements-lpos-integer|memtier_benchmark-1key-list-10k-elements-lpos-string full results table: | test case |baseline redis/redis unstable (median obs. +- std.dev)|comparison redis/redis moticless:info-keysizes (median obs. +- std.dev)|% change (higher-better)| note | |-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------:|-----------------------------------------------------------------------|------------------------|---------------------| |[a link] | 103487| 102072 +- 1.1% (8 datapoints) |-1.4% |no change | |[a link] | 10640| 10671 +- 0.4% (9 datapoints) |0.3% |no change | |[a link] | 28449| 28868 +- 1.2% (12 datapoints) |1.5% |no change | |[a link] | 27237| 25882 +- 1.9% (10 datapoints) |-5.0% |potential regression | |[a link] | 1913| 1926 +- 0.8% (12 datapoints) |0.7% |no change | |[a link] | 77195| 77508 +- 0.7% (10 datapoints) |0.4% |no change | |[a link] | 222824| 228231 +- 0.8% (8 datapoints) |2.4% |no change | |[a link] | 86269| 85517 +- 0.8% (9 datapoints) |-0.9% |no change | |[a link] | 302135| 298168 +- 0.9% (8 datapoints) |-1.3% |no change | |[a link] | 117600| 116512 +- 0.4% (8 datapoints) |-0.9% |no change | |[a link] | 118128| 117731 +- 0.5% (11 datapoints) |-0.3% |no change | |[a link] | 114750| 114716 +- 0.6% (10 datapoints) |-0.0% |no change | |[a link] | 109654| 110024 +- 0.5% (12 datapoints) |0.3% |no change | |[a link] | 671278| 669725 +- 1.1% (9 datapoints) |-0.2% |no change | |[a link] | 724807| 715804 +- 1.3% (10 datapoints) |-1.2% |no change | |[a link] | 661298| 655123 +- 0.3% (9 datapoints) |-0.9% |no change | |[a link] | 653748| 643110 +- 0.8% (10 datapoints) |-1.6% |no change | |[a link] | 658001| 647711 +- 0.6% (10 datapoints) |-1.6% |no change | |[a link] | 374937| 372021 +- 3.9% (12 datapoints) |-0.8% |no change | |[a link] | 737184| 715528 +- 2.0% (10 datapoints) |-2.9% |no change | |[a link] | 718197| 706440 +- 1.2% (12 datapoints) |-1.6% |no change | |[a link] | 109547| 108667 +- 0.5% (9 datapoints) |-0.8% |no change | |[a link] | 113532| 111379 +- 0.7% (11 datapoints) |-1.9% |no change | |[a link] | 107946| 106369 +- 0.8% (9 datapoints) |-1.5% |no change | |[a link] | 118455| 116899 +- 0.5% (8 datapoints) |-1.3% |no change | |[a link] | 540261| 530660 +- 0.7% (9 datapoints) |-1.8% |no change | |[a link] | 829095| 825754 +- 0.6% (10 datapoints) |-0.4% |no change | |[a link] | 112361| 112744 +- 0.7% (8 datapoints) |0.3% |no change | |[a link] | 113895| 112619 +- 2.1% (9 datapoints) |-1.1% |no change | |[a link] | 112523| 111720 +- 2.0% (10 datapoints) |-0.7% |no change | |[a link] | 65095| 64894 +- 0.5% (9 datapoints) |-0.3% |no change | |[a link] | 103882| 104842 +- 0.4% (8 datapoints) |0.9% |no change | |[a link] | 64967| 64969 +- 0.5% (12 datapoints) |0.0% |no change | |[a link] | 94760| 93700 +- 1.4% (10 datapoints) |-1.1% |no change | |[a link] | 102915| 100944 +- 1.8% (12 datapoints) |-1.9% |no change | |[a link] | 72172| 72297 +- 1.0% (8 datapoints) |0.2% |no change | |[a link] | 46316| 47190 +- 0.9% (10 datapoints) |1.9% |no change | |[a link] | 74627| 76891 +- 1.6% (10 datapoints) |3.0% |potential improvement| |[a link] | 82032| 82007 +- 0.6% (8 datapoints) |-0.0% |no change | |[a link] | 265326| 266894 +- 0.7% (9 datapoints) |0.6% |no change | |[a link] | 70693| 69650 +- 0.5% (8 datapoints) |-1.5% |no change | |[a link] | 160951| 160688 +- 0.4% (11 datapoints) |-0.2% |no change | |[a link] | 103830| 103227 +- 0.6% (10 datapoints) |-0.6% |no change | |[a link] | 488217| 490434 +- 1.0% (12 datapoints) |0.5% |no change | |[a link] | 108232| 106477 +- 2.2% (12 datapoints) |-1.6% |no change | |[a link] | 580359| 569525 +- 0.6% (9 datapoints) |-1.9% |no change | |[a link] | 98369| 97173 +- 0.5% (9 datapoints) |-1.2% |no change | |[a link] | 39932| 40157 +- 0.5% (12 datapoints) |0.6% |no change | |[a link] | 2596| 2615 +- 1.3% (12 datapoints) |0.7% |no change | |[a link] | 72054| 70642 +- 0.6% (10 datapoints) |-2.0% |no change | |[a link] | 77519| 75907 +- 0.6% (9 datapoints) |-2.1% |no change | |[a link] | 108760| 108026 +- 0.7% (9 datapoints) |-0.7% |no change | |[a link] | 619102| 603858 +- 0.6% (9 datapoints) |-2.5% |no change | |[a link] | 109236| 108831 +- 0.6% (9 datapoints) |-0.4% |no change | |[a link] | 117663| 116320 +- 1.1% (9 datapoints) |-1.1% |no change | |[a link] | 714333| 703603 +- 1.2% (9 datapoints) |-1.5% |no change | |[a link] | 117990| 116098 +- 0.3% (8 datapoints) |-1.6% |no change | |[a link] | 727751| 713079 +- 1.3% (10 datapoints) |-2.0% |no change | |[a link] | 117186| 115715 +- 0.4% (10 datapoints) |-1.3% |no change | |[a link] | 675072| 660299 +- 1.0% (10 datapoints) |-2.2% |no change | |[a link] | 112548| 111705 +- 0.6% (12 datapoints) |-0.7% |no change | |[a link] | 634572| 628918 +- 0.6% (9 datapoints) |-0.9% |no change | |[a link] | 104082| 96494 +- 0.5% (10 datapoints) |-7.3% |potential regression | |[a link] | 390760| 365249 +- 0.8% (8 datapoints) |-6.5% |potential regression | |[a link] | 77115| 75893 +- 0.4% (8 datapoints) |-1.6% |no change | |[a link] | 520259| 521054 +- 0.8% (8 datapoints) |0.2% |no change | |[a link] | 109608| 110081 +- 1.6% (10 datapoints) |0.4% |no change | |[a link] | 626186| 625137 +- 0.8% (10 datapoints) |-0.2% |no change | |[a link] | 19127| 3019 +- 0.2% (9 datapoints) |-84.2% |regression | |[a link] | 1225| 292 +- 0.3% (10 datapoints) |-76.2% |regression | |[a link] | 98266| 96576 +- 0.8% (9 datapoints) |-1.7% |no change | |[a link] | 65363| 64862 +- 0.9% (9 datapoints) |-0.8% |no change | |[a link] | 113698| 113332 +- 0.6% (9 datapoints) |-0.3% |no change | |[a link] | 716741| 707237 +- 1.4% (10 datapoints) |-1.3% |no change | |[a link] | 115730| 114425 +- 1.8% (12 datapoints) |-1.1% |no change | |[a link] | 752452| 735349 +- 1.3% (10 datapoints) |-2.3% |no change | |[a link] | 114621| 114417 +- 0.5% (8 datapoints) |-0.2% |no change | |[a link] | 745365| 734078 +- 0.9% (9 datapoints) |-1.5% |no change | |[a link] | 98723| 98560 +- 0.2% (9 datapoints) |-0.2% |no change | |[a link] | 97631| 97413 +- 0.5% (12 datapoints) |-0.2% |no change | |[a link] | 451254| 455171 +- 1.7% (11 datapoints) |0.9% |no change | |[a link] | 74076| 74155 +- 1.1% (10 datapoints) |0.1% |no change | |[a link] | 107144| 104590 +- 1.7% (10 datapoints) |-2.4% |no change | |[a link] | 505841| 486350 +- 1.0% (10 datapoints) |-3.9% |potential regression | |[a link] | 77780| 74513 +- 1.3% (12 datapoints) |-4.2% |potential regression | |[a link] | 131289| 122377 +- 1.9% (9 datapoints) |-6.8% |potential regression | |[a link] | 91531| 89939 +- 1.0% (12 datapoints) |-1.7% |no change | |[a link] | 77637| 77669 +- 1.2% (8 datapoints) |0.0% |no change | |[a link] | 4292| 3644 +- 3.4% (9 datapoints) |-15.1% |regression | |[a link] | 5267| 4934 +- 3.1% (10 datapoints) |-6.3% |potential regression | |[a link] | 4237| 3660 +- 3.1% (10 datapoints) |-13.6% |regression | |[a link] | 5676| 4660 +- 2.5% (9 datapoints) |-17.9% |regression | |[a link] | 13768| 13382 +- 2.6% (10 datapoints) |-2.8% |no change | |[a link] | 13360| 12885 +- 2.1% (11 datapoints) |-3.6% |potential regression | |[a link] | 4891| 5136 +- 1.7% (10 datapoints) |5.0% |potential improvement| |[a link] | 202629| 202655 +- 0.7% (9 datapoints) |0.0% |no change | |[a link] | 106148| 104905 +- 0.5% (9 datapoints) |-1.2% |no change | |[a link] | 513474| 502341 +- 0.7% (9 datapoints) |-2.2% |no change | |[a link] | 110541| 110760 +- 0.5% (12 datapoints) |0.2% |no change | |[a link] | 111039| 109246 +- 0.6% (11 datapoints) |-1.6% |no change | |[a link] | 107293| 104700 +- 1.9% (9 datapoints) |-2.4% |no change | |[a link] | 71411| 70971 +- 0.9% (10 datapoints) |-0.6% |no change | |[a link] | 107047| 100133 +- 0.6% (12 datapoints) |-6.5% |potential regression | |[a link] | 70418| 68937 +- 1.2% (12 datapoints) |-2.1% |no change | |[a link] | 109889| 110335 +- 0.3% (10 datapoints) |0.4% |no change | |[a link] | 13176| 12936 +- 1.0% (10 datapoints) |-1.8% |no change | |[a link] | 112344| 112139 +- 1.9% (10 datapoints) |-0.2% |no change | |[a link] | 112868| 112082 +- 0.6% (10 datapoints) |-0.7% |no change | |[a link] | 109917| 108590 +- 0.4% (10 datapoints) |-1.2% |no change | |[a link] | 27594| 27511 +- 1.0% (11 datapoints) |-0.3% |no change | |[a link] | 29341| 28917 +- 0.7% (10 datapoints) |-1.4% |no change | |[a link] | 28462| 28500 +- 0.6% (8 datapoints) |0.1% |no change | |[a link] | 64642| 65389 +- 0.7% (9 datapoints) |1.2% |no change | |[a link] | 29244| 28810 +- 0.6% (10 datapoints) |-1.5% |no change | |[a link] | 65493| 64841 +- 0.9% (10 datapoints) |-1.0% |no change | |[a link] | 81764| 81104 +- 0.5% (10 datapoints) |-0.8% |no change | |[a link] | 17921| 17569 +- 0.3% (10 datapoints) |-2.0% |no change | |[a link] | 17942| 17578 +- 0.1% (9 datapoints) |-2.0% |no change | |[a link]| 39330| 38568 +- 2.3% (10 datapoints) |-1.9% |no change | |[a link] | 51038| 52711 +- 1.7% (10 datapoints) |3.3% |potential improvement| |[a link] | 2636| 2629 +- 0.7% (10 datapoints) |-0.3% |no change | |[a link] | 752988| 735102 +- 0.7% (12 datapoints) |-2.4% |no change | |[a link] | 108128| 106792 +- 1.0% (9 datapoints) |-1.2% |no change | |[a link] | 644473| 636704 +- 1.0% (12 datapoints) |-1.2% |no change | |[a link] | 63751| 63906 +- 1.3% (9 datapoints) |0.2% |no change | |[a link] | 72878| 71972 +- 1.5% (10 datapoints) |-1.2% |no change | |[a link] | 20601| 20038 +- 1.0% (10 datapoints) |-2.7% |no change | |[a link] | 61584| 58869 +- 0.6% (8 datapoints) |-4.4% |potential regression | |[a link] | 25841| 25759 +- 0.8% (8 datapoints) |-0.3% |no change | |[a link] | 56887| 56701 +- 0.6% (9 datapoints) |-0.3% |no change | |[a link] | 102470| 103494 +- 0.6% (8 datapoints) |1.0% |no change | |[a link] | 2951| 2950 +- 1.1% (12 datapoints) |-0.0% |no change | |[a link] | 3467| 3385 +- 1.6% (9 datapoints) |-2.4% |no change | |[a link] | 108689| 108250 +- 1.3% (8 datapoints) |-0.4% |no change | |[a link] | 109803| 107841 +- 1.2% (9 datapoints) |-1.8% |no change |",0,0,0,0.9266310334205629,0.9899916648864746,0.950096070766449,0.0,accept,unanimous_agreement
2427131932,13592,", nice catch! actually there is no need to special care because **currently** sflush is degenerated that only if it covers all the slots it is applied.",1,1,1,0.9915931224822998,0.9929600358009338,0.9956218600273132,1.0,accept,unanimous_agreement
850821078,9003,"afair you started to investigate that to solve the `replica buffer don't induce eviction` test, but i see it fails in this pr. is that related or unrelated to this test?",0,0,0,0.9515885710716248,0.9807878732681274,0.9888647794723512,0.0,accept,unanimous_agreement
850822657,9003,"this pr solves the problem of ``replica buffer don't induce eviction`` test, but now the test fails because my modification causes the memory to cause the slave's querybuf to no longer be shrunk, and when the slave is killed, ``delta_no_repl`` does not count the memory added by the querybuf, i'm still testing.",0,0,0,0.9822759628295898,0.9856826663017272,0.99286288022995,0.0,accept,unanimous_agreement
850968312,9003,"let me see if i get the the full story here. by default when we read a query we use: [code block] proto_iobuf_len is 16k, and sdsmakeroomfor being greedy doubles that. so that, together with the (not so recent) change in ~~#7864~~ #7875. caused the sds size to be 48k (instead of 32k which it was before) so when it was combined with this check (proto_mbulk_big_arg is set to 32k, and using `>`) and avoid shrinking the default buffer size. [code block] so in fact this is a ""regression"" from ~~#7864~~ #7875.",0,0,0,0.9817267656326294,0.9830445051193236,0.9785618782043456,0.0,accept,unanimous_agreement
850969819,9003,it's introduced by #7875. sdsmakeroomfor will change sds size to be 40k([a link],0,0,0,0.9850055575370787,0.9912185072898864,0.9956578016281128,0.0,accept,unanimous_agreement
850973668,9003,"ohh, yeah, that's the pr i meant (took the wrong one since they had a similar title)",0,0,0,0.5707310438156128,0.9407495260238647,0.958436131477356,0.0,accept,unanimous_agreement
853744156,9003,"i take it back, this is not a regression from #7875. before 7875, we used to ask for 16k, and the greenness of sdsmakeroomfor gave us 32k, but since servercron uses sdsallocsize rather than sdsalloc (what #5013 wants to solve), it would readk 32k+6, so the fact the code there uses `>` and not `>=` doesn't help. so even before 7875 we would have shrieked the query buff right after it was allocated.",0,0,0,0.966145634651184,0.9557924270629884,0.9794070720672609,0.0,accept,unanimous_agreement
853751174,9003,):8 it's been around for 9 years.,0,0,0,0.6441359519958496,0.9774845242500304,0.9859424233436584,0.0,accept,unanimous_agreement
860165557,9003,-steinberg let me know if we're good to merge this one.,0,0,0,0.9654641151428224,0.9720314145088196,0.9534541964530944,0.0,accept,unanimous_agreement
861096289,9003,"sorry, i can't be in front of the pc these days. i make the following changes.",-1,-1,-1,0.9847729206085204,0.9919278621673584,0.9891553521156312,-1.0,accept,unanimous_agreement
861389835,9003,yes.,0,0,0,0.969875693321228,0.98186594247818,0.9851860404014589,0.0,accept,unanimous_agreement
1521198333,11907,"i have no objections to the code, however i find api design a bit convoluted. can we get rid of `global|shard|pattern channel` part and use something like: [code block] the only place where you are using `type` field is when determining dictionary to search, can't we search all of them and include type of the subscription into response?",0,0,0,0.8589239716529846,0.7529582977294922,0.849212110042572,0.0,accept,unanimous_agreement
1522172603,11907,filtering on the server side would be beneficial for the clients as it would lead to smaller payload (less network i/o) as well as less work on the client to determine the type of subscription. the client could always run multiple api call to get the output which you're suggesting. [code block],0,0,0,0.9782372117042542,0.9849376082420348,0.9920998215675354,0.0,accept,unanimous_agreement
1535336485,11907,i think i agree with this. we could just have three separate fields in the response. we don't care that much about network since this is a debugging command.,-1,0,0,0.784654974937439,0.9337536096572876,0.869663417339325,0.0,accept,majority_agreement
1535566747,11907,"from our internal discussion: 1. have pagination (cursor/count) to avoid returning all of the response at once. there is still a case of too many clients connected to a single channel/pattern. it's a nested response, so won't be able to add pagination on it. 2. exact matching would be difficult to figure out if the application subscription is not already determined. have glob based matching pattern as an optional parameter. new structure (all parameters are optional): [code block] default parameter value: 1. type: `channel` 2. match: `*` 3. count: `10` 4. cursor: `0` response: [code block]",0,0,0,0.9784936904907228,0.9914205074310304,0.984032928943634,0.0,accept,unanimous_agreement
1535567494,11907,please have a look at the new update. responding with all three subscription type together would cause confusion for cursor/count. let me know what you think of the above solution.,0,0,0,0.9835140705108644,0.9519252181053162,0.991222620010376,0.0,accept,unanimous_agreement
1542555382,11907,could you give it another look ?,0,0,0,0.9848490953445436,0.9791907668113708,0.9884361028671264,0.0,accept,unanimous_agreement
1542563200,11907,reply-schemas-linter is failing with no error message. ran it locally don't see any issue. could you please take a look?,0,0,0,0.982671082019806,0.7597295045852661,0.947062849998474,0.0,accept,unanimous_agreement
1557013851,11907,"i'm not certain the count feature is very useful (considering it doesn't allow gradual iteration, just a sneak peek)",0,-1,-1,0.9409064054489136,0.5667961239814758,0.660067081451416,-1.0,accept,majority_agreement
1557038712,11907,"some users in alibaba cloud have the problem too, this feature is very useful.",1,1,1,0.5242714285850525,0.8703016638755798,0.9706825017929076,1.0,accept,unanimous_agreement
1558013330,11907,"we had some internal conversation about this. the main goal was to prevent accidentally dumping like 10 million records for highly used clusters. adding a cursor is complex and once you have a rough count, you can follow up on individual channels, which are more important for high use cases which is what we were looking for. the api we designed specifically solves the case we saw in aws, but if there are other cases we could try to handle them.",0,0,0,0.9614547491073608,0.9363887906074524,0.9763367176055908,0.0,accept,unanimous_agreement
1560247548,11907,"discussed in core team, yossi will give a second review.",0,0,0,0.9700148701667786,0.9766966104507446,0.993108868598938,0.0,accept,unanimous_agreement
1572406656,11907,"hello , would you be able to take a look ?",0,0,0,0.9835013747215272,0.7707049250602722,0.9851943850517272,0.0,accept,unanimous_agreement
1577194007,11907,"hi , thanks for taking a look. i did think about this from `client list` perspective. here are the few things due to which i made it as part of `pubsub`. * through the `client list` we could expose the pubsub channel(s) which an individual client has subscribed to. further an application would need to collate the result(s) to understand the activity of a given channel. with the above proposed api, it would be rather straight forward to gather all the subscribers for a given channel. * `client list` is non resp friendly and didn't want to bring in more complexity to it by adding another level of nested elements int it. * `pubsub subscribers` currently only exposes three information about a client. but if the need arises we could always extend it with more information. it's a two way door. (similar to `slowlog get`).",1,1,1,0.8482799530029297,0.9273668527603148,0.9753093123435974,1.0,accept,unanimous_agreement
1589616666,11907,"we want to consider a new variant of `client list` that provides better introspection, see [a link]",0,0,0,0.9796225428581238,0.9880842566490172,0.9945899248123168,0.0,accept,unanimous_agreement
1594990879,11907,"i also want to follow up on a quick conversation we had during the meeting. basically the client list filter is still not sufficient for deep introspection, since it doesn't help find the channels with a large number of connected clients. do you have thoughts about simplifying this command so that is just prints out the list of channels with the number of subscribes to each?",0,0,0,0.9666464328765868,0.95333993434906,0.9770809412002563,0.0,accept,unanimous_agreement
1596085563,11907,"i think counting clients per *something* is a generic troubleshooting use case and not specific to channels. so, it could make sense to handle it as a `client` command, which would resemble something like `client count [by-channel | by-user | by-addr]`. is that generic and common enough to be handled server-side? not sure.",0,0,0,0.9813716411590576,0.9899882674217224,0.972291111946106,0.0,accept,unanimous_agreement
1597799611,11907,"that assumes you know *what* channel you are looking for, which was not the use case mentioned here. the solution you are suggesting be asking for would like a join, since you want to find the top channels, then find the clients that are listening to them top channels.",0,0,0,0.9843220114707948,0.9806441068649292,0.9911096096038818,0.0,accept,unanimous_agreement
1598320308,11907,"not necessarily, i was pointing out that we could extend `client` to handle grouping and aggregation on the server side, and theoretically apply that to more than the channels use case.",0,0,0,0.9855776429176332,0.9913991689682008,0.9846678376197816,0.0,accept,unanimous_agreement
1598854199,11907,that sounds so much more complex. i suppose in either case let's walk through the implementation and see if we can implement it so that it is simple. which i started documenting here: [a link],0,0,0,0.7922927141189575,0.9455918073654176,0.8853226900100708,0.0,accept,unanimous_agreement
1631183063,11907,the current inclination of the /core-team is to have a singular command i.e. `client` command to provide tooling for admin to debug. marking this as closed. further updates will be discussed in #12311,0,0,0,0.9861255288124084,0.9945330619812012,0.9923046231269836,0.0,accept,unanimous_agreement
1820687187,12742,"does the last force-push contains only rebase, or any other changes?",0,0,0,0.9856938123703004,0.992953360080719,0.9931334853172302,0.0,accept,unanimous_agreement
1821279923,12742,full cluster tests: [a link],0,0,0,0.9868309497833252,0.9309651851654052,0.9946660995483398,0.0,accept,unanimous_agreement
1822141663,12742,full ci: [a link],0,0,0,0.9845400452613832,0.8597871661186218,0.9959890246391296,0.0,accept,unanimous_agreement
1822354103,12742,"i think we should make an effort to protect clusterstate and clusternode etc from being used outside cluter_legacy.c, so that people won't try to access them from other c files. e.g. see [a link] quick ""fix"" for [a link] rebase issue. the two options i see are: 1. fold cluster_legacy.h into the top of cluster_legacy.c 2. add some `#error` in cluster_legacy.h when it's included from any other file. the main difference between the two, is that if we'll ever want to split cluster_legacy.c into several files, or for some reason access it's internals from another file, we need to take the second solution. wdyt?",0,0,0,0.95397287607193,0.9917972087860109,0.9645105600357056,0.0,accept,unanimous_agreement
1839818033,12742,"i think your option (1), making them opaque by moving the type definition into cluster_legacy.c, is a good idea. i've never cared about how large a file is as long as it's logically coherent.",1,0,1,0.7826055288314819,0.616969645023346,0.9551066160202026,1.0,accept,majority_agreement
1840185250,12742,can you make a pr for that? (move cluster_legacy.h into the c file),0,0,0,0.9885469675064088,0.994060218334198,0.9958040118217468,0.0,accept,unanimous_agreement
1123682772,10636,/core-team please approve. see top comment.,0,0,0,0.9787948727607728,0.9452040195465088,0.9872777462005616,0.0,accept,unanimous_agreement
1127256092,10636,approved in a core-team meeting.,0,0,0,0.9693827629089355,0.9929214119911194,0.9917929768562316,0.0,accept,unanimous_agreement
1183629564,10969,"didn't we agree to fix spop in a breaking manner? or did we say that we'll first fix it in a non-breaking manner, and then break it in a later version?",0,0,0,0.9500433206558228,0.9892590641975404,0.9880039691925048,0.0,accept,unanimous_agreement
1183638108,10969,"we said that he will first fix it without breaking so we can cherrypick this to 7.0 if we want to. we will fix it proparly after, in anothet rp.",0,0,0,0.9848328232765198,0.9853535294532776,0.9934640526771544,0.0,accept,unanimous_agreement
1184644132,10969,"ohh, i forgot. better outline that plan in the top comment. do you think we can put this in 7.0?",0,0,0,0.8151862621307373,0.8831315636634827,0.9055795669555664,0.0,accept,unanimous_agreement
1198156372,10969,"please take another look. we attempted to include only the bugfix in this pr without fixing other things that can be considered a breaking change. along the way a few other improvements were made and some mechanism was introduced. we need to decide if it's safe to backport to 7.0, maybe after a few weeks in unstable. maybe try to make a review sweep that ignores all the comments and tests, and just consider the actual changes that have a risk of breaking things...",0,0,0,0.9237107634544371,0.9649664759635924,0.8459188342094421,0.0,accept,unanimous_agreement
1211806014,10969,"/core-team not really a major-decision since there's no interface change, but it's a sensitive area so maybe one of you wants to review it too. besides merging it to unstable, i'd also like to consider backporting it to 7.0, so that's probably more of a major-decision, so if you have any opinion, please comment. either way, we better merge it to unstable to let it age a bit there, so i'd like to merge it soon.",0,0,0,0.9043607711791992,0.9538546800613404,0.8656631112098694,0.0,accept,unanimous_agreement
1214391528,10969,"the pr **doesn't** include a breaking change (just a bug fix). unless someone wrote code that actually relies on the bug (but that can be true for any bug fix). it is however sensitive area, so maybe it introduces a new bug.",0,0,0,0.9348913431167604,0.97201806306839,0.9833247661590576,0.0,accept,unanimous_agreement
1223771006,10969,"notice that we have an occasional test failure because of the changes on this pr: [a link] the issue is that if there is a lazy expire during the command invocation, the `del` command is added to the replication stream after the command placeholder. so the logical order on the primary is: * delete the key (lazy expiration) * command invocation but the replication stream gets it the other way around: * command invocation (because the command is written into the placeholder) * delete the key (lazy expiration) so if the command write to the key that was just lazy expired we will get inconsistency between primary and replica. the following test can constantly reproduce the issue: [code block] one solution we considered is to add another lazy expire replication stream and write all the lazy expire there. then when replicating, we will replicate the lazy expire replication stream first. this will solve this specific test failure but we realise that the issues does not ends here and the more we dig the more problems we find. one of the example we thought about (that can actually crashes redis) is as follow: 1. user perform `sinterstore` 2. when redis tries to fetch the second input key it triggers lazy expire 3. the lazy expire trigger a module logic that deletes the first input key 4. now redis hold the `robj` of the first input key that was actually freed we believe we took the wrong approach here and we would like to suggested a new approach: 1. **modules should not perform any writes during key space notifications**, we should document it and make it clear to any module writer. 2. module might still want to perform writes as a result of a key space notification, and the requirement is that it will be atomic along side the command that triggered the notification, for this we will introduce a new post command hook that will allow the module to run whatever logic it wants (including writes) **after** the command was finished. the logical order and the replication order will be the same because any writes logic was invoke **after** the command was finished and there is not risk in changing the key space while the command is running. a module will be able to register to key space notifications, collect them, and only react on them on the post command hook. we are planing to revert this pr and implements this post command hook idea on another pr.",0,0,0,0.9769778251647948,0.9819050431251526,0.9874277114868164,0.0,accept,unanimous_agreement
1131214187,10747,why would an application execute these commands on a non existing group? sounds like a bug in the application,-1,0,0,0.7155816555023193,0.931207239627838,0.9672451615333556,0.0,accept,majority_agreement
1132719081,10747,"i'm not familiar with redis guidelines on automatic creation of resources, but at least in streams we can see that things are created automatically. for example, you can use [a link] to add a message to a stream even if said stream doesn't exist. is that a bug in the application? of course not. so a stream is automatically created on some of the stream related commands. a consumer is also created automatically on [a link]. i'd expect the same behavior to apply to consumer groups. this allows the application to avoid having to write recovery code to deal with a cluster reset (i.e. recreate all the consumer group), and to not worry about creating the groups on startup if they don't exist.",0,0,0,0.9404011964797974,0.97783100605011,0.9379600286483764,0.0,accept,unanimous_agreement
1133837078,10747,"the fact that xadd creates a key is consistent with the rest of redis (same as lpush, hset, etc). looking at all the stream commands, it seems that consumers are created implicitly and groups must be created explicitly. i'm not sure i understand why maybe or do. in any case, maybe if we want to change that, in a backwards compatible way, we need to add an mkgoup argument (like the mkstreadm arg we have in xgroup create",0,0,0,0.966247260570526,0.9696525931358336,0.9723365306854248,0.0,accept,unanimous_agreement
1136253763,10747,"-rosenfeld i agree with what oran suggests, we could add an mkgoup argument for the xreadgroup. of course, we could add another standlone command to check if the group exists. how do you think? thanks",1,1,1,0.925402820110321,0.9377844929695128,0.8138831853866577,1.0,accept,unanimous_agreement
1136889256,10747,"imho it makes sense for `xadd` and `xgroup create` to have the ability to create a stream because in their nature they are write-commands. it feels a bit weird for a read-command such as `xpending` or `xreadgroup` to create a group (also, where does it stop? why not `xclaim`, xautoclaim`, etc.?)",-1,-1,-1,0.9108980894088744,0.9104841947555542,0.9798901081085204,-1.0,accept,unanimous_agreement
1136891933,10747,"having said that, if the fact that some stream ""read"" commands don't create consumer groups makes it difficult for users we can go ahead with the optional mkstream, but let's do it for all groups-related commands and in the future, we will make sure to always create the group if needed (i.e. get redis will not reply with ""no such group"" in all future commands)",0,0,0,0.9851551651954652,0.9932911396026612,0.9923617839813232,0.0,accept,unanimous_agreement
1136933872,10747,"mkgroup sounds good to me. creating a group on read might look weird, but a consumer group is essentially a read entity by nature. so it's only natural that the groups will be created automatically on read commands.",1,1,0,0.9596333503723145,0.8478012681007385,0.7181909680366516,1.0,accept,majority_agreement
1138387479,10747,ok. let's do that for 7.2,0,0,0,0.9706352949142456,0.9798429608345032,0.9816619753837584,0.0,accept,unanimous_agreement
1138590740,10747,"i will update this pr for adding an extra optional parameter mkgroup for xpengding and xreadgroup commands, thanks",1,1,1,0.9344229102134703,0.6086843609809875,0.8762749433517456,1.0,accept,unanimous_agreement
1142254567,10747,"-rosenfeld i already finsih the code part for the new command option, please take a look and later i will add test cases for them, thanks",1,1,1,0.9796083569526672,0.7720705270767212,0.9612066149711608,1.0,accept,unanimous_agreement
1145812662,10747,i'm not familiar with redis code but it looks good to me. do you think the same should happen with xclaim and xautoclaim?,1,1,1,0.930359423160553,0.8474175930023193,0.7136037945747375,1.0,accept,unanimous_agreement
1147521528,10747,"-rosenfeld when i read the doc of command xautoclaim , i notice it says that conceptually, xautoclaim is equivalent to calling [xpending] and then [xclaim]. personally, i think it makes sense adding similar function which create consumer group if it doesn't exists. beside this, i have another idea that: add a new command: xexist key group -- which indicates if there is one consumer group for one specific key, return 1 if it exists and return 0 if not. thus, it provides client side has another option to run the command: xpending, xreadgroup, xclaim, xautoclaim without adding the mkgroup option. thanks how do you think?",0,0,0,0.7276674509048462,0.9068361520767212,0.8731584548950195,0.0,accept,unanimous_agreement
1148484653,10747,the problem with commands like xexist is that they're begging you to implement an unsafe operation in your code (xexist and then xgroup create). such a command will be useless without a distributed lock which not everyone are using. i agree that both xclaim and xautoclaim should create a group using the same mkgroup flag.,0,0,0,0.9117480516433716,0.956800937652588,0.6932000517845154,0.0,accept,unanimous_agreement
1148685636,10747,"i got your point, i will implement this parameter in xclaim and xautoclaim command, thanks for your explanation!",1,1,1,0.9825878739356996,0.9034140706062316,0.977342963218689,1.0,accept,unanimous_agreement
1151512225,10747,"all codes and test cases are finished, please take a look thanks a lot.",1,1,1,0.9548509120941162,0.8825172781944275,0.9746088981628418,1.0,accept,unanimous_agreement
1153085601,10747,thanks. we'll have to revisit that when approaching 7.2 (can't make that change in 7.0),1,1,1,0.909759759902954,0.6637287139892578,0.8914756178855896,1.0,accept,unanimous_agreement
1153936143,10747,"no problem, thanks oran",1,1,1,0.946129560470581,0.9500945210456848,0.9515478610992432,1.0,accept,unanimous_agreement
1284481726,10747,"code is updated, please check them, thanks",1,0,1,0.9580479264259338,0.53892582654953,0.9405099749565125,1.0,accept,majority_agreement
1397129368,10747,"hi oran, could you please take sometime to check my latest update? thanks a lot",1,1,1,0.9775880575180054,0.9821307063102722,0.9883648157119752,1.0,accept,unanimous_agreement
1596653282,10747,"please avoid rebase and force-push, it makes it hard to know what changed and do incremental reviews. please let me know which commits represent new content since my last review, and please go over the unresolved comments and either respond or resolve them.",0,0,0,0.6221215724945068,0.946299135684967,0.9665102362632751,0.0,accept,unanimous_agreement
1839175717,10747,"we need this for testing our app. our unit tests rely on flushed database and without a mkgroup, this causes them all to fail. as it is now, we have to flush and then issue five create groups. it would be good for production as well as it would make initialization more fault-tolerant. it is very odd indeed that streams are the only data type that require you to create a group before issuing the read commands -- sets, zsets, lists, and hashes don't require anything like this.",-1,0,-1,0.605413556098938,0.9054286479949952,0.854423463344574,-1.0,accept,majority_agreement
1841603850,10747,"hi oran, i have updated the code and also addressed the comments and replied. and i have even updated json file for 8.0. here are the commit details which had covered the comments. [a link] [a link] [a link]",0,0,0,0.9381961822509766,0.6946994662284851,0.8971425890922546,0.0,accept,unanimous_agreement
850952157,8999,"i really don't like the (ab)use of server event here, this is not an event (like client disconnecting or role change, etc). i certainly agree that we need to solve this though, and provide the information to the module in some way. i think it's relatively easy to add add that to aof and rdb saving / loading etc, by following the footsteps of `rm_getkeynamefromio`, and also for some other cases by following the footsteps of `rm_getkeynamefrommodulekey`. i suppose that for `digest` and `defrag` it may not be critical, but since they both take some kind of a context, we can create a similar mechanism. sadly, i don't currently have an idea of how to solve it for the `free`, `unlink` and `free_effort`, `copy`, `mem_usage` etc. pity we didn't think of that when we recently created `unlink`, `free_effort`, and `copy` (when we had the idea / concern of adding the key name in them we should have added dbid too). btw, ironically, iirc when we created the `unlink` you argued against adding the keyname. 8-) /core-team please respond if you have any idea how to better resolve this concern, or state what's your preference.",-1,1,-1,0.961396098136902,0.5911319255828857,0.9522823691368104,-1.0,accept,majority_agreement
851084409,8999,"yes, i very much agree. i also think the current implementation is ugly. adding an api similar to redismodule_getkeynamefromio was also what i first thought of. in fact, i first modified it in this way, on another branch, but because i couldn’t solve` free, unlink and free_effort, copy, mem_usage` etc., so i give up that plan. i also express a pity that the `unlink` did not directly bring in dbid or use redismodulekey. there are many usage scenarios for modules, but our module api was designed without too much consideration for compatibility and scalability.in other words, it is really difficult to design the apis correctly at the beginning. since these apis such as `free, unlink, free_effort, copy, mem_usage` do not have specific ctx parameters, apart from using event or we separately expose an api such as `getcurrentunlinkctx`, i can't think of other solutions at present. this is really a thorny issue, and it has bothered me for a long time.",-1,-1,-1,0.9838751554489136,0.9876766204833984,0.9461923241615297,-1.0,accept,unanimous_agreement
851196814,8999,"maybe one brave alternative would be to completely deprecate the existing `rm_createdatatype` in favor of a new api with a better set of callbacks. or maybe slightly better idea is to add new overlapping callbacks to the exiting api. i.e. in addition to the old `free` callback that takes only a pointer, one would be able to register a `free2` callback that takes an additional key name and dbid. a module in that case would be able to create a v4 struct, with both `free` and `free2` pointers, so that old redis versions would use the `free` pointer, and new ones will use `free2`. and please also share your thoughts / preference and if you happen to have a better idea.",0,0,0,0.9326703548431396,0.9924806356430054,0.9738064408302308,0.0,accept,unanimous_agreement
851279236,8999,"i think that the free2 idea is good but i would not lock the callback signature just to key name and dbid. instead, i would give an opaque object and give an api to extract information from this opaque object such as dbid or key name. this way, if in the future, we will want to add more information to those callbacks we will not have o break the api, wdyt?",0,0,1,0.7642256617546082,0.92071795463562,0.7572982907295227,0.0,accept,majority_agreement
852057965,8999,"i agree that it's about time to solve the missing-info-in-callbacks and having a new structure to hold the new callbacks is probably the way to go. we need to dedicate some thought to how we want the callbacks signatures to look like. generally speaking we can always provide some opaque structure + functions to extract information from it. in that case this struct could be the only argument for all callbacks. another option is to have multiple args to each callback, but all of them should be opaque. e.g. `free` will take redismodulectx* and redismodulekey*. i tend to prefer the first approach because it's more flexible. on the other hand it's a bit less intuitive (the user has to be familiar with all data extraction functions + it's uncertain if each callback can support all of them) and it's more work.",0,0,0,0.9540510177612304,0.9610210061073304,0.9085615277290344,0.0,accept,unanimous_agreement
853454307,8999,"thank you for your review and very good suggestions. the following are the prototypes of all current callbacks: [code block] among them, the callbacks without the opaque structure parameter are: [code block] i want to add an opaque structure parameter named redismodulexxxctx to these callbacks, such as the following: [code block] i wonder if this is enough?",1,1,1,0.975385844707489,0.9872854351997375,0.9883928894996644,1.0,accept,unanimous_agreement
853579307,8999,"i think that probably `free`, `freeeffort`, and `unlink` can share the same type of context (they're all different stages of the same operation). and maybe we can even go further and say that all of these (5 callbacks) can share the same type since they're all operations on a key. i.e. something like `redismodulekeyopctx`, then one can wonder why we don't have that type as argument for the other callbacks (like defrag, and save), in theory we can, but since we don't have to, we may prefer not to change these.",0,0,0,0.9703137874603271,0.9865534901618958,0.9814473390579224,0.0,accept,unanimous_agreement
853876358,8999,"well, let me spend some time to modify this pr according to this plan.",0,0,0,0.9799228310585022,0.9784391522407532,0.9924307465553284,0.0,accept,unanimous_agreement
854439605,8999,"please review this pr again, thanks :). [code block] i did not refactor the `free` callback because it is too special. first of all, because `free` may be called in bio thread, we cannot get the **key** and **dbid** information in this context. secondly, all the meta-information that free wants is available in `unlink2` now.",1,1,1,0.983845829963684,0.9835936427116394,0.9915831685066224,1.0,accept,unanimous_agreement
855357859,8999,"i agree, `free` should remain as is. that's exactly why we added `unlink`, since that's the actual / logical point in time were the value is detached from redis. `free` should only release memory.",0,0,0,0.9872876405715942,0.9892845749855042,0.9797119498252868,0.0,accept,unanimous_agreement
856405519,8999,"please review my changes again, i have basically solved the problems you mentioned above. the major change is to rewrite the test module. in order to fully reflect and use these enhanced calllbacks, i constructed a very simple memory allocator. after all, if you want to construct db-related and key-related resources, the memory is one very typical example.",0,0,0,0.950313150882721,0.8895891904830933,0.9511734843254088,0.0,accept,unanimous_agreement
859974014,8999,resolve conflicts with the unsatble branch,0,0,0,0.94164377450943,0.9909802079200744,0.9910401701927184,0.0,accept,unanimous_agreement
862956397,8999,"we have failures in your test modules in daily ci, can you please look into it? specifically in: libc malloc, 32 bit, alpine linux, and macos. i.e. anything that's no jemalloc + 32bit. [a link] (the errors in the valgrind run are unrelated, but they aborted the test before your new test got to run, so please try to run it with valgrind, so we don't need to wait for next daily to see)",0,0,0,0.9719550609588624,0.989337921142578,0.99001544713974,0.0,accept,unanimous_agreement
863426051,8999,should be fixed by #9102,0,0,0,0.986119270324707,0.9881672263145448,0.9938188195228576,0.0,accept,unanimous_agreement
887242778,8999,"we have another issue with a test that's introduced by this pr. `datatype2: test rdb save and load` hang with valgrind. [a link] i suppose this is some very rare timing issue. usually these tests take 5 minutes with valgrind (10 seconds without), but in this run it was killed after some 25 minutes. can you please look into it if you have time?",0,0,0,0.9563213586807252,0.7074722647666931,0.95795339345932,0.0,accept,unanimous_agreement
1001140171,8999,"[code block] this has failed several times recently (i am not able to create a review comment..., so i comment in here) ![a link]",-1,0,-1,0.8486784100532532,0.7775997519493103,0.5553303956985474,-1.0,accept,majority_agreement
1001162852,8999,"yes, i've seen that test fail 3 times in the past week. i wonder what changed. can someone look into it and debug that failure?",-1,0,0,0.8302408456802368,0.5475257635116577,0.7613422870635986,0.0,accept,majority_agreement
1001169724,8999,"i reverted to its first commit and it failed just as often, but it's been occurring particularly frequently lately. when add `save ""` config, it no longer failed. the main reason is that when creating k2 sds, zmalloc_size is normally 24, but sometimes it can be 40. not sure if this is in the form of memory fragmentation.",0,0,0,0.8833823800086975,0.9399885535240172,0.9601160883903505,0.0,accept,unanimous_agreement
1001177837,8999,"since we don't count external fragmentation, and since the internal one should be completely predictable and consistent, i don't think that's it.. we probably need deeper debugging to figure it out.",0,0,0,0.8606717586517334,0.948719561100006,0.9657317996025084,0.0,accept,unanimous_agreement
1001304953,8999,"i don't mean that zmalloc_size contains fragmentation, but that malloc allocates more memory than expected when there is a lot of internal memory fragmentation. btw, this test only occurs when malloc in libc.",0,0,0,0.9850789308547974,0.9799456596374512,0.9886261820793152,0.0,accept,unanimous_agreement
1001358250,8999,"i think this error is very strange, it is impossible from a code point of view. `expected '12384' to be equal to '12400' ` means that the memory occupied by the two keys differs by 16 bytes (they should actually be equal). [code block] the following calculation logic is impossible to have a difference of 16 bytes. [code block]",-1,0,-1,0.6128455400466919,0.5388686656951904,0.9523209929466248,-1.0,accept,majority_agreement
1001359802,8999,"the 16-byte difference is not in the memblock, but in the two keys k1 k2. there is a very small chance that the actual memory allocated will be 40 bytes, and we should expect 24 bytes. btw. the minimum allocation of malloc is 24, then 40. [code block]",0,0,0,0.9830927848815918,0.9924639463424684,0.9924692511558532,0.0,accept,unanimous_agreement
1001361688,8999,"okay, it doesn't seem to have anything to do with the datatype2 code. so we need to look at what prs related to memory allocation have been merged recently? [code block]",0,0,0,0.9850180745124816,0.9894625544548036,0.9915973544120787,0.0,accept,unanimous_agreement
1001362125,8999,"ohh, i didn't realize that the sanitizer build is using libc malloc. i don't know much about the internals of this allocator. if it can return a different size of allocation on the same requested size, maybe our only way out is to exclude this test assertion when not using jemalloc.",0,-1,0,0.8538500666618347,0.6081287860870361,0.8857924938201904,0.0,accept,majority_agreement
1001362531,8999,"the only thing that was done recently in that respect is the introduction of the sanitizer ci, but even that was quite a while ago, and this test only started to fail a week ago afaik",0,0,0,0.964935839176178,0.9737918376922609,0.8942268490791321,0.0,accept,unanimous_agreement
1001428093,8999,i can reproduce this phenomenon with the following code: [code block] build and run: [code block],0,0,0,0.9843851327896118,0.9850249886512756,0.9938753247261048,0.0,accept,unanimous_agreement
1001431383,8999,i think we should just skip this check when `[s mem_allocator]` doesn't match `jemalloc*`,0,0,0,0.9859360456466676,0.99125075340271,0.9852563738822936,0.0,accept,unanimous_agreement
1001433143,8999,"ok, i will add it.",0,0,0,0.9826520681381226,0.9746254682540894,0.99366694688797,0.0,accept,unanimous_agreement
1001434979,8999,[code block] please also do some cleanups... like: [code block],0,0,0,0.9876170754432678,0.9843318462371826,0.9934489130973816,0.0,accept,unanimous_agreement
1247122545,8999,"i know i'm commenting late on this update. but as a whole, i find this update disturbing. the problem is that fundamental apis are being altered, and not necessarily in a good (planned) way. as an example, consider `lazyfreegetfreeeffort()`. originally, this function was defined as: [code block] the purpose of this function is (as described in the comment) ""return the amount of work needed in order to free an object"". there is no implication that the object is a current entry in the database. there is no implication that the object has an associated key. later, in this update ( [a link] ) the function is defined as: [code block] this update is logically a bug. by adding a key, we are implying that they object is a current entry in the db (which wasn't implied before). the reason for this addition is solely for use by modules and implies the possibility of additional data (not included in the robj) which must be memory managed (freed) along with the robj. but the key alone isn't enough to define an entry in the db. a dbid/key pair is needed. so the function is redefined (again) in this update as: [code block] this new function works in 2 ways. * for everything but modules, only the `*obj` is used * for modules, all 3 parameters are used the api signature itself is weird. `dbid` and `key` are associated, and should fundamentally be presented as a pair. but `key` was added at the beginning, and `dbid` added at the end. i would personally list `dbid` first, because `key` is a further subdivision after db. note that if dbid/key were consistently referred to as a pair, it might have avoided the original logical bug where only the key was provided. dbid/key implies a robj. so this api could really be an either/or type interface - and broken into 2 apis. either dbid/key or robj. (it might be useful to provide an optional robj in the key case to prevent having to look it up again if the value is already available.) adding these (dbid/key) parameters necessitated changes in multiple layers of code to pass these values only for the module case. in general, if parameters are added to a function only for the purpose of calling another function, this could be an indication that there is a design issue. i would assert that since modules are intended to be modular, it could/should be up to the module whether async free is desired or not. i suggest that a more ""modular"" mechanism for this would be to not even attempt to determine ""free effort"" for module items. it shouldn't be up to the redis engine to decide if a module item should be freed in a sync or async manner. when it comes time to free a module item, the redis core should always call the module's free function (synchronously) and let the module decide if it should perform the free in a sync or async manner. a similar, change is made in `objectcomputesize()`. luckily, this function is only used in 1 place. but it does the same things: * adds key and dbid to an existing function, only for the support of modules. it would be better to have a separate function just for modules. * dbid/key is a logical pair. however, they are split in the parameter list with key at the beginning, and dbid at the end. * as before, the dbid/key are only provided to be passed to another function.",-1,0,-1,0.8129492402076721,0.9740438461303712,0.9680042862892152,-1.0,accept,majority_agreement
1247488259,8999,"essentially you are right, i think we can do better if we design all apis from the ground up. but the problem is that the entire module api is very simple at the beginning of the design. for a data type, we often care about not only its value, but also its `keyname` and `dbid` meta-information (i believe a slightly more complex module need them). and these, in the previous api are missing state. we put a lot of effort into patching these apis (like the two prs you mentioned, and things like #3650 ), even though they seem to make changes to the original internal apis, but i think it's worth it. even if the api is completely redesigned, we will find ways to pass this information to the module. regarding the problem that dbid is located after the key, i think one reason is because dbid is an appended parameter, and the other reason is the existing style of the redis source code. you can see similar usage in functions such as `notifykeyspaceevent`, `modulenotifykeyspaceevent`, and `streamcreateconsumer`. i don't think there's anything wrong with that. a possible explanation is that the redis believes that the key is the most important information, and the dbid is the auxiliary information, which is meaningful only when it is needed.",0,0,0,0.8542941212654114,0.9310193061828612,0.9259156584739684,0.0,accept,unanimous_agreement
2126243763,13285,should the todo in [a link] be handled here?,0,0,0,0.9874820709228516,0.9950167536735536,0.9947777986526488,0.0,accept,unanimous_agreement
2126923630,13285,i will handle it on the next commit.,0,0,0,0.9831157326698304,0.9853196144104004,0.9829182624816896,0.0,accept,unanimous_agreement
1489855209,11982,"testing the performance impact of `extend_to_usable()` with sds. test environment: ubuntu 22.04, 8 core, 16g mem. test code: [code block] result: [code block] [code block]",0,0,0,0.9856353998184204,0.9939634203910828,0.990540623664856,0.0,accept,unanimous_agreement
1489862975,11982,"test environment: ubuntu 22.04, 8 core, 16g mem. benchmark command: memtier_benchmark --command=""lpush l fooboo"" --hide-histogram --test-time 30 -x 5 -c 1000 --pipeline=10 ~~although `extend_to_usable()` has 18% degration impact on the performance of `sdsnew()`, no significant impact is seen on real scenarios.~~ [wrong result, please see the following]",0,0,0,0.9739006161689758,0.9805620312690736,0.9816942811012268,0.0,accept,unanimous_agreement
1490577689,11982,"as far as the block extension hint for gcc/clang is concerned, i think this looks ok; i can't comment on the coding conventions/style (edit: because i don't have experience with the redis code base). i wonder what the overhead would be if the usable_size functions didn't return any additional memory, i.e. they returned the exact same size that was requested. most allocators are quite efficient with resizing upwards if the new size is not too different from the earlier one. istm that the difference would be one allocator call vs usable_size call but not as a 1:1 replacement; there ought to be less allocator calls than usable_size calls.",0,0,1,0.8030775189399719,0.8896026611328125,0.6535384058952332,0.0,accept,majority_agreement
1490623774,11982,"we actually had a recent change about using je_nallocx to avoid an excessive call to realloc for shrinking an allocation, in case it's bound to fall into the same allocation bin, see #11766. not sure what's the reason, but that's an indication that a no-op realloc could still be expensive.",0,0,0,0.9781194925308228,0.9629881381988524,0.9868289828300476,0.0,accept,unanimous_agreement
1490631390,11982,"please describe at the top comment the implications of the problem (compilers, specifically new ones), would sigabrt due to fortification checks. if we can somehow narrow it down to specific redis versions and specific set of compilers that were vulnerable that would be nice (but i suppose we can't).",0,0,0,0.9869354367256165,0.981687605381012,0.9824240803718568,0.0,accept,unanimous_agreement
1490648552,11982,"i can answer the compilers part; it'll be most likely at `_fortify_source=3` with all compilers that support it, i.e. clang 9.0 or later and gcc 12 or later alongside glibc 2.33 or later.",0,0,0,0.9881302118301392,0.9913961291313172,0.9935998916625975,0.0,accept,unanimous_agreement
1490650107,11982,"that pr seems to indicate a performance issue with jemalloc realloc, so maybe it's an unrelated issue?",0,0,0,0.9786336421966552,0.9856008887290956,0.9840962886810304,0.0,accept,unanimous_agreement
1491232292,11982,"updating the latest benchmarking results, it seems that the result of [a link] is wrong. as can be seen from the results, the overall performance degradation is ~1.7%. memtier_benchmark --command=""lpush l fooboo"" --hide-histogram --test-time 600 -x 1 -c 1000 --pipeline=10 [code block] [code block] [code block] memtier_benchmark --command=""lpush l fooboo"" --hide-histogram --test-time 600 -x 1 -c 1000 --pipeline=100 [code block] [code block]",0,0,0,0.9050453305244446,0.9873191118240356,0.9715221524238586,0.0,accept,unanimous_agreement
1491809190,11982,"this issue only affects after 7.0.3 (#11196). we didn't add the `alloc_size` attribute before this, so gcc couldn't see the real memory size, which was confirmed by disassembling the binary of version 6.2.8. please help to confirm if my verification is correct. [code block] assembly code: [code block] add `__attribute__((malloc)) __attribute__((alloc_size(1))) ` for zmalloc() [code block]",0,0,0,0.9876082539558412,0.9943536520004272,0.9888939261436462,0.0,accept,unanimous_agreement
1491816018,11982,"without `alloc_size` the compiler won't see the `zmalloc` functions as being allocators and hence will not add the fortification checks. note however that the other allocator functions (jemalloc, glibc malloc, etc.) do have the `alloc_size` attribute and if those get called (and their corresponding malloc_usable_size) then the compiler will see the allocation size and add fortification. so the problem won't be gone, only papered over until a code change exposes it years later.",0,0,0,0.9853996634483336,0.9919129610061646,0.9867708683013916,0.0,accept,unanimous_agreement
1492337118,11982,"right, even before 7.0.3, the compiler can see that zmalloc calls malloc, and be able to track the size of the allocation. and we do use the memory reported by malloc_usable_size in many places for quite some time.. so it doesn't look like anything dramatic was changed in #11666 (which was what uncovered this). i.e. the problem could be hiding in 6.2.0, and even before as well.",0,0,0,0.9702272415161132,0.991503894329071,0.9841904044151306,0.0,accept,unanimous_agreement
1493595798,11982,"correct, it's because the compiler can't see it even with lto. yes, in that case `rm_mallocusablesize` would also need a call to `extend_to_usable` following the `usable_size` call to ensure that code that does see it will be able to see the additional available size too.",0,0,0,0.9868898391723632,0.994201362133026,0.9879993200302124,0.0,accept,unanimous_agreement
1493816569,11982,this issue starts from 5.0.0 that zmalloc_usable() was introduced first.,0,0,0,0.9896071553230286,0.9945113658905028,0.9847126007080078,0.0,accept,unanimous_agreement
1495244734,11982,this issue was triggered on the latest ubuntu 23.04 (default gcc 12).,0,0,0,0.9887388944625854,0.9925054907798768,0.9941790103912354,0.0,accept,unanimous_agreement
1495359669,11982,"do you mean you did this manually? (i see gh ubuntu-latest is 22.04) maybe we can add some daily ci that uses a bleeding edge toolchain. i would like to do some additional manual testing if you can: * please run some performance smoke test again on the last version to see that we didn't get any noticeable hit. * please do some similar tests (old and new toolchains) without lto (or better yet, try to backport this to 7.0) * please also try building it with `malloc=libc` (again with and without lto, or redis 7.0, and various toolchains) * maybe run a quick smoke test using `cflags=-dno_malloc_usable_size` (should be unaffected, right?)",0,0,0,0.8149846792221069,0.991267740726471,0.9874261617660522,0.0,accept,unanimous_agreement
1495363106,11982,"yes, i'm testing in a vm. i am also working on the daily ci.",0,0,0,0.9666156768798828,0.9703015089035034,0.9883033037185668,0.0,accept,unanimous_agreement
1498700178,11982,"in the last commit i fixed the compile warnings and errors under clang with fortify, i'm not sure if they should be fixed in this pr, but it looks like they can be backported to 7.0 easily. 1) fix jemalloc.c [code block] 2) fix config.h they need to be defined before include [code block] 2) fix redis-cli.c and redis-benchmark.c [code block]",0,0,0,0.9584996700286864,0.9907487034797668,0.9696567058563232,0.0,accept,unanimous_agreement
1499891338,11982,the daily ci yml with fortify(in the bottom): [a link] unstable with commit [a link] gcc-12 (failed): [a link] clang (failed): [a link] gcc-12 without lto (failed): [a link] clang without lto (failed): [a link] this pr: gcc-12 (passed): [a link] gcc-11 (no effect): [a link] gcc-12 without lto (passed): [a link] gcc-11 without lto (no effect): [a link] clang (passed): [a link] clang without lto (passed): [a link],0,0,0,0.9716981649398804,0.9887323379516602,0.990354299545288,0.0,accept,unanimous_agreement
1499966449,11982,"now there are still some compile warnings under non-fortify, but it is certain that some of them are due to gcc bugs, maybe we can wait for the next gcc release to fix them. .e.g [a link] this will cause the warning disabled in #11538 to appear again.",0,0,0,0.986562192440033,0.983475923538208,0.991865575313568,0.0,accept,unanimous_agreement
1500247341,11982,"benchmarking under gcc and clang with jemalloc. environment: ubuntu 22.04, 12 cores, 16g mem compiler: gcc 12.1, clang 14 start redis: `taskset -c 0-1 ./src/redis-server --save """"` benchmark command: `taskset -c 4-11 memtier_benchmark --hide-histogram --test-time 60 -x 10 -c 100 --pipeline=10 -t 10` [code block] 1) gcc * unstable [code block] * pr (-2%) [need check] [code block] 2) gcc without lto * unstable [code block] * pr (-0.47%) [code block] 3) clang * unstable [code block] * pr (+1.6%) [need check] [code block] 4) clang without lto * unstable [code block] * pr (-0.36%) [code block]",0,0,0,0.9613154530525208,0.9921649694442748,0.9940212368965148,0.0,accept,unanimous_agreement
1500252913,11982,"benchmarking under gcc and clang with libc (malloc=libc). environment: ubuntu 22.04, 12 cores, 16g mem compiler: gcc 12.1, clang 14 start redis: taskset -c 0-1 ./src/redis-server --save """" benchmark command: taskset -c 4-11 memtier_benchmark --hide-histogram --test-time 60 -x 10 -c 100 --pipeline=10 -t 10 [code block] 1) gcc * unstable [code block] * pr (+0.7%) [code block] 2) gcc without lto * unstable [code block] * pr (-0.25%) [code block] 3) clang * unstable [code block] * pr (+0.55%) [code block] 4) clang without lto * unstable [code block] * pr (~0%) [code block]",0,0,0,0.8670578598976135,0.9914823770523072,0.9913806319236756,0.0,accept,unanimous_agreement
1500261327,11982,"as we can see from the above results, this pr does not result in any significant performance degradation. note that the performance degradation in [a link] was due to my use of `lpush`, but now that the listpack fix is in place with [a link] my initial tests also showed no significant performance degradation, and further testing is needed.",0,0,0,0.97763854265213,0.9896513223648072,0.9875001311302184,0.0,accept,unanimous_agreement
1500310222,11982,"benchmarking under gcc with no_malloc_usable_size. environment: ubuntu 22.04, 12 cores, 16g mem compiler: gcc 12.1 start redis: taskset -c 0-1 ./src/redis-server --save """" benchmark command: taskset -c 4-11 memtier_benchmark --hide-histogram --test-time 60 -x 10 -c 100 --pipeline=10 -t 10 * unstable [code block] * pr (+0.35%) [code block]",0,0,0,0.9805712699890136,0.9929417371749878,0.9929022192955016,0.0,accept,unanimous_agreement
1500559909,11982,"i'm not sure i understand, is that regression gone now? how did that commit affected it? from the plain set/get benchmarks you made (who does rely on usable_size features of sds and the output buffer), we see there's no meaningful impact, so i guess we can proceed to merge it. (please update the top comment). regarding all the compilation warnings / error changes, and additional ci jobs to use bleeding edge toolchains, i suggest to revert these changes from this pr and introduce them in a followup pr. we can backport both to 7.0 later, but i think it'll be clearer to have one deal with the malloc fortification, and another with compilation errors and ci.",0,0,0,0.9636631608009338,0.966500759124756,0.9148720502853394,0.0,accept,unanimous_agreement
1501507917,11982,"after a lot of repeating the tests in [a link] (using rpush instead of lpush to avoid memory copies), i believe this pr does not have any impact. and reset to 2f547caf27149406ab8ce91329ffc038f70377ee did not find any impact, i'm not sure where i'm going wrong.",-1,0,0,0.7757136821746826,0.8817140460014343,0.8557833433151245,0.0,accept,majority_agreement
1501538167,11982,reverted the last commit and completed the top comment.,0,0,0,0.9828944206237792,0.9880010485649108,0.9943056702613832,0.0,accept,unanimous_agreement
1506677467,11982,please don't forget to make a pr with the compilation fixes and the ci coverage.,0,0,0,0.9706504940986632,0.9847560524940492,0.9925406575202942,0.0,accept,unanimous_agreement
1506686124,11982,#12035 is in progress.,0,0,0,0.9741981625556946,0.9845536947250366,0.992189645767212,0.0,accept,unanimous_agreement
1506866570,11982,"note, after discussing it with yossi, we decided not to take this change to 6.x releases, only to 7.0. and even there, in the 7.0 backport we don't declare malloc_size attributes in zmalloc.h so that we don't take the risk of inducing any crashes in a bugfix release, so will only have effect if lto was enforced from outside.",0,0,0,0.9827262163162231,0.9936256408691406,0.9900033473968506,0.0,accept,unanimous_agreement
2182471920,13359,"thanks, could you make a benchmark for this pr?",0,1,0,0.5204640626907349,0.5405148863792419,0.5861629247665405,0.0,accept,majority_agreement
2182691402,13359,i think some arm and risc-v extensions have popcount instructions too. maybe it would be better to use __builtin_popcountll [1]? it would help other architectures assuming their compiler supports it. it appears that intrinsic is just an inline call to that anyways. [code block] [1] [a link],0,0,0,0.9814206957817078,0.9914608001708984,0.9880442023277284,0.0,accept,unanimous_agreement
2182701696,13359,"yes, `__builtin_popcountll` is a good choice.",0,0,0,0.6285038590431213,0.8459115624427795,0.974515736103058,0.0,accept,unanimous_agreement
2185583511,13359,"how about just use __gnuc__ && __clang__ macro. i will assume __builtin_popcountll will use the best choice. here is the benchmark using __builtin_popcountll: the unit of input string len is bytes, and unit of execution time is us: - cpu: hygon c86-4g - os: ubuntu 22.04 - kernel: 5.15.0 - gcc: 11.4 len=256 before optimize = 0.195393 after optimize= 0.106510 len=512 before optimize = 0.390815 after optimize= 0.216116 len=1024 before optimize = 0.779776 after optimize= 0.424560 len=2048 before optimize = 1.508710 after optimize= 0.844279 len=4096 before optimize = 3.015011 after optimize= 1.676645",0,0,0,0.9761717319488524,0.9921808242797852,0.9910195469856262,0.0,accept,unanimous_agreement
2185586807,13359,can you make a benchmark for `bitcount` command?,0,0,0,0.9895824193954468,0.9938122034072876,0.9949363470077516,0.0,accept,unanimous_agreement
2185831367,13359,"here is benchmark for bitcount command. the total time is separated to three parts: client-->server, server processing command, server->client. it seems bitcount processing occupation is not high. the benchmark shows about 10% performance boost. the platform used just as above: len before optimize after optimize 1m 0.000342 0.000312 2m 0.000632 0.000571 4m 0.001212 0.001093 8m 0.002376 0.002147 16m 0.004822 0.004420 the python code is for your reference: import time import redis import random import string def random_string_generator(str_size, allowed_chars): return ''.join(random.choice(allowed_chars) for x in range(str_size)) chars = string.ascii_letters + string.punctuation size = 1024 * 1024 * 16 setkeycmd = 'set ' + 'key1 ' + random_string_generator(size, chars) def get_command_execution_time(command): start_time = time.time() for i in range(1000): result = redis_client.execute_command(command) end_time = time.time() execution_time = (end_time - start_time)/1000 return execution_time, result redis_client = redis.redis(host='localhost', port=6379) redis_client.execute_command(setkeycmd) execution_time, result = get_command_execution_time('bitcount key1') print(""execution time: %.6f seconds"" % execution_time)",0,0,0,0.9675588011741638,0.9492866396903992,0.8300662636756897,0.0,accept,unanimous_agreement
2185907035,13359,please have a look the failue in the cis.,0,0,0,0.9806486964225768,0.9853630065917968,0.9917616844177246,0.0,accept,unanimous_agreement
2262706232,13359,"i realize we are in the wrong way, please have a look [a link] we still need to prove this pr will be faster, including large string, .e.g 512mb",-1,0,-1,0.8817741870880127,0.9586553573608398,0.5708143711090088,-1.0,accept,majority_agreement
2335157920,13359,"wrt [a link] i've added 2 benchmarks for bitcount on bitmaps of 100m(14mb) and 1billion entries (140mb). the benchmark varies the bitcount start and always goes to the end of bitmap. sample command: `""bitcount"" ""users"" ""243001"" ""-1""` for now i don't see any improvement on the results. i'll profile the use-cases and reply back. to reproduce (assuming redis is available at port 6379 and pinned to core 0 ) : [code block] we get: ## baseline unstable ( 31227f4faf8c3bfce92ec458f37485d6bdb2dc62 ) : | test name | metric json path |metric value| |----------------------------------------------------|--------------------------------------------------|-----------:| |memtier_benchmark-1key-1billion-bits-bitmap-bitcount|""all stats"".totals.""ops/sec"" | 347.770| |memtier_benchmark-1key-1billion-bits-bitmap-bitcount|""all stats"".totals.""latency"" | 570.798| |memtier_benchmark-1key-1billion-bits-bitmap-bitcount|""all stats"".totals.""misses/sec"" | 0.000| |memtier_benchmark-1key-1billion-bits-bitmap-bitcount|""all stats"".totals.""percentile latencies"".""p50.00""| 561.151| |memtier_benchmark-1key-100m-bits-bitmap-bitcount |""all stats"".totals.""ops/sec"" | 3506.040| |memtier_benchmark-1key-100m-bits-bitmap-bitcount |""all stats"".totals.""latency"" | 57.022| |memtier_benchmark-1key-100m-bits-bitmap-bitcount |""all stats"".totals.""misses/sec"" | 0.000| |memtier_benchmark-1key-100m-bits-bitmap-bitcount |""all stats"".totals.""percentile latencies"".""p50.00""| 52.991| ## comparison this branch ( f48e329b81038701b8480893cd98de79fb692273 ) : | test name | metric json path |metric value| |----------------------------------------------------|--------------------------------------------------|-----------:| |memtier_benchmark-1key-1billion-bits-bitmap-bitcount|""all stats"".totals.""ops/sec"" | 334.050| |memtier_benchmark-1key-1billion-bits-bitmap-bitcount|""all stats"".totals.""latency"" | 589.658| |memtier_benchmark-1key-1billion-bits-bitmap-bitcount|""all stats"".totals.""misses/sec"" | 0.000| |memtier_benchmark-1key-1billion-bits-bitmap-bitcount|""all stats"".totals.""percentile latencies"".""p50.00""| 589.823| |memtier_benchmark-1key-100m-bits-bitmap-bitcount |""all stats"".totals.""ops/sec"" | 3364.460| |memtier_benchmark-1key-100m-bits-bitmap-bitcount |""all stats"".totals.""latency"" | 59.422| |memtier_benchmark-1key-100m-bits-bitmap-bitcount |""all stats"".totals.""misses/sec"" | 0.000| |memtier_benchmark-1key-100m-bits-bitmap-bitcount |""all stats"".totals.""percentile latencies"".""p50.00""| 55.039|",0,0,0,0.9056264758110046,0.9628351926803588,0.976588487625122,0.0,accept,unanimous_agreement
2357781416,13359,"i test 100m/256m/512m with python above, the performance boost is obvious. cpu: hygon c86-4g os: red hat enterprise linux release 9.4 kernel: 5.14.0 gcc: 11.4.1 len original(s) optimized(s) 100m 0.044603 0.010833 256m 0.112749 0.026016 512m 0.224174 0.051313 actually i noticed there is no improvement in certain circumstances, when i use gcc version = 7.3.0 i objdump bitops.o and find there is no popcnt comparing using gcc version = 11.4.1 could you pls check your gcc version and bitops.o if popcnt is compiled or not?",0,0,0,0.9442554712295532,0.9703606963157654,0.9770411849021912,0.0,accept,unanimous_agreement
2357861744,13359,"adding #pragma gcc target (""popcnt"") will help gcc to compile __builtin_popcountll() to popcnt maybe you can try add this in bitops.c",0,0,0,0.9792842864990234,0.9939644932746888,0.9936230778694152,0.0,accept,unanimous_agreement
2357941207,13359,"a second check, still can't see any benefit, am i missing something? [code block] benchmark data prepration: [code block] [code block] unstable: [code block] this pr: [code block]",0,0,0,0.9730481505393982,0.9782280921936036,0.9124175310134888,0.0,accept,unanimous_agreement
2357998472,13359,"from the assembly code, i am afraid pocnt is not used by your gcc. you can add #pragma gcc target (""popcnt"") in bitops.c or upgrade you gcc version.",-1,0,0,0.8399457335472107,0.7603539228439331,0.908030092716217,0.0,accept,majority_agreement
2358250201,13359,"thanks, i made a commit to tell compiler to use popcnt(), let me wait for the benchmark report.",1,0,1,0.7888563275337219,0.6163032054901123,0.8636181354522705,1.0,accept,majority_agreement
2358613060,13359,"### automated performance analysis summary this comment was automatically generated given there is performance data available. using platform named: intel64-ubuntu22.04-redis-icx1 to do the comparison. in summary: - detected a total of 1 stable tests between versions. - detected a total of 2 improvements above the improvement water line. - median/common-case improvement was 291.5% and ranged from [274.9%,308.1%]. you can check a comparison in detail via the [a link] ### comparison between unstable and bitcount. time period from 5 months ago. (environment used: oss-standalone) #### improvements table | test case |baseline redis/redis unstable (median obs. +- std.dev)|comparison hanhui365/redis bitcount (median obs. +- std.dev)|% change (higher-better)| note | |-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------|-----------------------------------------------------------:|------------------------|-----------| |[a link] | 3420 +- 1.0% (3 datapoints) | 13958|308.1% |improvement| |[a link]| 331 +- 0.4% (3 datapoints) | 1243|274.9% |improvement| improvements test regexp names: memtier_benchmark-1key-100m-bits-bitmap-bitcount|memtier_benchmark-1key-1billion-bits-bitmap-bitcount full results table: | test case |baseline redis/redis unstable (median obs. +- std.dev)|comparison hanhui365/redis bitcount (median obs. +- std.dev)|% change (higher-better)| note | |-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------|-----------------------------------------------------------:|------------------------|-----------| |[a link] | 938168 +- 3.2% (3 datapoints) | 937498|-0.1% |no change | |[a link] | 3420 +- 1.0% (3 datapoints) | 13958|308.1% |improvement| |[a link]| 331 +- 0.4% (3 datapoints) | 1243|274.9% |improvement|",0,0,0,0.8912217020988464,0.987562656402588,0.9213723540306092,0.0,accept,unanimous_agreement
2359835248,13359,"we still need to use `__builtin_cpu_supports` to see whether the cup support `popcnt` instruction, otherwise, it could result in an illegal instruction. 1. we can add new redispopcount method to support `popcnt`, and leave the old one there. 2. use `__builtin_cpu_supports()` to determine which is used.",0,0,0,0.9853555560112,0.9954518675804138,0.992895781993866,0.0,accept,unanimous_agreement
2359860889,13359,"sure, like: if ( __builtin_cpu_supports (""popcnt"") ) { redispopcounthw(); //use popcnt instruction } else { redispopcountsw(); //generic implementation }",0,0,0,0.9880809783935548,0.9917864203453064,0.9918431639671326,0.0,accept,unanimous_agreement
2361455777,13359,"where did you see that `__builtin_cpu_supports` is required? i was under the impression that gcc would use the optimal method for the specific target, regardless if it supports a popcnt instruction or not.",0,0,0,0.9772235155105592,0.9908789992332458,0.99251127243042,0.0,accept,unanimous_agreement
2362499450,13359,"what if we compile with popcnt instruction using gcc, but execute it on another cpu that doesn't support popcnt? i found a similar issue: [a link] and the document in [a link] [code block]",0,0,0,0.97407203912735,0.9871419668197632,0.9919720888137816,0.0,accept,unanimous_agreement
2362501740,13359,"sorry for my bad, i fixed the complaints.",-1,-1,-1,0.983576238155365,0.9929404258728028,0.9948893785476683,-1.0,accept,unanimous_agreement
2362553152,13359,"this seems like an issue where the compiler supports more modern cpu instructions than the host machine. so the code used avx2 instructions that the cpu didn't support, if i understand correctly. in gnulib we just use `__builtin_popcount` if the compiler supports it (based on `__has_builtin` or version checks since that macro is newer) [1]. also, in gcc i see this comment, which makes me think it isn't required [2]: [code block] so, i'm thinking for most machines a simple `popcnt` instruction is emitted by the compiler. for systems without it, a suitable replacement is emitted. [1] [a link] [2] [a link]",0,0,0,0.9412152171134948,0.9902338981628418,0.9866961240768432,0.0,accept,unanimous_agreement
2362572563,13359,"yes, but this is all at compile phase, not runtime. `__has_builtin` is only used to check whether the compiler supports `__builtin_popcount`, and has nothing to do with the popcnt instruction. but we now enforce the popcnt target, telling the compiler to use `popcnt` anyway instead of the expensive builtin. :smiling_face_with_tear: at runtime, the cpu can no longer do the replacement operation, maybe we can test it by using `qemu`.",-1,0,1,0.3770262002944946,0.9930075407028198,0.8301307559013367,,review,no_majority_disagreement
2362650589,13359,"gcc should preform feature checks and compile for the host machine, unless you use something like `gcc -march=some-other-cpu` [1]. or in this case `gcc -mpopcnt` on a cpu that doesn't support the instruction. i thought about it, but i forget what cpu's don't support it anymore. any recent x86 should have it, atleast i think. :cry: [1] [a link]",-1,0,-1,0.9135701060295104,0.7110124826431274,0.9920725226402284,-1.0,accept,majority_agreement
2362707944,13359,"the only scenario i can think of is compiling on model x86 cpu and running on older cup, but that seems to be rarely. so i think we can leave it there, wait for others' opinions. ping",0,0,0,0.9434823393821716,0.9783032536506652,0.978487193584442,0.0,accept,unanimous_agreement
2363590030,13359,can you also take a look? thanks.,1,1,1,0.7558049559593201,0.772728681564331,0.7508556246757507,1.0,accept,unanimous_agreement
2365205516,13359,nice!! i test in my local and found that the trick can bring a boost. please take a look.,1,1,1,0.9907788038253784,0.9948611855506896,0.9965489506721495,1.0,accept,unanimous_agreement
2366785736,13359,"benchmark after the last change: use the same test data in [a link] unstable [code block] this pr (7x): [code block] thanks a lot, please don't mind i made some many changes, just want to verify early.",1,1,1,0.9096869230270386,0.8882911801338196,0.9317137002944946,1.0,accept,unanimous_agreement
2366938210,13359,"using gcc-8.5, with the latest change (or anything after [a link] ) we don't see a variance vs unstable. in summary: - detected a total of 2 stable tests between versions. you can check a comparison in detail via the [a link] ### comparison between unstable and 92e6523e2d4648bf55dd96bf0e604f288bde1f28. time period from 5 months ago. (environment used: oss-standalone) full results table: | test case |baseline redis/redis unstable (median obs. +- std.dev)|comparison hanhui365/redis 92e6523e2d4648bf55dd96bf0e604f288bde1f28 (median obs. +- std.dev)|% change (higher-better)| note | |-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------|-------------------------------------------------------------------------------------------:|------------------------|---------| |[a link] | 3424 +- 0.1% (3 datapoints) | 3427|0.1% |no change| |[a link]| 331 +- 0.2% (3 datapoints) | 331|-0.0% |no change|",0,0,0,0.9752198457717896,0.9300724267959596,0.9875483512878418,0.0,accept,unanimous_agreement
2367122923,13359,"since __has_builtin was supported since gcc 10, please do the benchmark again, thx.",0,0,0,0.9878790974617004,0.9904688000679016,0.9901973009109496,0.0,accept,unanimous_agreement
2367406691,13359,i'm still unsure what feedback you wanted from me and if you got what you needed. if you did please ack.,-1,-1,-1,0.8381147980690002,0.5463793277740479,0.896023154258728,-1.0,accept,unanimous_agreement
2367413779,13359,":grinning_face_with_smiling_eyes: already got answer from you, thanks.",1,0,1,0.947620451450348,0.5428824424743652,0.9922900795936584,1.0,accept,majority_agreement
2367415657,13359,fully ci: [a link],0,0,0,0.9862918257713318,0.9342020750045776,0.9890276193618774,0.0,accept,unanimous_agreement
2367618166,13359,"### automated performance analysis summary this comment was automatically generated given there is performance data available. using platform named: intel64-ubuntu22.04-redis-icx1 to do the comparison. in summary: - detected a total of 2 improvements above the improvement water line. - median/common-case improvement was 442.7% and ranged from [368.0%,517.4%]. you can check a comparison in detail via the [a link] ### comparison between unstable and bitcount. time period from 5 months ago. (environment used: oss-standalone) #### improvements table | test case |baseline redis/redis unstable (median obs. +- std.dev)|comparison hanhui365/redis bitcount (median obs. +- std.dev)|% change (higher-better)| note | |-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------|-----------------------------------------------------------:|------------------------|-----------| |[a link] | 3424 +- 0.1% (3 datapoints) | 21141|517.4% |improvement| |[a link]| 331 +- 0.2% (3 datapoints) | 1550|368.0% |improvement| improvements test regexp names: memtier_benchmark-1key-100m-bits-bitmap-bitcount|memtier_benchmark-1key-1billion-bits-bitmap-bitcount full results table: | test case |baseline redis/redis unstable (median obs. +- std.dev)|comparison hanhui365/redis bitcount (median obs. +- std.dev)|% change (higher-better)| note | |-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------|-----------------------------------------------------------:|------------------------|-----------| |[a link] | 3424 +- 0.1% (3 datapoints) | 21141|517.4% |improvement| |[a link]| 331 +- 0.2% (3 datapoints) | 1550|368.0% |improvement|",0,0,0,0.951682448387146,0.9867116212844848,0.8721596002578735,0.0,accept,unanimous_agreement
2367627686,13359,"please take a look at the last ci failure, it looks like we still need to determine if it's x86, thanks.",1,1,0,0.692017138004303,0.5478774905204773,0.7512021660804749,1.0,accept,majority_agreement
2398826565,13359,is it ready to merge?,0,0,0,0.98382967710495,0.98954176902771,0.9943804144859314,0.0,accept,unanimous_agreement
2409621205,13359,"yes, it is ok.",0,0,0,0.9520711898803712,0.7858712077140808,0.6528158783912659,0.0,accept,unanimous_agreement
2427881665,13359,### ce performance automation : step 2 of 2 (benchmark) running... this comment was automatically generated given a benchmark was triggered. started benchmark suite at 2024-10-22 00:26:58.369905 and took 1.177628 seconds up until now. status: [###########################-----------------------------------------------------] 33.33% completed. in total will run 3 benchmarks. - 2 pending. - 1 completed: - 0 successful. - 1 failed. you can check a the status in detail via the [a link],0,0,0,0.8629245758056641,0.9241183996200562,0.8916419744491577,0.0,accept,unanimous_agreement
739640315,8094,"thanks for your review, follow this tcl case, my original idea is as follows: ![a link] - the circle is the actual search area of 200 km byradius. - the rectangle is the actual search area of 200 km bybox. q: how to determine that a point is within the axis-aligned rectangle? if the coordinates of this point are greater than the minimum latitude and longitude and less than the maximum latitude and longitude. [code block] after you suggest that you should use height/2, width/2, i think your opinion is correct and logical. so in order to reach the search range of the above pictures, the new parameters are as follows: [code block] is that right?",1,1,1,0.959218442440033,0.9499142169952391,0.9847448468208312,1.0,accept,unanimous_agreement
739718343,8094,"yes, width and height of that box are 400. i'm not aware of a word in english that describes half the width (other than saying `half_width`), so i guess we need to stick to the terminology of the command syntax, and adjust the code.",0,0,0,0.977744996547699,0.8141910433769226,0.9873161315917968,0.0,accept,unanimous_agreement
739865317,8094,"updated: - modify the position of the `bounds` variable and add comments to geoshape. - modify the calculation method of radius_meters, remove sqrt, ues the larger of height/2 and width/2. - geosearch with store|storedist and geosearchstore with store means syntax error. - add extractboxorreply() for parse box - refactor some function name and modify comment i will close some of the comments in your previous comments, please review again when you have time.",0,0,0,0.9840901494026184,0.9935626983642578,0.9887219667434692,0.0,accept,unanimous_agreement
740527234,8094,/core-team please approve the new commands and their syntax.,0,0,0,0.9841092228889464,0.9799829721450806,0.987349569797516,0.0,accept,unanimous_agreement
742272136,8094,can you please also make a redis-doc pr for this?,0,0,0,0.9840736389160156,0.9912289381027222,0.9958004355430604,0.0,accept,unanimous_agreement
742276535,8094,"sure, i will do it.",0,0,0,0.90283203125,0.9770424365997314,0.7208154201507568,0.0,accept,unanimous_agreement
2017135489,13169,[a link] all committers have signed the cla.,0,0,0,0.9875962734222412,0.9764910340309144,0.995100438594818,0.0,accept,unanimous_agreement
2017192313,13169,lgtm :rocket:,0,0,1,0.91496741771698,0.5381109714508057,0.9147164225578308,0.0,accept,majority_agreement
2017303094,13169,:),1,1,1,0.8757086992263794,0.7730679512023926,0.9915001392364502,1.0,accept,unanimous_agreement
2017888202,13169,sounds reasonable. :flexed_biceps:,0,0,1,0.7842085361480713,0.888037919998169,0.9932923316955566,0.0,accept,majority_agreement
2019002443,13169,go ahead :handshake:,0,0,1,0.9296249151229858,0.9336830377578736,0.5377950072288513,0.0,accept,majority_agreement
2019102755,13169,"roses are red, violets are blue, the license is back, as good as new.",0,0,0,0.6302836537361145,0.6432881951332092,0.6675488948822021,0.0,accept,unanimous_agreement
2019224196,13169,thanks for assigning the task to yourself :beaming_face_with_smiling_eyes: hopefully we can get more approvals,1,1,1,0.9740979075431824,0.9169199466705322,0.994813084602356,1.0,accept,unanimous_agreement
2019232662,13169,this seems like a quality pull request,0,0,0,0.9799608588218688,0.9662985801696776,0.8541431427001953,0.0,accept,unanimous_agreement
2019244660,13169,lgtm,0,0,0,0.9795994758605956,0.7242414951324463,0.9618706703186036,0.0,accept,unanimous_agreement
2019291577,13169,is redis development dead? clearly this fix is ready to be merged what they waiting for tsk tsk tsk,0,-1,0,0.9770004749298096,0.6262421011924744,0.9203734993934632,0.0,accept,majority_agreement
2019323743,13169,lgtm,0,0,0,0.9795994758605956,0.7242414951324463,0.9618706703186036,0.0,accept,unanimous_agreement
2019333827,13169,makes sense! let get it merged,1,1,1,0.5492668151855469,0.868434488773346,0.9581241607666016,1.0,accept,unanimous_agreement
2019371978,13169,lgtm,0,0,0,0.9795994758605956,0.7242414951324463,0.9618706703186036,0.0,accept,unanimous_agreement
2019687885,13169,lgtm,0,0,0,0.9795994758605956,0.7242414951324463,0.9618706703186036,0.0,accept,unanimous_agreement
2019958811,13169,lgtm :thumbs_up: make redis great again.,1,1,1,0.8970579504966736,0.9888830780982972,0.9959820508956908,1.0,accept,unanimous_agreement
2020038484,13169,+1,0,0,0,0.696722686290741,0.7702900171279907,0.9816582202911376,0.0,accept,unanimous_agreement
2020418145,13169,lgtm,0,0,0,0.9795994758605956,0.7242414951324463,0.9618706703186036,0.0,accept,unanimous_agreement
2020428474,13169,lgtm!,1,1,1,0.7413927316665649,0.9057464599609376,0.8055883049964905,1.0,accept,unanimous_agreement
2020691156,13169,"what's crazy is that this same pr could be applied to any future version of redis, no matter what their licenses, and probably be ok in a court of law, on the grounds of ""correcting damages"" of the previous crime: violating the copyrights of all the contributors who did not consent to the license change.",-1,-1,-1,0.972967565059662,0.8215810060501099,0.9740741848945618,-1.0,accept,unanimous_agreement
2020693531,13169,looks good to me!,1,1,1,0.988797128200531,0.9916608929634094,0.976562201976776,1.0,accept,unanimous_agreement
2020705868,13169,lgtm,0,0,0,0.9795994758605956,0.7242414951324463,0.9618706703186036,0.0,accept,unanimous_agreement
2021009024,13169,-jo good luck on your future business adventures. the community has spoken. you didn't listen. see you over at [a link],1,1,1,0.9820201992988586,0.9939131140708924,0.9422600865364076,1.0,accept,unanimous_agreement
2021245310,13169,oh rip. the community moved to the fork (lead by maintainers not working at redis). community fork: [a link] (previously placeholderkv) by linux foundation,0,-1,-1,0.9191023111343384,0.8591854572296143,0.9155332446098328,-1.0,accept,majority_agreement
2026449203,13169,linux foundation created [a link] today.,0,0,0,0.9840155839920044,0.9919970631599426,0.9932911396026612,0.0,accept,unanimous_agreement
2027068950,13169,i think he's well aware of that: [a link],0,0,0,0.8784016370773315,0.7734878063201904,0.9718406796455384,0.0,accept,unanimous_agreement
2027670134,13169,don't think this change is a breaking change why refuse the pr,-1,0,0,0.5009121894836426,0.6616491675376892,0.6219701170921326,0.0,accept,majority_agreement
2027679633,13169,valkey is placeholderkv. (they gave it a name),0,0,0,0.9838646650314332,0.9859214425086976,0.989657461643219,0.0,accept,unanimous_agreement
2027708251,13169,"thanks, i updated my comment",1,1,1,0.7364676594734192,0.9017574787139891,0.7607497572898865,1.0,accept,unanimous_agreement
2050466476,13169,lgtm,0,0,0,0.9795994758605956,0.7242414951324463,0.9618706703186036,0.0,accept,unanimous_agreement
2234860048,13169,lgtm,0,0,0,0.9795994758605956,0.7242414951324463,0.9618706703186036,0.0,accept,unanimous_agreement
147048935,2795,"+1, this would be handy",1,1,1,0.5139264464378357,0.7590450644493103,0.8318653106689453,1.0,accept,unanimous_agreement
880650483,2795,like this. can you rebase and add a test. wdyt?,0,0,0,0.9559207558631896,0.8008378744125366,0.9833835959434508,0.0,accept,unanimous_agreement
880659063,2795,"definitely useful for saving a 7-liner scriprt - i'm for it. my only beef is the naming - `nx` means ""if not exists"" elsewhere in redis, whereas here we're talking about ""if no ttl/expiry"". so, mebbe `ne` or `nt`?",1,1,1,0.7928484678268433,0.4949036538600921,0.9729297161102296,1.0,accept,unanimous_agreement
882006808,2795,"i agree this would be nice to have. in addition to the nx feature, maybe it would also be useful to gave gt and lt (grater than / less than) feature. my other concern is that as soon as e add these argument flags, it would mean that we can't change the command to be variadic in the future (taking multiple key names). also, looking at the linked issue, there was a request to return the ttl rather than 0 / 1. /core-team please share your thoughts.",1,0,1,0.5275668501853943,0.6976344585418701,0.827044665813446,1.0,accept,majority_agreement
882014843,2795,no problem. let me catch up,0,1,1,0.8094571828842163,0.545478105545044,0.9026896357536316,1.0,accept,majority_agreement
882053852,2795,`nx` here means if a ttl not exists. i just followed previous attempts of this feature for naming. and i'm pretty open for other suggestions. ~~for now i don't have a use case for `gt` and `lt` but the code base is open for adding more flags here.~~ i just realized `lt` is to fix same issue with my `nx`. these two tasks can be done via pipeline as a workaround (pipelining multiple `expire` and `ttl` command). so for now i think it's not very urgent.,1,0,0,0.5166975855827332,0.7196437120437622,0.7578487396240234,0.0,accept,majority_agreement
882063490,2795,"you mean that lt would fix your problem the same way that nx does, but there are probably use cases that will benefit from one and not from the other. in fact, now that i think of it, there are probably also use cases for xx (modify only if already volatile).",0,0,0,0.9868242144584656,0.9888217449188232,0.9875848293304444,0.0,accept,unanimous_agreement
882072395,2795,i agree and there are always possibilities. but before we have real scenario i would suggest to keep the code simple and open for new options.,0,0,0,0.9272415637969972,0.832829475402832,0.966516137123108,0.0,accept,unanimous_agreement
882073991,2795,"i think that having smarter `expire` is more useful than having it variadic, which i believe has very little value over pipelining.",0,0,0,0.926684021949768,0.9612933397293092,0.972950041294098,0.0,accept,unanimous_agreement
882181742,2795,the patch is ready for review.,0,0,0,0.977831244468689,0.9765821099281312,0.9830654859542848,0.0,accept,unanimous_agreement
882283107,2795,"i think we wanna take this opportunity to add all the other useful options. and we still need to conclude if we can / want to change the response type when any of these options are used (as requested in the linked issue). maybe that's valid to do similarly as how spop changes return value when count is added, or maybe we can add a getttl argument. let's wait for further feedback..",0,0,0,0.9732613563537598,0.9674555659294128,0.7202463150024414,0.0,accept,unanimous_agreement
883127992,2795,"`lt`/`gt`/`xx` sound good to me. regarding return value, if i'm correct 0 can still be a valid value telling us the key expired or never existed while anything larger than 0 will give us the current ttl (this will need to be adjusted to unixtime for `at` variants and millisecs for `p` variants). this seems like a relatively small compatibility break, so i'm for it.",1,0,1,0.9448807835578918,0.5669434666633606,0.8032205104827881,1.0,accept,majority_agreement
886203158,2795,all issues resolved. thanks for your patient and great suggestions.,1,1,1,0.9810225367546082,0.9867796897888184,0.9892927408218384,1.0,accept,unanimous_agreement
886210488,2795,thanks.. can you also make a pr for [a link] ?,1,1,1,0.9284768104553224,0.6700851321220398,0.876848042011261,1.0,accept,unanimous_agreement
886430306,2795,please merge my indentation fixes suggestions (it seems this pr doesn't let me edit the code) /core-team please approve the new flags for the expire group commands.,0,0,0,0.9839339852333068,0.971950352191925,0.9945138692855836,0.0,accept,unanimous_agreement
886834564,2795,doc changes created at [a link],0,0,0,0.9882609248161316,0.9848408102989196,0.9950892329216005,0.0,accept,unanimous_agreement
717120000,7953,"just mentioning that i didn't go over the code yet (just took a quick look at the comments gave). p.s. maybe in a followup pr, you can add module api callback so that modules can provide cloning functionality. but let's put that aside for now.",0,0,0,0.9771009683609008,0.9734398126602172,0.9851810932159424,0.0,accept,unanimous_agreement
718262786,7953,"thanks ! i fixed the code, but i wasn't sure how to fix things like removing the fast flag and keeping the lru. please review.",1,1,1,0.9780689477920532,0.9843251705169678,0.988730490207672,1.0,accept,unanimous_agreement
718812665,7953,"you did remove the `fast` flag (added `use-memory` instead). and i think the conclusion from the discussion was that we want it to behave like restore, so we we use dbdelete and dbadd (don't keep the lru). both of these aspects are fine in the last version of the code in that respect (unless we hear other concerns / opinions that support changing that). i see you changed to lookupkeyread, which is wrong imho, let's wait for to respond in that discussion.",0,0,0,0.9496464729309082,0.9727087020874025,0.9743008017539978,0.0,accept,unanimous_agreement
721899433,7953,"the lookupkeyread behavior is weird, but it looks like it's well established in other places, so i guess we should keep that consistent. still a couple of other open comments.",-1,-1,-1,0.8274751305580139,0.9158790111541748,0.9361882209777832,-1.0,accept,unanimous_agreement
723064720,7953,"thanks in addition to the areas you suggested i should fix, i have also changed the code that copies the inset object. should i also do a fix to include the consumer group information in the digest of the stream key? i'm not sure i can implement it correctly.",1,0,1,0.6350613236427307,0.8009569048881531,0.6756573915481567,1.0,accept,majority_agreement
723522346,7953,"thank you for reviewing my code. please check it out as i have corrected it. i have also fixed my mistake that i found in the dupstream function. and, i don't need to use the stream consumer group's digest, so i won't touch it.",1,1,1,0.8951950669288635,0.9479663372039796,0.9824678301811218,1.0,accept,unanimous_agreement
723576576,7953,"thanks i have corrected my codes. i wrote the code in reference to streamreplywithrange in t_stream.c, but i didn't understand the streamreplywithrange function correctly, so the part about adding nack to the group and consumer pel got complicated. after reading the code again, i thought that the same key nack would never go into multiple people's pel. i also fixed the part about creating a new consumer. let me know if there is any problems with this fix.",1,1,1,0.8648992776870728,0.5168209075927734,0.9653015732765198,1.0,accept,unanimous_agreement
723578987,7953,looks like i [a link] to your last push before you submitted your last message. afaik the entries in the pel of the group and consumers point to the same memory (see rdb loading).,0,0,0,0.9878820180892944,0.992096781730652,0.9929500818252563,0.0,accept,unanimous_agreement
724728192,7953,"before we seal this one, i have a couple of notes: * should this be a multi-key command? i'm torn between liking the simplicity (e.g. `move`), my general dislike of multi-key commands but acknowledging there may be a future (see `migrate`). * should this support a `delete` (or similar) option to delete the source key? this would make this into an uber-command that could eventually replace/deprecate `move` and `rename` * the current implementation copies the source's ttl, if set. i agree with this default, but perhaps we'd like to add a `nottl` (or similar) option.",0,0,0,0.5838181376457214,0.9015439748764038,0.7952826023101807,0.0,accept,unanimous_agreement
724738849,7953,"1. it must be a multi-key command, unless we only want to let it copy a key to another db (keeping it's name), which i think is a big miss 2. i would have considered it a good idea to add a delete argument if they where sharing code, but the fact is that if we do that, it'll just be a shortcut to the move/rename implementation which is a `fast` command. so i'm not sure that's wise. but anyway, we can always add more arguments in the future. 3. i guess we can add `nottl`, `ex `, and even `keepttl`. not sure if that's needed, but either way we can add later.",0,0,0,0.8038010597229004,0.9694079756736756,0.7531523704528809,0.0,accept,unanimous_agreement
724741679,7953,"wrt 1, i guess i meant variadic keys, something like `copy source key [...] dest key [...]`",0,0,0,0.9865162372589112,0.9854589700698853,0.9913492202758788,0.0,accept,unanimous_agreement
724744439,7953,"ohh... i don't see the value of that (just sending it inside a multi will do the same thing, no disadvantages). has anyone ever requested such a thing for move or rename?",0,0,0,0.5787463784217834,0.7855899930000305,0.9148067831993104,0.0,accept,unanimous_agreement
724867215,7953,"i don't like the multi-key approach when there are flags involved, it really complicates the command. as oran said you can always multi-exec them. i also agree that we can add flags later as needed. from my understanding the follows up here are: 1. reorganize the functions so that they are located in their respective types. 2. extend the debug digest functionality so that it also includes consumer groups. since neither of these require a major consensus, i am ok with the current iteration. /core-team conensus?",-1,0,0,0.918106496334076,0.6520726084709167,0.684544026851654,0.0,accept,majority_agreement
728823710,7953,thank you. merged.,1,1,1,0.894658625125885,0.9310064315795898,0.9145276546478271,1.0,accept,unanimous_agreement
712811430,7912,"it's a very useful feature and already worked in our product env. i think is's safe to merge after suggestion : ) btw, this pr contains another change, record counts of object freed in bio, info reply modification needs a major decision too.",1,1,1,0.9782146215438844,0.9818852543830872,0.9970197081565856,1.0,accept,unanimous_agreement
714394192,7912,thanks for the clarification! chenyang8094 please consider updating the pr description to more clearly indicate it includes both aspects (api change and info addition).,1,1,1,0.965188443660736,0.9416700005531312,0.9904441237449646,1.0,accept,unanimous_agreement
714872366,7912,"ok, i have updated it",0,0,0,0.9847226738929749,0.9204340577125548,0.9935181736946106,0.0,accept,unanimous_agreement
715368424,7912,"i would like to use this opportunity to add a few more changes in that area. first, in the spirit of #7865, it would be nice if a module can get the compiled version of redismodule_type_method_version so that when registering the callbacks it knows which of them are gonna be used and which will be ignored. secondly, i know a few complicated modules also keep some global information about their keys (outside redis's dict), while this is a bit of a violation, they get away with it since redis is not able to do anything with that pointer without the module's assistance (not even free it), but now that redis can detach that key from the keyspace in the main thread, and free it in a background thread, i think we need to give the module a little bit more help. what i think would solve it is to add another `detach` callback which will also carry the keyname being detached. this will solve two things. 1) in some say it covers for the missing keyname argument from the free callback (i know some modules really miss it). 2) it tells the module that this pointer is no longer part of the database, and also that it will be soon freed by a thread. i would like to get some feedback from , , . if we agree on the details, can ask to extend this pr, or one of us can make a followup pr, but we probably don't wanna release that without these (and more?) additions (due to the version bump). p.s. i vaguely remember there were a few other things we wanna do by iterating on module data structure (inside its value pointer), which would be nice to add soon too, but i don't recall what. i.e. we already have `mem_usage` and `digest`, we're missing a `defrag` and i think something else, but i don't recall. maybe clone support for #6599?",0,0,0,0.9147288799285888,0.9222882986068726,0.9077479839324952,0.0,accept,unanimous_agreement
715591510,7912,"i agree we need the `detach` callback so we can also detach the object from globals and just free the memory on the free callback. i am not sure if the `detach` and `free_effort` can somehow combine, do we use the `free_effort` on more places other then free the key?",0,0,0,0.9784476161003112,0.9892617464065552,0.9905678629875184,0.0,accept,unanimous_agreement
715710513,7912,"i meant that the `detach` callback should be called before a normal `free` too. this way the detach is always called when detaching a value, from the db, and the free callback is just to release the memory. also, it mans you kinda have the key name for the non-lazy free. p.s it might be a good idea to add an `attach` callback too, so on `rdbload` you don't assume it was added to the db. this way we can some day do the deserialization in a thread, and the `attach` in the main thread, and modules that do funny things will do them in the attach and wont suffer from multi threading issues.",0,0,0,0.9151155352592468,0.7995906472206116,0.981855809688568,0.0,accept,unanimous_agreement
716035230,7912,"i get the idea of the `detach`, i was just wondering if it makes sense to combine the `detach` and the `free_effort` to make the api less verbose (for example, maybe the `detach` can return the free effort, and then redis can choose if he wants to free the memory immediately or pass it to a background thread). not sure if it's a good idea, just a thought, what do you think? regarding the `attach`, today we have the `loaded` keyspace notification for modules only, maybe we can somehow combine it there? again to make the api less verbose?",0,0,0,0.8740678429603577,0.9545815587043762,0.8878511190414429,0.0,accept,unanimous_agreement
716107878,7912,"i'm not sure about combining free_effort and detach. usually redis knows if it's willing to consider doing a lazy free or not, so combining these will force the module to ""compute"" the free effort even when there's no need. on the other hand, this computation is meant to be quick (and doesn't have to be accurate). regarding `loaded` keyspace notification, and attach, i don't think these two are related. the keyspace notification is received for all keys types, and the attach would happen only for the module type. in addition the attach can carry the actual module data pointer with it (possibly saving the module an rm_openkey). -m fyi - i think you also had trouble with `free` not providing the key name.",0,0,0,0.9332866072654724,0.9760846495628356,0.964148998260498,0.0,accept,unanimous_agreement
716363515,7912,"i think all of this calls for a more careful design, and we don't necessarily have to address it all at once or as part of this pr. one issue is exposing the two-step lazy free to modules, so they also get an `unlink` callback. this makes sense to me as a direct extension to this pr. it will require considering what is the expected outcome depending on what callbacks are/aren't implemented, and how this affects backwards compatibility. the issue of the opposite `link` callback seems to me unrelated, and i'm not even sure it represents a problem at present time. note that `rdbload` already does not imply the key gets linked (e.g. with `rm_loaddatatypefromstring()`) and this is indicated by not having a key name associated with the `redismoduleio` context.",0,0,0,0.863143265247345,0.9672712683677672,0.859825074672699,0.0,accept,unanimous_agreement
716499195,7912,"i agree much of what i raised is not directly related and can be handled separately (just felt that this is an opportunity to raise it). the detach / unlink thing though is a bit related, at least in the sense that we should avoid releasing the api in this pr until we finished designing that part too since we might wanna change that api. p.s. even if the additional api doesn't change the api of this pr, i rather not increment redismodule_type_method_version twice. regarding backwards compatibility, if the detach and free effort come in the same version, then i see no problem, old modules that don't implement, will always be freed from the main thread. so what do you suggest? wanna call the core-team to approve it and merge it as is, and then open another pr to add / modify it? or try to at least integrate the detach / unlink callback into this pr?",0,0,0,0.9101126194000244,0.7852603197097778,0.9182249307632446,0.0,accept,unanimous_agreement
717656276,7912,"i can understand what you mean very much. `detach` mainly solves the problem that `free` callback does not provide parameter key. when i was writing a module product, i did save the key separately in the index of the module, but it was really difficult for me to get this information when the key was deleted. i think `detach` is useful for `free/lazyfree`. if possible, i can extend this pr and add `detach` and `free_effort` to the v3 version.",0,0,1,0.8821372389793396,0.9693740010261536,0.6632074117660522,0.0,accept,majority_agreement
717737266,7912,"i think that we all agree that `unlink` (better fits redis terminology) makes sense, and probably is a must-have feature in order to push the lazy free feature in. we need to make sure this can work in a backwards compatible manner for old modules that are not aware of it, but i actually don't see any problem (seems it'll work good already with the trivial implementation). please go ahead and add this change to this pr, it'll be easier to discuss when we see the code.",0,0,1,0.9272568225860596,0.7224850058555603,0.803133487701416,0.0,accept,majority_agreement
718391465,7912,"i have tried to modify the code according to my own understanding, please review it again, thank you.",1,1,0,0.8291226625442505,0.6870898604393005,0.713649332523346,1.0,accept,majority_agreement
720560931,7912,"thank you. one more thing i need to ask: we need a better commit comment that describes the changes. one way is to edit / update the top comment in this pr, which i can use when squash-merging the pr (will create one commit in unstable). or if we want to keep it two separate commits, you need to edit the commit comments and force-push. /core-team please approve: - new rm_gettypemethodversion - new module type callback for lazy free effort - new module type callback for detaching a key from the database prior to freeing it (comes with key name and value) - new lazyfreed_objects info field",1,1,1,0.9527183771133424,0.8670229315757751,0.9613171219825744,1.0,accept,unanimous_agreement
721079274,7912,"i have used rebase to merge the two commits into one, and re-edited the commit message. please check again if there are any problems, thank you.",1,1,1,0.8606970310211182,0.7814149260520935,0.7380756735801697,1.0,accept,unanimous_agreement
721589180,7912,"i read the codes and discussions above, if i understand right the `unlink` callback in this pr is a notify way to tell module the specific key-value is unlinked from redis db's dict, it's ok to me. and seems we don't implement the `detach`(i think this func means the move semantics) and `attach` yet, maybe need another pr right?",0,0,0,0.9776880145072936,0.9600423574447632,0.9604912400245668,0.0,accept,unanimous_agreement
721604628,7912,the `detach` is the `unlink` same thing. for now we left the `attach` for some future pr.,0,0,0,0.9880779385566713,0.9909808039665222,0.9942553639411926,0.0,accept,unanimous_agreement
721862927,7912,"(i have only read the code, not the correspondence) why won't the unlink and free_effort functions take a redismodulekey* instead of just the value (and keyname) inside the redismodulekey we can access the keyname, the value and the redismodulectx",0,0,0,0.9827221035957336,0.9916917085647584,0.9926580786705016,0.0,accept,unanimous_agreement
721891301,7912,"it was discussed here: [a link] please take a look and if you can, it would help if you can provide reasoning for changing it. i.e. what are the limitations of the current code. thanks.",1,1,1,0.8968896269798279,0.757720410823822,0.9686505794525146,1.0,accept,unanimous_agreement
721911470,7912,"well, there are many things that are impossible to do without a redis context.. like opening a key. logging is possible but it won't know which module logged it. generally speaking, this is an api that we can't change later on, why not give as much information as possible? i think everybody here knows how painful and annoying it is that the `free` cb doesn't have a keyname...",-1,-1,-1,0.9854612946510316,0.980063796043396,0.98993182182312,-1.0,accept,unanimous_agreement
721943349,7912,"logging is no longer an issue, as there is a way to create a detached thread safe context from a regular context so the module identity is preserved. i understand your concern but i also see the other side of giving modules too much rope, especially in such low-level callbacks that can be triggered by many unexpected code paths (just think about the re-entrancy options here!). i think that the downside of creating and providing a redismodulekey for every data type callback (or at least the new ones we add here) is greater than the upside. having an additional (optional) name parameter makes more sense to me, and based on past experience it may also be all that modules really need.",0,0,0,0.6736915707588196,0.924842894077301,0.9230556488037108,0.0,accept,unanimous_agreement
722276604,7912,ok i'm convinced - but let's at least add the keyname to the free_effort function?,0,0,0,0.9829809665679932,0.9712575674057008,0.984614372253418,0.0,accept,unanimous_agreement
722298495,7912,seems legitimate to me.. (adding key name to free_effort too). anyone has any objection? or can we ask to add it and merge this monster (43 posts)?,0,0,0,0.9209461212158204,0.7074266076087952,0.971830666065216,0.0,accept,unanimous_agreement
722385748,7912,"i don’t think this is really necessary. let’s revisit the function of the `free_effort` callback. it is used to return the amount of work needed in order to free an module **value**, not a key. the release of the key is always synchronous. therefore, `free_effort` only needs to pass the value and let the module return the corresponding effort based on this value. its work is so simple and clear, which is why the `lazyfreegetfreeeffort` function has only one parameter `value`. i don't understand the practical significance of adding a key parameter to `free_effort`. or i did not fully understand what you mean, maybe you can explain it clearly, thank you.",0,0,0,0.6101699471473694,0.973076343536377,0.8712786436080933,0.0,accept,unanimous_agreement
722388151,7912,"well, let's say you have a bug in your free_effort function and you want to investigate it, one might want to add some logs, which will not be very informative without the keyname",0,0,0,0.9721803665161132,0.963836133480072,0.980140507221222,0.0,accept,unanimous_agreement
722401988,7912,"i think we should consider the functions to be implemented when designing the api, rather than for the purpose of debugging. if this is the case, then many apis have to be designed very bloated (such as rdb_save/rdb_load/mem_usage, etc.). the scenario you mentioned is more likely to appear in `free` callback. if you have to know which key is to be freed, because `unlink` must be the first to be called back, it is possible to use a global variable to record, and so is `free_effort`. . in short, this is just a debugging method, the same as using gdb. this is not its job.this is my understanding.",0,0,0,0.9786511063575744,0.9852604269981384,0.9802349805831908,0.0,accept,unanimous_agreement
722407232,7912,"yes, it's just for debugging/logging purposes afaiu - but in that case, what's the justification of adding keyname to the `unlink` function? it's also for logging/debugging, no?",0,0,0,0.9866610169410706,0.9913408160209656,0.9928089380264282,0.0,accept,unanimous_agreement
722414904,7912,"i agree with . i see more use-cases to get the key other than debugging. we say we pass the key to the unlink function because modules might save information out of the keyspace (in some global memory) and they want to ""unlink"" this memory also. in the same manner, modules might want to report the free_effort of this global memory and they might need the key name to find it.",0,0,0,0.9693264365196228,0.9656544923782348,0.9582260847091676,0.0,accept,unanimous_agreement
722759284,7912,"if there is a global memory related to this key that needs to be freed, then i think `unlink` can already provide the name of the key (because `unlink` will definitely be called before `free_effort`), which is also our original intention to add `unlink` callback. if must add the parameter **key** to `free_effort`, then the corresponding `lazyfreegetfreeeffort` must also add too. this will lead to several linkage modifications. please see if this is necessary.",0,0,0,0.9860379099845886,0.9938485026359558,0.9908717274665833,0.0,accept,unanimous_agreement
722875947,7912,"the purpose of `unlink` is not for debugging/logging, you can see the discussion above.",0,0,0,0.9886124730110168,0.991693675518036,0.9925501346588136,0.0,accept,unanimous_agreement
723447668,7912,"maybe i'm missing the point of your last post, but i think what meant is that in case of error, he'd like to log the key name (on both unlink and free_effort callbacks). p.s. regarding your previous post, we currently call the `free_effort` before `unlink` (not after it). [code block] we can probably change that, but i don't like modules to rely on the order of these. at first i thought that modules should not include the global data in their estimated free_effort, since they must release that global data in the `unlink` (not in the `free` callback, since it can be called form a thread), but maybe some module would like to detach data from a global structure and attach it to the object, and that the freeing of that data will be done from the thread. in such case the effort of the unlink remains small, and the one in free becomes bigger. since we can't predict what some crazy modules will do or need, i think we should have the key in both callbacks. and i think we can probably change the order of these calls (just makes a little bit more sense). i don't understand what you mean by ""this will lead to several linkage modifications"".",0,0,0,0.5190681219100952,0.97161203622818,0.9354568123817444,0.0,accept,unanimous_agreement
723563188,7912,"yes, the `free_effort` order is indeed wrong, i have modified it. i have added the parameter **key** to `free_effort`, which also led to the modification of the `freeobjasync` function. please review it, thank you.",1,1,1,0.6713375449180603,0.5193919539451599,0.7808443307876587,1.0,accept,unanimous_agreement
724951181,7912,"so in my eyes, only thing that's left is a [a link] to `modulenotifykeyunlink` in `dboverwrite` can you take care of this?",0,0,0,0.9761202931404114,0.9894543886184692,0.9948339462280272,0.0,accept,unanimous_agreement
725071599,7912,i'll buy the simplicity argument. i do think we should say returning 0 should be explicitly documented that it calls the async free function every time. seem reasonable?,0,0,0,0.9712120294570924,0.7342900037765503,0.9735448956489564,0.0,accept,unanimous_agreement
725071729,7912,"yes, i understand very well, it is very necessary to do things right. thanks for your review, i also thought that `modulenotifykeyunlink` also needs to be called during `flushall/flushdb`, right? since i have been busy with the double 11 these past two days, i will update pr again after this. thank you for your suggestions and patience.",1,1,1,0.9803951382637024,0.988778829574585,0.9867330193519592,1.0,accept,unanimous_agreement
725499564,7912,"good point about flushdb, and more importantly flushdb async. but i don't think we can afford to run on the entire keyspace when detaching the dict from the database, just to call the module unlink callback. wdyt? maybe it should be documented that when the module gets the flushdb event hook it should expect the unexpected?",0,0,1,0.5980361104011536,0.5391632318496704,0.601428747177124,0.0,accept,majority_agreement
725504829,7912,so on flushbd async we will not get the unlink callback and module should use the flush event to handle such case? lgtm but we need to make sure to document it.,0,0,0,0.9874152541160583,0.9917431473731996,0.9938482642173768,0.0,accept,unanimous_agreement
725524082,7912,"trying to sum up what's left (64 comments, lol): 1. call modulenotifykeyunlink in dboverwrite (better be done before calling dictsetval) 2. when free_effort returns 0, always do an async free (consider that an infinite return value) 3. document the above. 4. document in the unlink callback, that it won't be called on flushdb (both sync and async), and the module can use the redismoduleevent_flushdb to hook into that.",0,1,1,0.9529247283935548,0.902006983757019,0.9362986087799072,1.0,accept,majority_agreement
727824371,7912,merged. thank you for your patience and dedication.,1,1,1,0.9564629793167114,0.9809083938598632,0.9793633222579956,1.0,accept,unanimous_agreement
